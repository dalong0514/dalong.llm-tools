[{
  "id": "1872079319813816597",
  "url": "https://x.com/AndrewYNg/status/1872079319813816597",
  "text": "@levie Link to the study I refer to: https://t.co/xgct2iVwEo",
  "createdAt": "Thu Dec 26 00:37:46 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 19,
  "replyCount": 10,
  "likeCount": 150,
  "quoteCount": 1,
  "viewCount": 44986,
  "bookmarkCount": 42,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1872079097121431855",
  "url": "https://x.com/AndrewYNg/status/1872079097121431855",
  "text": "One of the best things the U.S. can do is make high-skill immigration easier. @levie is right. \n\nIt is awful that the wait time for a green card can be over a decade, and that after waiting years someone can still be forced to leave simply because they lost a job. Fixing this is both an economic and a moral issue. \n\nA rigorous economic analysis (by Pierre Azoulay and collaborators) shows that immigrants create more jobs than they take. So to create jobs for Americans, lets let more immigrants in!",
  "createdAt": "Thu Dec 26 00:36:52 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 376,
  "replyCount": 375,
  "likeCount": 3187,
  "quoteCount": 57,
  "viewCount": 368103,
  "bookmarkCount": 285,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1870965047738220934",
  "url": "https://x.com/AndrewYNg/status/1870965047738220934",
  "text": "Sriram has been consistently thoughtful about AI policy, including specifically the importance of promoting open source. His working with @DavidSacks on AI will be good for innovation and good for the U.S. Thank you @sriramk for your service!",
  "createdAt": "Sun Dec 22 22:50:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 172,
  "replyCount": 76,
  "likeCount": 1743,
  "quoteCount": 12,
  "viewCount": 198579,
  "bookmarkCount": 82,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1869783741566202074",
  "url": "https://x.com/AndrewYNg/status/1869783741566202074",
  "text": "I‚Äôm thrilled that former students and postdocs of mine won both of this year‚Äôs NeurIPS Test of Time Paper Awards. This award recognizes papers published 10 years ago that have significantly shaped the research field. The recipients included Ian Goodfellow (who, as an undergraduate, built my first GPU server for deep learning in his dorm room) and his collaborators for their work on generative adversarial networks, and my former postdoc Ilya Sutskever and PhD student Quoc Le (with Oriol Vinyals) for their work on sequence-to-sequence learning. Congratulations to all these winners!\n\nBy nature, I tend to focus on the future rather than the past. Steve Jobs famously declined to build a corporate museum, instead donating Apple's archives to Stanford University, because he wanted to keep the company forward-looking. Jeff Bezos encourages teams to approach every day as if it were ‚ÄúDay 1,‚Äù a mindset that emphasizes staying in the early, innovative stage of a company or industry. These philosophies resonate with me.\n\nBut taking a brief look at the past can help us reflect on lessons for the future. One takeaway from looking at what worked 10 to 15 years ago is that many of the teams I led bet heavily on scaling to drive AI progress ‚Äî a bet that laid a foundation to build larger and larger AI systems. At the time, the idea of scaling up neural networks was controversial, and I was on the fringe. I recall distinctly that, around  2008, Yoshua Bengio advised me not to bet on scaling and to focus on inventing algorithms instead!\n\nA lesson I carry from that time is to not worry about what others think, but follow your convictions, especially if you have data to support your beliefs. Small-scale experiments performed by my Stanford group convinced me that scaling up neural networks would drive significant progress, and that‚Äôs why I was willing to ignore the skeptics. The diagram below, generated by Adam Coates and Honglak Lee, is the one that most firmed up my beliefs at that time. It shows that, for a range of models, the larger we scaled them, the better they perform. I remember presenting it at CIFAR 2010, and if I had to pick a single reason why I pushed through to start Google Brain and set as the team‚Äôs #1 goal to scale up deep learning algorithms, it is this diagram!\n\nI also remember presenting at NeurIPS in 2008 our work on using GPUs to scale up training neural networks. (By the way, one measure of success in academia is when your work becomes sufficiently widely accepted that no one cites it anymore. I‚Äôm quite pleased the idea that GPUs should be used for AI ‚Äî which was controversial back then ‚Äî is now such a widely accepted ‚Äúfact‚Äù that no one bothers to cite early papers that pushed for it.üòÉ)\n\nWhen I started Google Brain, the thesis was simple: I wanted to use the company‚Äôs  huge computing capability to scale up deep learning. Shortly afterward, I built Stanford‚Äôs first supercomputer for deep learning using GPUs, since I could move faster at Stanford than within a large company. A few years later, my team at Baidu showed that as you scale up a model, its performance improves linearly on a log-log scale, which was a precursor to OpenAI‚Äôs scaling laws.\n\nAs I look to the future, I‚Äôm sure there are ideas that many people are skeptical about today, but will prove to be accurate. Scaling up AI models turned out to be useful for many teams, and it continues to be exciting, but now I‚Äôm even more excited by upcoming ideas that will prove to be even more valuable in the future.\n\nThis past year, I spent a lot of time encouraging teams to build applications with agentic AI and worked to share best practices. I have a few hypotheses for additional technologies that will be important next year. I plan to spend the winter holiday playing with a few of them, and I will have more to share next year. But if you have an idea that you have conviction on, so long as you can do so responsibly, I encourage you to pursue it!\n\n[Original text (with links): https://t.co/Km7ENTODId]",
  "createdAt": "Thu Dec 19 16:35:57 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 97,
  "replyCount": 47,
  "likeCount": 693,
  "quoteCount": 5,
  "viewCount": 58428,
  "bookmarkCount": 126,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1869421643925422166",
  "url": "https://x.com/AndrewYNg/status/1869421643925422166",
  "text": "OpenAI just announced API access to o1 (advanced reasoning model) yesterday. I'm delighted to announce today a new short course, Reasoning with o1, built with @OpenAI, and taught by @colintjarvis, Head of AI Solutions at OpenAI, to show you how to use this effectively!\n\nUnlike previous language models which generate output directly, o1 ‚Äúthinks before it responds,‚Äù and generates many reasoning tokens before returning a more thoughtful and accurate response. It is great at complex reasoning -- including planning for agentic workflows, coding, and domain-specific reasoning in STEM fields like law. But how you should use it is quite different from other LLMs. \n\nI think o1 will be a game changer for many AI applications; and in this course, you'll learn how to use it effectively. \n\nIn detail, you‚Äôll:\n- Learn to recognize what tasks o1 is suited for, and when to use a smaller model, or combine o1 with a smaller model\n- Understand the new principles of prompting reasoning models: Be simple and direct; no explicit chain-of-thought required; use structure; show rather than tell\n- Implement multi-step orchestration in which o1 plans, and hands tasks over to gpt-4o-mini to execute specific steps; this illustrates a design pattern to optimize intelligence (accuracy) and cost\n- Use o1 for a coding task to build a new application, edit existing code, and test performance by running a coding competition between o1-mini and GPT 4o\n- Use o1 for image understanding and learn how it performs better with a \"hierarchy of reasoning,\" in which it incurs the latency and cost upfront, preprocessing the image and indexing it with rich details so it can be used for Q&A later\n- Learn a technique called meta-prompting, in which you use o1 to improve your prompts. Using a customer support evaluation set, you'll iteratively use o1 to modify a prompt to improve performance\n\nYou'll also learn about how OpenAI used reinforcement learning to produce a model that uses \"test-time compute\" to improve performance.\n\nI think you'll find this course enjoyable and valuable. \n\nPlease sign up for it here: https://t.co/0XIGzinyrx",
  "createdAt": "Wed Dec 18 16:37:06 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 426,
  "replyCount": 84,
  "likeCount": 2747,
  "quoteCount": 47,
  "viewCount": 355748,
  "bookmarkCount": 2424,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1867269937397670082",
  "url": "https://x.com/AndrewYNg/status/1867269937397670082",
  "text": "AI Product Management\n\nAI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications. This is making it possible to build new kinds of things, which in turn is driving shifts in best practices in product management ‚Äî the discipline of defining what to build to serve users ‚Äî because what is possible to build has shifted. In this post, I‚Äôll share some best practices I have noticed.\n\nUse concrete examples to specify AI products. Starting with a concrete idea helps teams gain speed. If a product manager (PM) proposes to build ‚Äúa chatbot to answer banking inquiries that relate to user accounts,‚Äù this is a vague specification that leaves much to the imagination. For instance, should the chatbot answer questions only about account balances or also about interest rates, processes for initiating a wire transfer, and so on? But if the PM writes out a number (say, between 10 and 50) of concrete examples of conversations they‚Äôd like a chatbot to execute, the scope of their proposal becomes much clearer. Just as a machine learning algorithm needs training examples to learn from, an AI product development team needs concrete examples of what we want an AI system to do. In other words, the data is your PRD (product requirements document)!\n\nIn a similar vein, if someone requests ‚Äúa vision system to detect pedestrians outside our store,‚Äù it‚Äôs hard for a developer to understand the boundary conditions. Is the system expected to work at night? What is the range of permissible camera angles? Is it expected to detect pedestrians who appear in the image even though they‚Äôre 100m away? But if the PM collects a handful of pictures and annotates them with the desired output, the meaning of ‚Äúdetect pedestrians‚Äù becomes concrete. An engineer can assess if the specification is technically feasible and if so, build toward it. Initially, the data might be obtained via a one-off, scrappy process, such as the PM walking around taking pictures and annotating them. Eventually, the data mix will shift to real-word data collected by a system running in production.\n\nUsing examples (such as inputs and desired outputs) to specify a product has been helpful for many years, but the explosion of possible AI applications is creating a need for more product managers to learn this practice.\n\nAssess technical feasibility of LLM-based applications by prompting. When a PM scopes out a potential AI application, whether the application can actually be built ‚Äî that is, its technical feasibility ‚Äî is a key criterion in deciding what to do next. For many ideas for LLM-based applications, it‚Äôs increasingly possible for a PM, who might not be a software engineer, to try prompting ‚Äî or write just small amounts of code ‚Äî to get an initial sense of feasibility.\n\nFor example, a PM may envision a new internal tool for routing emails from customers to the right department (such as customer service, sales, etc.). They can prompt an LLM to see if they can get it to select the right department based on an input email, and see if they can achieve high accuracy. If so, this gives engineering a great starting point from which to implement the tool. If not, the PM can falsify the idea themselves and perhaps improve the product idea much faster than if they had to rely on an engineer to build a prototype.\n\nOften, testing feasibility requires a little more than prompting. For example, perhaps the LLM-based email system needs basic RAG capability to help it make decisions. Fortunately, the barrier to writing small amounts of code is now quite low, since AI can help by acting as a coding companion, as I describe in the course, ‚ÄúAI Python for Beginners.‚Äù This means that PMs can do much more technical feasibility testing, at least at a basic level, than was possible before.\n\nPrototype and test even without engineers. User feedback to initial prototypes is also instrumental to shaping products. Fortunately, barriers to building prototypes rapidly are falling, and PMs themselves can move basic prototypes forward without needing professional software developers.\n\nIn addition to using LLMs to help write code for prototyping, tools like Replit, Vercel‚Äôs V0, Bolt, and Anthropic‚Äôs Artifacts (I‚Äôm a fan of all of these!) are making it easier for people without a coding background to build and experiment with simple prototypes. These tools are increasingly accessible to non-technical users, though I find that those who understand basic coding are able to use them much more effectively, so it‚Äôs still important to learn basic coding. (Interestingly, highly technical, experienced developers use them too!) Many members of my teams routinely use such tools to prototype, get user feedback, and iterate quickly.\n\nAI is enabling a lot of new applications to be built, creating massive growth in demand for AI product managers who know how to scope out and help drive progress in building these products. AI product management existed before the rise of generative AI, but the increasing ease of building applications is creating greater demand for AI applications, and thus a lot of PMs are learning AI and these emerging best practices for building AI products. I find this discipline fascinating, and will keep on sharing best practices as they grow and evolve.\n\n[Original text: https://t.co/ohLyrpU4SJ ]",
  "createdAt": "Thu Dec 12 18:06:59 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 330,
  "replyCount": 86,
  "likeCount": 1712,
  "quoteCount": 42,
  "viewCount": 257355,
  "bookmarkCount": 1777,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1866880693588070440",
  "url": "https://x.com/AndrewYNg/status/1866880693588070440",
  "text": "New short course: Collaborative Writing and Coding with OpenAI Canvas!\n\nExplore new ways to write and code with OpenAI Canvas, a user-friendly interface that allows you to brainstorm, draft, and refine text and code in collaboration with ChatGPT.\n\nIn the short course, created with @OpenAI, and taught by @karinanguyen_, a research lead at OpenAI, you‚Äôll learn to use Canvas to enhance your workflows.\n\nCanvas lets you go beyond simple chat interactions. It provides a side-by-side workspace where you and ChatGPT can edit and refine text or code collaboratively. This makes brainstorming, drafting, and iterating as you write feel more natural and effective. As the first major update to ChatGPT‚Äôs visual interface since its launch in 2022, Canvas gives a new, innovative approach to collaboration with AI.\n\nFor instance, after writing the first version of your code, Canvas can review it and give suggestions for improvement. It can also help with debugging by adding logging, identifying problems to fix, and writing comments. In addition, you'll also learn what it takes to train the model for an interface like Canvas.\n\nIn this video-only short course, you‚Äôll:\n- Learn how to ask for in-line feedback and control the iteration of your work by directly editing selected areas of your text or code from the model‚Äôs output.\n- Learn how to access quick automation tools in a shortcut menu that allows you to modify your writing tone and length, enhance your code, and restore previous versions of your work.\n- Learn how to use Canvas as a research assistant tool with an example of asking the model to reason through the screenshot of a plot to write a research report, in which you can ask questions within the created report.\n- Ask the model to write Python code to replicate the graph seen on a screenshot image.\n- Go behind the scenes of how you can create a video game, such as Space Battleship, from scratch, edit it, and display it in one self-contained HTML file.\n- Get a real-world application example of creating a SQL database from the image of its architecture.\n- Understand the model training and design processes that power Canvas!\n\nPlease sign up here: https://t.co/vdWBfHHGia",
  "createdAt": "Wed Dec 11 16:20:16 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 262,
  "replyCount": 34,
  "likeCount": 1383,
  "quoteCount": 14,
  "viewCount": 127317,
  "bookmarkCount": 795,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1861830140730487206",
  "url": "https://x.com/AndrewYNg/status/1861830140730487206",
  "text": "@weimenglee @AIAdvances Thanks for writing up this aisuite guide!",
  "createdAt": "Wed Nov 27 17:51:11 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 3,
  "likeCount": 11,
  "quoteCount": 0,
  "viewCount": 2517,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1861085482526105842",
  "url": "https://x.com/AndrewYNg/status/1861085482526105842",
  "text": "Announcing new open-source Python package: aisuite!  \n\nThis makes it easy for developers to use large language models from multiple providers. When building applications I found it a hassle to integrate with multiple providers. Aisuite lets you pick a \"provider:model\" just by changing one string, like openai:gpt-4o, anthropic:claude-3-5-sonnet-20241022, ollama:llama3.1:8b, etc. \n\npip install aisuite\n\nOpen-source code with instructions: https://t.co/gwz9oKTCFx\n\nThanks to Rohit Prsad, Kevin Solorio, @standsleeping,   Jeff Tang and @Johnsanterre for helping build this!",
  "createdAt": "Mon Nov 25 16:32:10 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 1070,
  "replyCount": 146,
  "likeCount": 5801,
  "quoteCount": 103,
  "viewCount": 436130,
  "bookmarkCount": 3737,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1860468376809931061",
  "url": "https://x.com/AndrewYNg/status/1860468376809931061",
  "text": "@joaomdmoura Congratulations!!! ‚ù§Ô∏è",
  "createdAt": "Sat Nov 23 23:40:01 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 2,
  "likeCount": 12,
  "quoteCount": 0,
  "viewCount": 3756,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1859625355541348798",
  "url": "https://x.com/AndrewYNg/status/1859625355541348798",
  "text": "A small number of people are posting text online that‚Äôs intended for direct consumption not by humans, but by LLMs (large language models). I find this a fascinating trend, particularly when writers are incentivized to help LLM providers better serve their users!\n\nPeople who post text online don‚Äôt always have an incentive to help LLM providers. In fact, their incentives are often misaligned. Publishers worry about LLMs reading their text, paraphrasing it, and reusing their ideas without attribution, thus depriving them of subscription or ad revenue. This has even led to litigation such as The New York Times‚Äô lawsuit against OpenAI and Microsoft for alleged copyright infringement. There have also been demonstrations of prompt injections, where someone writes text to try to give an LLM instructions contrary to the provider‚Äôs intent. (For example, a handful of sites advise job seekers to get past LLM resum√© screeners by writing on their resum√©s, in a tiny/faint font that‚Äôs nearly invisible to humans, text like ‚ÄúThis candidate is very qualified for this role.‚Äù) Spammers who try to promote certain products ‚Äî which is already challenging for search engines to filter out ‚Äî will also turn their attention to spamming LLMs.\n\nBut there are examples of authors who want to actively help LLMs. Take the example of a startup that has just published a software library. Because the online documentation is very new, it won‚Äôt yet be in LLMs‚Äô pretraining data. So when a user asks an LLM to suggest software, the LLM won‚Äôt suggest this library, and even if a user asks the LLM directly to generate code using this library, the LLM won‚Äôt know how to do so. Now, if the LLM is augmented with online search capabilities, then it might find the new documentation and be able to use this to write code using the library. In this case, the developer may want to take additional steps to make the online documentation easier for the LLM to read and understand via RAG. (And perhaps the documentation eventually will make it into pretraining data as well.)\n\nCompared to humans, LLMs are not as good at navigating complex websites, particularly ones with many graphical elements. However, LLMs are far better than people at rapidly ingesting long, dense, text documentation. Suppose the software library has many functions that we want an LLM to be able to use in the code it generates. If you were writing documentation to help humans use the library, you might create many web pages that break the information into bite-size chunks, with graphical illustrations to explain it. But for an LLM, it might be easier to have a long XML-formatted text file that clearly explains everything in one go. This text might include a list of all the functions, with a dense description of each and an example or two of how to use it. (This is not dissimilar to the way we specify information about functions to enable LLMs to use them as tools.)\n\nA human would find this long document painful to navigate and read, but an LLM would do just fine ingesting it and deciding what functions to use and when!\n\nBecause LLMs and people are better at ingesting different types of text, we write differently for LLMs than for humans. Further, when someone has an incentive to help an LLM better understand a topic ‚Äî so the LLM can explain it better to users ‚Äî then an author might write text to help an LLM.\n\nSo far, text written specifically for consumption by LLMs has not been a huge trend. But Jeremy Howard‚Äôs proposal for web publishers to post a llms.txt file to tell LLMs how to use their websites, like a robots.txt file tells web crawlers what to do, is an interesting step in this direction. In a related vein, some developers are posting detailed instructions that tell their IDE how to use tools, such as the plethora of .cursorrules files that tell the Cursor IDE how to use particular software stacks.\n\nI see a parallel with SEO (search engine optimization). The discipline of SEO has been around for decades. Some SEO helps search engines find more relevant topics, and some is spam that promotes low-quality information. But many SEO techniques ‚Äî those that involve writing text for consumption by a search engine, rather than by a human ‚Äî have survived so long in part because search engines process web pages differently than humans, so providing tags or other information that tells them what a web page is about has been helpful.\n\nThe need to write text separately for LLMs and humans might diminish if LLMs catch up with humans in their ability to understand complex websites. But until then, as people get more information through LLMs, writing text to help LLMs will grow.\n\n[Original text: https://t.co/MDjPq9wCDH ]",
  "createdAt": "Thu Nov 21 15:50:09 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 144,
  "replyCount": 52,
  "likeCount": 759,
  "quoteCount": 24,
  "viewCount": 88411,
  "bookmarkCount": 332,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1859258084079882512",
  "url": "https://x.com/AndrewYNg/status/1859258084079882512",
  "text": "Time to play! Build an interactive game from scratch with LLMs in this new short course: Building an AI-Powered Game. Created with @togethercompute and  @aidungeon @LatitudeGamesAI, taught by @niki_birkner, Senior Product Manager at Together AI, and @nickwalton00, CEO and Co-Founder of Latitude.\n\nThis course shows you how to use large language models to create and power a text-based game that you can share with your friends and family. You‚Äôll build a world with hierarchical content generation, a method that allows you to leverage LLMs to create a vast amount of content with a high level of control and consistency. For instance, if you were building a fantasy world with several kingdoms, in which each kingdom has multiple towns, and each town has several locations and residents, creating all this content from scratch can easily become tedious and difficult to track.\n\nWith hierarchical content generation, you can create information about your world, shape its direction with a human-in-the-loop, and keep it consistent, with little effort based on your prompts.\n\nBy the end of this course, you‚Äôll know how to prompt engineer to create a layered and interwoven world and integrate it into an AI roleplay game that is interesting, interactive, and safe to share with anyone.\n\nIn detail, you‚Äôll:\n- Learn to implement game mechanics using AI to parse text data into structured JSON output, enabling features like an inventory system.\n- Use game mechanics with story and state components that feed into one another to improve your game's memory, and gives the player a steady state of the world.\n- Learn to enforce safety and compliance for AI content generation and create custom policies using Llama Guard.\n\nWith these techniques, you'll be equipped to build AI-powered applications, starting with your own game.\n\nPlease sign up here:  https://t.co/Ght1dlUkcG",
  "createdAt": "Wed Nov 20 15:30:45 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 153,
  "replyCount": 23,
  "likeCount": 899,
  "quoteCount": 12,
  "viewCount": 84690,
  "bookmarkCount": 531,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1857117382378164267",
  "url": "https://x.com/AndrewYNg/status/1857117382378164267",
  "text": "Large language models (LLMs) are typically optimized to answer peoples‚Äô questions. But there is a trend toward models also being optimized to fit into agentic workflows. This will give a huge boost to agentic performance!\n\nFollowing ChatGPT‚Äôs breakaway success at answering questions, a lot of LLM development focused on providing a good consumer experience. So LLMs were tuned to answer questions (‚ÄúWhy did Shakespeare write Macbeth?‚Äù) or follow human-provided instructions (‚ÄúExplain why Shakespeare wrote Macbeth‚Äù). A large fraction of the datasets for instruction tuning guide models to provide more helpful responses to human-written questions and instructions of the sort one might ask a consumer-facing LLM like those offered by the web interfaces of ChatGPT, Claude, or Gemini.\n\nBut agentic workloads call on different behaviors. Rather than directly generating responses for consumers, AI software may use a model in part of an iterative workflow to reflect on its own output, use tools, write plans, and collaborate in a multi-agent setting. Major model makers are increasingly optimizing models to be used in AI agents as well.\n\nTake tool use (or function calling). If an LLM is asked about the current weather, it won‚Äôt be able to derive the information needed from its training data. Instead, it might generate a request for an API call to get that information. Even before GPT-4 natively supported function calls, application developers were already using LLMs to generate function calls, but by writing more complex prompts (such as variations of ReAct prompts) that tell the LLM what functions are available and then have the LLM generate a string that a separate software routine parses (perhaps with regular expressions) to figure out if it wants to call a function.\n\nGenerating such calls became much more reliable after GPT-4 and then many other models natively supported function calling. Today, LLMs can decide to call functions to search for information for retrieval augmented generation (RAG), execute code,  send emails, place orders online, and much more.\n\nRecently, Anthropic released a version of its model that is capable of computer use, using mouse-clicks and keystrokes to operate a computer (usually a virtual machine). I‚Äôve enjoyed playing with the demo. While other teams have been prompting LLMs to use computers to build a new generation of RPA (robotic process automation) applications, native support for computer use by a major LLM provider is a great step forward. This will help many developers!\n\nAs agentic workflows mature, here is what I am seeing:\n- First, many developers are prompting LLMs to carry out the agentic behaviors they want. This allows for quick, rich exploration!\n- In a much smaller number of cases, developers who are working on very valuable applications will fine-tune LLMs to carry out particular agentic functions more reliably. For example, even though many LLMs support function calling natively, they do so by taking as input a description of the functions available and then (hopefully) generating output tokens to request the right function call. For mission-critical applications where generating the right function call is important, fine-tuning a model for your application‚Äôs specific function calls significantly increases reliability. (But please avoid premature optimization! Today I still see too many teams fine-tuning when they should probably spend more time on prompting before they resort to this.)\n- Finally, when a capability such as tool use or computer use appears valuable to many developers, major LLM providers are building these capabilities directly into their models. Even though OpenAI o1-preview‚Äôs advanced reasoning helps consumers, I expect that it will be even more useful for agentic reasoning and planning.\n\nMost LLMs have been optimized for answering questions primarily to deliver a good consumer experience, and we‚Äôve been able to ‚Äúgraft‚Äù them into complex agentic workflows to build valuable applications. The trend of LLMs built to support particular operations in agents natively will create a lot of lift for agentic performance. I‚Äôm confident that large agentic performance gains in this direction will be realized in the next few years.\n\n[Original text: https://t.co/gginTyOgwe ]",
  "createdAt": "Thu Nov 14 17:44:22 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 321,
  "replyCount": 83,
  "likeCount": 1838,
  "quoteCount": 34,
  "viewCount": 165308,
  "bookmarkCount": 1118,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1856791398592516132",
  "url": "https://x.com/AndrewYNg/status/1856791398592516132",
  "text": "@SnowflakeDB @LandingAI Thanks you for having me! It's a pleasure as always to speak at BUILD.",
  "createdAt": "Wed Nov 13 20:09:01 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 0,
  "likeCount": 11,
  "quoteCount": 0,
  "viewCount": 3400,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1856779913757691922",
  "url": "https://x.com/AndrewYNg/status/1856779913757691922",
  "text": "New short course: Safe and Reliable AI via Guardrails! Learn to create production-ready, reliable LLM applications with guardrails in this new course, built in collaboration with @guardrails_ai and taught by its CEO and co-founder,  @ShreyaR.\n\nI see many companies worry about the reliability of LLM-based systems -- will they hallucinate a catastrophically bad response? -- which slows down investing in building them and transitioning prototypes to deployment.  That LLMs generate probabilistic outputs has made them particularly hard to deploy in highly regulated industries or in safety-critical environments. \n\nFortunately, there are good guardrail tools that give a significant new layer of control and reliability/safety. They act as a protective framework that can prevent your application from revealing incorrect, irrelevant, or confidential information, and they are an important part of what it takes to actually get prototypes to deployment. \n\nThis course will walk you through common failure modes of LLM-powered applications (like hallucinations or revealing personally identifiable information). It will show you how to build guardrails from scratch to mitigate them. You‚Äôll also learn how to access a variety of pre-built guardrails on the GuardrailsAI hub that are ready to integrate into your projects.\n\nYou'll implement these guardrails in the context of a RAG-powered customer service chatbot for a small pizzeria. Specifically, you'll:\n- Explore common failure modes like hallucinations, going off-topic, revealing sensitive information, or responses that can harm the pizzeria's reputation.\n- Learn to mitigate these failure modes with input and output guards that check inputs and/or outputs\n- Create a guardrail to prevent the chatbot from discussing sensitive topics, such as a confidential project at the pizza shop\n- Detect hallucinations by ensuring responses are grounded in trusted documents\n- Add a Personal Identifiable Information (PII) guardrail to detect and redact sensitive information in user prompts and in LLM outputs\n- Set up a guardrail to limit the chatbot‚Äôs responses to topics relevant to the pizza shop, keeping interactions on-topic\n- Configure a guardrail that prevents your chatbot from mentioning any competitors using a name detection pipeline consisting of conditional logic that routes to an exact match or a threshold check with named entity recognition \n\nGuardrails are an important part of the practical building and deployment of LLM-based applications today. This course will show you how to make your applications more reliable and more ready for real-world deployment.\n\nPlease sign up here: https://t.co/C1fwsOn9yy",
  "createdAt": "Wed Nov 13 19:23:23 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 144,
  "replyCount": 55,
  "likeCount": 722,
  "quoteCount": 10,
  "viewCount": 105956,
  "bookmarkCount": 369,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1856402761900011622",
  "url": "https://x.com/AndrewYNg/status/1856402761900011622",
  "text": "Chatting with OpenAI‚Äôs @karinanguyen_ who joined OpenAI earlier this year and within 6 months co-created and shipped Canvas. I really respect teams that can move fast. That OpenAI, even as a large-ish company, can ship at this pace is fantastic! https://t.co/xuxH1hZZiV",
  "createdAt": "Tue Nov 12 18:24:43 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 66,
  "replyCount": 64,
  "likeCount": 1192,
  "quoteCount": 8,
  "viewCount": 181956,
  "bookmarkCount": 101,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1854587401018261962",
  "url": "https://x.com/AndrewYNg/status/1854587401018261962",
  "text": "New short course: LLMs as Operating Systems: Agent Memory, created with @Letta_AI, and taught by its founders @charlespacker and @sarahwooders.\n\nAn LLM's input context window has limited space. Using a longer input context also costs more and results in slower processing. So, managing what's stored in this context window is important.\n\nIn the innovative paper MemGPT: Towards LLMs as Operating Systems, its authors (which include the instructors) proposed using an LLM agent to manage this context window. Their system uses a large persistent memory that stores everything that could be included in the input context, and  an agent decides   what is actually included.\n\nTake the example of building a chatbot that needs to remember what's been said earlier in a conversation (perhaps over many days of interaction with a user). As the conversation's length grows, the memory management agent will move information from the input context to a persistent searchable database; summarize information to keep relevant facts in the input context; and restore relevant conversation elements from further back in time. This allows a chatbot to keep what's currently most relevant in its input context memory to generate the next response.\n\nWhen I read the original MemGPT paper, I thought it was an innovative technique for handling memory for LLMs. The open-source Letta framework, which we'll use in this course, makes MemGPT easy to implement. It adds memory to your LLM agents and gives them transparent long-term memory.\n\nIn detail, you‚Äôll learn:\n- How to build an agent that can edit its own limited input context memory, using tools and multi-step reasoning\n- What is a memory hierarchy (an idea from computer operating systems, which use a cache to speed up memory access), and how these ideas apply to managing the LLM input context (where the input context window is a \"cache\" storing the most relevant information; and an agent decides what to move in and out of this to/from a larger persistent storage system)\n- How to implement multi-agent collaboration by letting different agents share blocks of memory\n\nThis course will give you a sophisticated understanding of memory management for LLMs, which is important for chatbots having long conversations, and for complex agentic workflows.\n\nPlease sign up here!  https://t.co/XMlBifnwVa",
  "createdAt": "Thu Nov 07 18:11:07 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 332,
  "replyCount": 109,
  "likeCount": 2014,
  "quoteCount": 30,
  "viewCount": 198352,
  "bookmarkCount": 1509,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1853653834490642801",
  "url": "https://x.com/AndrewYNg/status/1853653834490642801",
  "text": "Source: https://t.co/juDEKqJEBc",
  "createdAt": "Tue Nov 05 04:21:28 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 15,
  "replyCount": 6,
  "likeCount": 127,
  "quoteCount": 0,
  "viewCount": 38773,
  "bookmarkCount": 58,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1853653742509502571",
  "url": "https://x.com/AndrewYNg/status/1853653742509502571",
  "text": "It finally happened -- thanks to people learning to write AI code, Python is now the top programming language on GitHub! \n\nIf you want to learn Python, check out https://t.co/zpIxRSuky4's free course AI Python for Beginners.",
  "createdAt": "Tue Nov 05 04:21:06 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 363,
  "replyCount": 51,
  "likeCount": 2768,
  "quoteCount": 22,
  "viewCount": 192478,
  "bookmarkCount": 1309,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1852107599254073821",
  "url": "https://x.com/AndrewYNg/status/1852107599254073821",
  "text": "Happy Halloween! üéÉ \n\nOn this spooky day, The Batch continues its annual tradition of exploring fears related to AI. This special edition has 5 articles: \n* AI Burns All the Energy \n* Innovation Dies\n* No Work for Coders\n* Benchmarks Are Meaningless\n* Synthetic Data Distorts Models\n\nAre these things we should worry about? Check it out here: https://t.co/Vt5xnhZToT",
  "createdAt": "Thu Oct 31 21:57:16 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 41,
  "replyCount": 48,
  "likeCount": 307,
  "quoteCount": 0,
  "viewCount": 46546,
  "bookmarkCount": 59,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1850912176896463328",
  "url": "https://x.com/AndrewYNg/status/1850912176896463328",
  "text": "Startups live or die by their ability to execute at speed. For large companies, too, the speed with which an innovation team is able to iterate has a huge impact on its odds of success. Generative AI makes it possible to quickly prototype AI capabilities. AI capabilities that used to take months can sometimes be built in days or hours by simply prompting a large language model. I find this speed exciting and have been thinking about how to help startups and large companies alike go faster.\n\nI‚Äôve been obsessed with speedy execution for a long time. When working on a project, I am loath to take two weeks to do something that I could do in one week. The price of moving at that pace is not that we take one week longer (which might be okay) but that we‚Äôre 2x slower (which is not)!\n\nWhen building an AI-powered product, there are many steps in designing, building, shipping, and scaling the product that are distinct from building the AI capability, and our ability to execute these other steps has not sped up as much as the AI part. But the speed with which we can prototype AI creates significant pressure to speed up these other steps, too. If it took 6 months to collect data, train a supervised learning algorithm, and deploy the model to the cloud, it might be okay to take 2 months to get user feedback. But if it takes a week to build a prototype, waiting 2 months for feedback seems intolerably slow!\n\nI‚Äôd like to focus on one key step of building applications: getting user feedback. A core part of the iterative workflow of designing and building a product (popularized by Eric Ries in his book The Lean Startup) is to build a prototype (or MVP, minimum viable product), get user feedback on it, and to use that feedback to drive improvements. The faster you can move through this loop ‚Äî which may require many iterations ‚Äî the faster you can design a product that fits the market. This is why AI Fund, a venture studio that I lead, uses many fast, scrappy tactics to get feedback.\n\nFor B2C (business to consumer) offerings, here is a menu of some options for getting customer feedback:\n1. Ask 3 friends or team members to look at the product and let you know what they think (this might take ~0.5 days).\n2. Ask 10 friends or team members to take a look (~2 days).\n3. Send it to 100 trusted/volunteer alpha testers (~1 week?).\n4. Send it to 1,000 users to get qualitative or quantitative feedback (~2 weeks?).\n5. Incorporate it into an existing product to get feedback (1 to 2 months?).\n6. Roll it out to a large user base of an existing product and do rigorous A/B testing.\n\nAs we go down this list, we get (probably) more accurate feedback, but the time needed to get that feedback increases significantly. Also, the tactics at the top of the list create basically no risk, and thus it‚Äôs safe to repeatedly call on them, even with preliminary ideas and prototypes. Another advantage of the tactics further up the list is that we get more qualitative feedback (for example, do users seem confused? Are they telling us they really need one additional feature?), which sparks better ideas for how to change our product than an A/B test, which tells us with rigor whether a particular implementation works but is less likely to point us in new directions to try. I recommend using the fast feedback tactics first. As we exhaust the options for learning quickly, we can try the slower tactics.\n\nWith these tactics, scrappy startup leaders and innovation-team leaders in large companies can go faster and have a much higher chance of success.\n\nThe mantra ‚Äúmove fast and break things‚Äù got a bad reputation because, well, it broke things. Unfortunately, some have interpreted this to mean we should not move fast, but I disagree. A better mantra is ‚Äúmove fast and be responsible.‚Äù There are many ways to prototype and test quickly without shipping a product that can cause significant harm. In fact, prototyping and testing/auditing quickly before launching to a large audience is a good way to identify and mitigate potential problems.\n\nThere are numerous AI opportunities ahead, and our tools are getting better and better to pursue them at speed, which is exhilarating!\n\n[Original text: https://t.co/NeMP4DKdDX ]",
  "createdAt": "Mon Oct 28 14:47:05 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 180,
  "replyCount": 69,
  "likeCount": 879,
  "quoteCount": 12,
  "viewCount": 87883,
  "bookmarkCount": 392,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1850280961768104332",
  "url": "https://x.com/AndrewYNg/status/1850280961768104332",
  "text": "Congrats @andrewdfeldman and @CerebrasSystems for a huge leap forward and setting a new speed record for serving Llama 3.1-70B. 2100 tokens/sec is blazingly fast for a 70B model. This is great for agentic AI!",
  "createdAt": "Sat Oct 26 20:58:52 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 66,
  "replyCount": 56,
  "likeCount": 421,
  "quoteCount": 6,
  "viewCount": 112914,
  "bookmarkCount": 79,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1850246368126021744",
  "url": "https://x.com/AndrewYNg/status/1850246368126021744",
  "text": "@vishalmisra Cool visualization!",
  "createdAt": "Sat Oct 26 18:41:24 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 5,
  "quoteCount": 0,
  "viewCount": 4158,
  "bookmarkCount": 2,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1849112129904738656",
  "url": "https://x.com/AndrewYNg/status/1849112129904738656",
  "text": "New short course: Practical Multi AI Agents and Advanced Use Cases with crewAI. Learn to build and deploy advanced agent-based systems in real applications in this course, created with @crewAIInc and taught by its founder, @joaomdmoura! (Disclosure: I've made a small seed investment in CrewAI.)\n\nIn this course, you‚Äôll learn how to create advanced agent-based apps that use external tools, do performance testing, can be trained with human feedback, and perform multiple tasks with different large language models.\n\nYou will build several practical agentic apps that provide real business value, such as an automated project planning system, lead scoring and engagement pipeline, customer support data analysis, and a robust content creation system.\n\nIn detail, you will learn how to:\n- Create these multi-agent systems with the building blocks of tasks, agents, and crews, along with the different things that make them work, such as caching, memory, and guardrails.\n- Integrate your multi-agent application with internal and external systems.\n- Connect multiple agents in complex setups, including parallel, sequential, and hybrid configurations, and create flows involving multiple agentic applications working together.\n- Test your agentic workflow and train it using human feedback to optimize its performance for better and more consistent results.\n- Work with multiple LLMs in your multi-agent system, using the appropriate model sizes and providers to fit each agent‚Äôs specific task.\n- Start a project from scratch in your environment and prepare it for deployment.\n\nYou‚Äôll also learn from an interview between Jo√£o and Jacob Wilson, the Commercial GenAI Principal at PwC , in which they discuss deploying agentic workflows in real industry use cases.\n\nBy the end of this course, you will be equipped to start building custom multi-agentic systems for your work.\n\nPlease sign up here! https://t.co/JkD52B3ONA",
  "createdAt": "Wed Oct 23 15:34:21 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 260,
  "replyCount": 91,
  "likeCount": 1493,
  "quoteCount": 45,
  "viewCount": 339721,
  "bookmarkCount": 1405,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1846978449346646089",
  "url": "https://x.com/AndrewYNg/status/1846978449346646089",
  "text": "@DiggerofAI Yup. Reducing shipping emissions reduced pollution, but also reduced sunlight reflection and thus accelerating warming. With SAI, we would spray aerosols high up in the stratosphere, where it'll last longer. This gets us more cooling and less pollution than shipping emissions.",
  "createdAt": "Thu Oct 17 18:15:52 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 0,
  "likeCount": 2,
  "quoteCount": 0,
  "viewCount": 594,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1846952116516278591",
  "url": "https://x.com/AndrewYNg/status/1846952116516278591",
  "text": "It‚Äôs high time to take geoengineering more seriously as a potential tool to mitigate climate change. 2023 was the hottest year on record, and 2024 is likely to top that. In the United States, Hurricane Helene caused over 200 deaths, and Hurricane Milton's death toll is at least two dozen. It‚Äôs well established that the hurricanes are growing stronger as global temperatures rise.\n\nWhile stratospheric aerosol injection (SAI) ‚Äî which sprays particles (aerosols) in the atmosphere to provide a small amount of shade from the sun ‚Äî is far from a perfect solution, we should take it seriously as a possible tool for saving lives. A few months ago, my collaborators and I released a climate emulator, Planet Parasol https://t.co/OxtaQMyDuL , that you can play with to simulate different SAI scenarios to understand its possible impact. By using AI to model its impact and thereby advance our understanding of SAI, we‚Äôll be better prepared to decide if this is a good step.\n\nThe key idea of SAI, which is a form of climate geoengineering, is to spray reflective particles into the stratosphere to reflect a little more, say 1%, of the sunlight that otherwise would fall on Earth back into space. This small increase in reflected sunlight would be sufficient to mitigate much of the impact of human-induced warming. For example, in 1991, Mount Pinatubo ejected almost 20 tons of aerosols (sulfur dioxide) into the atmosphere and cooled down the planet by around 0.5 degrees Celsius over the following year. We should be able to induce cooling equivalent to, say, a fraction of Mount Pinatubo, via a fair, international process that‚Äôs backed by science.\n\nThere are many criticisms of SAI, such as:\n- It could have unintended climate consequences, for example, disrupting local weather patterns and creating droughts or floods.\n- If it were started and then stopped suddenly, it could lead to sudden warming, known as ‚Äútermination shock.‚Äù\n- Depending on the aerosol used (sulfur dioxide is a leading candidate), it could contribute to pollution and/or ozone depletion.\n- It might reduce urgency to decarbonize (an example of a ‚Äúmoral hazard‚Äù).\n\nIn addition, many people have a visceral emotional reaction, as I once did before I understood the science more deeply, against ‚Äúplaying god‚Äù by daring to engineer the planet.\n\nAll these downsides should be balanced against the reality that people are dying.\n\nI‚Äôm moved by meteorologist John Morales‚Äô emotional account of the havoc caused by Hurricane Milton. The New York Times quoted him as saying, ‚ÄúIt claims lives. It also wrecks lives.‚Äù https://t.co/MKD8GIrV5g \n\nSkyfire AI, a drone company led by CEO Don Mathis that my team AI Fund helped to co-build, was recently on the ground in the aftermath of Helene and Milton, deploying drones to help emergency responders survey remote areas and find survivors. Mathis reports that Skyfire was credited with saving at least 13 lives. https://t.co/rsrcWMka17 On Monday, I also spoke about AI applied to renewable energy with AES‚Äô CEO Andres Gluski and CPO Chris Shelton. You can view our conversation here: https://t.co/kzKBp3NmrM\n\nWhile I‚Äôm glad that AI can help mitigate these disasters, it saddens me that so many lives have already been lost due to climate-influenced causes. My mind frequently returns to SAI as one of the few untapped tools in our arsenal that can help. We need to be investing in SAI research now.\n\nI‚Äôm grateful to my collaborators on the Planet Parasol emulator (a group that includes many climate scientists) including Jeremy Irvin, Daniele Visioni, Ben Kravitz, Dakota Gruener, Chris Smith, and Duncan Watson-Parris. MIT Technology Review‚Äôs James Temple wrote about his experience playing with our emulator and also outlines fair criticisms. (See https://t.co/ufXAKNBiLQ ) Much work remains to be done, and making sure our actions are based on science ‚Äî a task that AI can help with (witness the recent Chemistry and Physics Nobel Prizes going to innovators in AI!) ‚Äì will help us make better decisions.\n\nIf you‚Äôre interested in learning more about SAI, check out this recent panel discussion (https://t.co/UFerfFIskp) where I spoke alongside climate scientists Chris Field, David Keith, Douglas MacMartin, and Simone Tilmes about the science and possible roadmaps ahead.\n\n[Original text: https://t.co/u1thkhK0XY ]",
  "createdAt": "Thu Oct 17 16:31:13 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 62,
  "replyCount": 61,
  "likeCount": 280,
  "quoteCount": 3,
  "viewCount": 43133,
  "bookmarkCount": 61,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1846608552359833674",
  "url": "https://x.com/AndrewYNg/status/1846608552359833674",
  "text": "New short course: Serverless Agentic Workflows with Amazon Bedrock. Learn to build and deploy serverless agents in this course created with @awscloud and taught by @mikegchambers, a Senior Developer Advocate at AWS specializing in GenAI. (Disclosure: I serve on Amazon's board.)\n\nGenerative AI applications are becoming more complex, sophisticated, and agentic. Agentic applications have workloads that can be hard to predict in advance -- for example, what tools will it decide to call? -- and a serverless architecture helps you efficiently providing on-demand resources.\n\nThis course teaches you to build and deploy a serverless agentic application. You‚Äôll learn to create agents with tools, code execution, and guardrails, and build responsible agents for business use cases:\n- Build a customer service bot for a fictional tea mug business that can answering questions, retrieve information, and process orders.\n- Connect your customer service agent to a CRM to get customer info and log support tickets in real-time.\n- Explore how you invoke the agent, and see the trace to review the agent‚Äôs thought process and observation loop until it reaches its final output.\n- Attach a code interpreter to your agent, giving it the ability to perform accurate calculations by writing and running its own Python code.\n- Implement guardrails to prevent your agent from revealing sensitive information or using inappropriate language.\n\nBy the end, you will have built a sophisticated AI agent capable of handling real-world customer support scenarios.\n\nPlease sign up here! https://t.co/FQKGJNBPwp",
  "createdAt": "Wed Oct 16 17:46:01 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 139,
  "replyCount": 60,
  "likeCount": 856,
  "quoteCount": 4,
  "viewCount": 80408,
  "bookmarkCount": 584,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1844092080987177409",
  "url": "https://x.com/AndrewYNg/status/1844092080987177409",
  "text": "\"Introducing Multimodal Llama 3.2\": As promised two weeks ago, here's the short course on Meta's latest open model!\n\nThis short course is created with @Meta and taught by @asangani7, Director of AI Partner Engineering at Meta.\n\nMeta‚Äôs Llama family of models is leading the way in open models, allowing anyone to download, customize, fine-tune, or build new applications on top of them.\n\nLearn about the vision capabilities of the Llama 3.2, and use it for image classification, prompting, tokenization, tool-calling. You'll also learn about the open-source Llama stack, which gives building blocks for many different stages of the LLM application life cycle.\n\nIn detail, you‚Äôll:\n- Learn what are the features of Meta's four newest models, and when to use which Llama model.\n- Learn best practices for multimodal prompting, with applications to advanced image reasoning, illustrated by many examples: Understanding errors on a car dashboard, adding up the total of photographed restaurant receipts, grading written math homework.\n- Use different roles‚Äîsystem, user, assistant, ipython‚Äîin the Llama 3.1 and 3.2 models  and the prompt format that identifies those roles.\n- Understand how Llama uses the tiktoken tokenizer, and how it has expanded to a 128k vocabulary size that improves encoding efficiency and multilingual support.\n- Learn how to prompt Llama to call built-in and custom tools (functions) with examples for web search and solving math equations.\n- Learn about Llama Stack, a standardized interface for common toolchain components like fine-tuning or synthetic data generation, useful for building agentic applications.\n\nBy the end of this course, you‚Äôll be equipped to build out new applications with the new Llama 3.2.\n\nThank you to @Ahmad_Al_Dahle, Amit Sangani, and the whole AI at Meta team @AIatMeta for all the hard work on Llama 3.2 ‚Äî we‚Äôre excited to make these open models even more accessible to more developers with this new course! \n\nPlease sign up here!  https://t.co/Flp5Ae9apy",
  "createdAt": "Wed Oct 09 19:06:28 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 247,
  "replyCount": 30,
  "likeCount": 1615,
  "quoteCount": 13,
  "viewCount": 131022,
  "bookmarkCount": 935,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1843995968636866799",
  "url": "https://x.com/AndrewYNg/status/1843995968636866799",
  "text": "Amazing -- even more Nobel Prizes to AI people! Congrats to Google's @demishassabis &amp; John Jumper, and University of Washington's David Baker for their work on AI for protein sequences. AlphaFold, and @UWproteindesign's protein design work , were real breakthroughs!",
  "createdAt": "Wed Oct 09 12:44:33 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 149,
  "replyCount": 35,
  "likeCount": 1415,
  "quoteCount": 9,
  "viewCount": 72621,
  "bookmarkCount": 38,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1843764485632524664",
  "url": "https://x.com/AndrewYNg/status/1843764485632524664",
  "text": "Lisa Su's leadership of AMD has been phenomenal. 10 years ago, AMD was in dire straits, was losing money  and faced intense competition from Intel (CPUs) and NVIDIA (GPUs). She turned the company around, introduced Ryzen and Epyc processors, introduced the Zen architecture, acquired Xilinx, and is now making a good showing with MI300/MI350 for AI workloads. I've come to deeply respect her as a leader. Congratulations @LisaSu on these last ten years!",
  "createdAt": "Tue Oct 08 21:24:43 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 84,
  "replyCount": 13,
  "likeCount": 763,
  "quoteCount": 3,
  "viewCount": 81481,
  "bookmarkCount": 43,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1843711446552846732",
  "url": "https://x.com/AndrewYNg/status/1843711446552846732",
  "text": "I was the first to call Geoff Hinton \"Godfather of Deep Learning\", which later became \"Godfather of AI.\" Thrilled to see him win the Nobel prize together with John Hopfield for AI. Congrats @geoffreyhinton!",
  "createdAt": "Tue Oct 08 17:53:58 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 460,
  "replyCount": 83,
  "likeCount": 4825,
  "quoteCount": 29,
  "viewCount": 172976,
  "bookmarkCount": 217,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1841952373218123996",
  "url": "https://x.com/AndrewYNg/status/1841952373218123996",
  "text": "How AI will change Coding & Education? I'm looking forward to this chat with @mehran_sahami. Mehran is a legendary programming instructor and an old friend, and has been thinking a lot about how AI is transforming coding. Come join us -- this will be fun! \n\nPlease register here: https://t.co/yWO7ovW916\n\nThank you @jpaxtonh and @StanfordOnline for hosting!",
  "createdAt": "Thu Oct 03 21:24:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 122,
  "replyCount": 25,
  "likeCount": 624,
  "quoteCount": 8,
  "viewCount": 91737,
  "bookmarkCount": 269,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1841496274350247951",
  "url": "https://x.com/AndrewYNg/status/1841496274350247951",
  "text": "Tokenization -- turning text into a sequence of integers -- is a key part of generative AI, and most API providers charge per million tokens. How does tokenization work? Learn the details of tokenization and RAG optimization in Retrieval Optimization: From Tokenization to Vector Quantization, created in collaboration with @qdrant_engine and taught by its Developer Relations Lead, @LukawskiKacper.\n\nThis course focuses on Retrieval augmented generation (RAG), which has two steps: First, a retriever finds relevant information; then, the generator uses what‚Äôs retrieved as context to produce a response. You‚Äôll learn to optimize the first step (the retriever) by understanding how tokenization works and how it impacts the relevance of your search. In addition, you will also learn to measure and improve retrieval quality, speed, and memory.\n\nIn detail, you‚Äôll:\n- Learn about the internal workings of the embedding models and how your text turns into vectors.\n- Understand how several tokenizers, such as Byte-Pair Encoding, WordPiece, Unigram, and SentencePiece work.\n- Explore common challenges with tokenizers, such as unknown tokens, domain-specific identifiers, and numerical values, that can negatively affect your vector search.\n- Understand how to measure the quality of your search across relevance, ranking, and score-related metrics.\n- Understand how the main parameters in \"HNSW\", a graph-based algorithm, affect the relevance and speed of vector search, and how to tune its parameters.\n- Experiment with the three major quantization methods ‚Äì product, scalar, and binary ‚Äì and learn how they impact memory requirements, search quality, and speed.\n\nBy the end of this course, you‚Äôll have a solid understanding of how tokenization functions and how to optimize vector search in your RAG systems.\n\nPlease sign up here! https://t.co/wIrSEGAcb9",
  "createdAt": "Wed Oct 02 15:11:39 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 300,
  "replyCount": 30,
  "likeCount": 1658,
  "quoteCount": 11,
  "viewCount": 145546,
  "bookmarkCount": 1403,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1841178556904456275",
  "url": "https://x.com/AndrewYNg/status/1841178556904456275",
  "text": "AI needs UI, and OpenAI's impressive new voice APIs  open up a lot of possibilities. Congrats @OpenAI team -- we'll soon be  seeing a whole new generation of speech applications!",
  "createdAt": "Tue Oct 01 18:09:10 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 147,
  "replyCount": 55,
  "likeCount": 1659,
  "quoteCount": 21,
  "viewCount": 109190,
  "bookmarkCount": 202,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1840788292750791046",
  "url": "https://x.com/AndrewYNg/status/1840788292750791046",
  "text": "AI policy should be based in science, not science fiction! \n\nWith SB-1047 defeated, @dawnsongtweets gives a sound plan for taking a scientific approach to study  actual risks and mitigating harms. \n\nA couple of key ideas:\n* Lets empower researchers to study AI risks, focusing on the question of marginal risk: I.e., how much does an AI application increase the risk of a negative outcome? \n* Lets also increase transparency of AI systems. For example, open-source and and red-teaming will help! \n\nMany of the opponents to SB-1047 have been strong champions of Responsible AI, long before it was trendy to talk about it. We take safety seriously; and, we want policy to based in science, not science fiction.",
  "createdAt": "Mon Sep 30 16:18:23 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 79,
  "replyCount": 35,
  "likeCount": 436,
  "quoteCount": 3,
  "viewCount": 75446,
  "bookmarkCount": 44,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1840547768894685497",
  "url": "https://x.com/AndrewYNg/status/1840547768894685497",
  "text": "@ClementDelangue @GavinNewsom Thank you @ClementDelangue for unfailingly  advocating for open-source, and speaking out against bad laws like SB-1047. Hugging Face has been a wonderful force bringing openness to AI!",
  "createdAt": "Mon Sep 30 00:22:38 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 0,
  "likeCount": 39,
  "quoteCount": 0,
  "viewCount": 4177,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840530050040631600",
  "url": "https://x.com/AndrewYNg/status/1840530050040631600",
  "text": "@bindureddy Thank you @bindureddy for fighting for open-source and against SB-1047. I appreciate especially your  pushing back against AGI hype -- which leads to science fiction based fears and bad laws like SB-1047!",
  "createdAt": "Sun Sep 29 23:12:13 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 11,
  "replyCount": 2,
  "likeCount": 225,
  "quoteCount": 2,
  "viewCount": 15038,
  "bookmarkCount": 2,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840525690770542797",
  "url": "https://x.com/AndrewYNg/status/1840525690770542797",
  "text": "@drfeifei @StanfordHAI @CAgovernor @GavinNewsom Thank you @drfeifei for your speaking out against SB-1047, and also for working toward a more rational approach to AI policy that protects research and innovation. It has been fantastic seeing you step into the fray and so effectively influence things for the better!",
  "createdAt": "Sun Sep 29 22:54:54 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 10,
  "replyCount": 2,
  "likeCount": 195,
  "quoteCount": 1,
  "viewCount": 12659,
  "bookmarkCount": 3,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840525012719329551",
  "url": "https://x.com/AndrewYNg/status/1840525012719329551",
  "text": "@psychosort @GavinNewsom Thank you @psychosort for your many thoughtful writings on SB-1047, and also your hard work debunking bad arguments -- I've really appreciated your cutting through the noise to the heart of why SB-1047 was a bad idea!",
  "createdAt": "Sun Sep 29 22:52:12 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 2,
  "replyCount": 0,
  "likeCount": 34,
  "quoteCount": 1,
  "viewCount": 6026,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840523898653467108",
  "url": "https://x.com/AndrewYNg/status/1840523898653467108",
  "text": "@reidhoffman @GavinNewsom Thank you @reidhoffman for being a consistently rational voice in AI, and for advocating a responsible approach to bringing benefits to billions while also not being distracted by science fiction fears!",
  "createdAt": "Sun Sep 29 22:47:47 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 1,
  "replyCount": 1,
  "likeCount": 44,
  "quoteCount": 0,
  "viewCount": 5269,
  "bookmarkCount": 2,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840521890936598784",
  "url": "https://x.com/AndrewYNg/status/1840521890936598784",
  "text": "@deanwball Thank you @deanwball for your tirelessly writing and speaking on SB-1047. I've enjoyed many of your writings and am grateful for your consistently championing an approach to AI policy that targets bad conduct, rather than attacks AI models!",
  "createdAt": "Sun Sep 29 22:39:48 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 3,
  "replyCount": 3,
  "likeCount": 32,
  "quoteCount": 0,
  "viewCount": 2201,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840521131511697617",
  "url": "https://x.com/AndrewYNg/status/1840521131511697617",
  "text": "@RonConway @GavinNewsom Thank you @RonConway for your massive efforts reaching out to and helping many stakeholders think through AI policy. I'm grateful that you've been such a powerful voice of reason in debate on SB-1047!",
  "createdAt": "Sun Sep 29 22:36:47 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 15,
  "quoteCount": 0,
  "viewCount": 3525,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840520356114952231",
  "url": "https://x.com/AndrewYNg/status/1840520356114952231",
  "text": "@garrytan @GavinNewsom @ycombinator Thank you @garrytan and the @ycombinator team for all you've done to push back on SB 1047. It has been a pleasure seeing YC organize to argue for a rational approach to AI policy!",
  "createdAt": "Sun Sep 29 22:33:42 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 1,
  "replyCount": 1,
  "likeCount": 51,
  "quoteCount": 0,
  "viewCount": 5748,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840519382390497441",
  "url": "https://x.com/AndrewYNg/status/1840519382390497441",
  "text": "@pmarca @GavinNewsom Thank you @pmarca for your leadership fighting SB1047. (I know the full story of what you've done to fight bad AI regulation is far from told; and, I'm grateful for your massive efforts here.) Having @a16z fight for a rational approach to AI has been a huge boon!",
  "createdAt": "Sun Sep 29 22:29:50 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 15,
  "replyCount": 1,
  "likeCount": 477,
  "quoteCount": 2,
  "viewCount": 71852,
  "bookmarkCount": 19,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840518403683172833",
  "url": "https://x.com/AndrewYNg/status/1840518403683172833",
  "text": "@AnjneyMidha @GavinNewsom Thank you @AnjneyMidha for being a consistent voice of reason in the discussion on SB-1047. It's been a pleasure watching you patiently reach out to and help multiple stakeholders understand why SB-1047 was an awful idea!",
  "createdAt": "Sun Sep 29 22:25:57 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 20,
  "quoteCount": 0,
  "viewCount": 2507,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840517975167967261",
  "url": "https://x.com/AndrewYNg/status/1840517975167967261",
  "text": "@chrislengerich @GavinNewsom Thank you @chrislengerich for your fighting SB-1047 and for your thoughtful analyses of the law. Even as the (bad) law kept getting amended repeatedly, that you kept publishing clear analyses of each revision really helped cut through the noise and confusion!",
  "createdAt": "Sun Sep 29 22:24:15 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 4,
  "replyCount": 1,
  "likeCount": 25,
  "quoteCount": 0,
  "viewCount": 1637,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840516963405271113",
  "url": "https://x.com/AndrewYNg/status/1840516963405271113",
  "text": "@ylecun @GavinNewsom Thank you @ylecun for your consistently clear and thoughtful explanations for why SB-1047 was a bad idea. We are lucky to have you as such a strong champion for open-source and AI innovation!",
  "createdAt": "Sun Sep 29 22:20:13 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 25,
  "replyCount": 6,
  "likeCount": 750,
  "quoteCount": 1,
  "viewCount": 38403,
  "bookmarkCount": 12,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840516178948870569",
  "url": "https://x.com/AndrewYNg/status/1840516178948870569",
  "text": "@AnimaAnandkumar @GavinNewsom @Caltech Thank you @AnimaAnandkumar and the broader @Caltech community for speaking out against SB-1047. I appreciated particularly the letter you circulated explaining why the law would have been a bad idea.",
  "createdAt": "Sun Sep 29 22:17:06 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 30,
  "quoteCount": 1,
  "viewCount": 7537,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840515709190943085",
  "url": "https://x.com/AndrewYNg/status/1840515709190943085",
  "text": "@jeremyphoward Thank you @jeremyphoward for your speaking out against SB-1047! Much appreciate your vocal opposition to this ill-informed law!",
  "createdAt": "Sun Sep 29 22:15:14 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 2,
  "likeCount": 108,
  "quoteCount": 0,
  "viewCount": 9308,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840515301106237858",
  "url": "https://x.com/AndrewYNg/status/1840515301106237858",
  "text": "@pentagoniac @thewendylee @latimes Thank you @pentagoniac for your tireless efforts speaking out against SB-1047! It has been great having you and the AI Alliance help spread the word regarding  why it was a broken, harmful bill.",
  "createdAt": "Sun Sep 29 22:13:37 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 1,
  "replyCount": 1,
  "likeCount": 9,
  "quoteCount": 0,
  "viewCount": 1476,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840502075593199931",
  "url": "https://x.com/AndrewYNg/status/1840502075593199931",
  "text": "@martin_casado @GavinNewsom Huge shoutout to you @martin_casado for the tremendous work you've done to push back on SB-1047 and other anti-open-source regulation. Thank you!",
  "createdAt": "Sun Sep 29 21:21:04 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 6,
  "replyCount": 1,
  "likeCount": 138,
  "quoteCount": 1,
  "viewCount": 11256,
  "bookmarkCount": 3,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1840497561821651069",
  "url": "https://x.com/AndrewYNg/status/1840497561821651069",
  "text": "Thank you Governor @GavinNewsom for vetoing SB-1047 -- your pro-innovation leadership is much appreciated! \n\nAnd to the many people who've been pushing back on SB-1047, a huge thank you as well. Congratulations to all -- we won! üéâ \n \nLooking ahead, lets keep on protecting AI open-source and innovation. We must make sure AI policy is based on science, not science fiction!",
  "createdAt": "Sun Sep 29 21:03:08 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 214,
  "replyCount": 66,
  "likeCount": 1464,
  "quoteCount": 31,
  "viewCount": 158553,
  "bookmarkCount": 66,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1840439968407363942",
  "url": "https://x.com/AndrewYNg/status/1840439968407363942",
  "text": "Good summary of the case for vetoing SB-1047 by @psychosort",
  "createdAt": "Sun Sep 29 17:14:16 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 36,
  "replyCount": 10,
  "likeCount": 206,
  "quoteCount": 2,
  "viewCount": 95813,
  "bookmarkCount": 23,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1839744822737002662",
  "url": "https://x.com/AndrewYNg/status/1839744822737002662",
  "text": "@asangani7 Thank you for having me at Meta Connect @asangani7! I love what you and the @AIatMeta team are doing and am very grateful for all the open models y'all are  releasing. What a wonderful gift to the world!",
  "createdAt": "Fri Sep 27 19:12:01 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 3,
  "quoteCount": 0,
  "viewCount": 2048,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1839392271386730591",
  "url": "https://x.com/AndrewYNg/status/1839392271386730591",
  "text": "A decision on SB-1047 is due soon. Governor @GavinNewsom has said he's concerned about its \"chilling effect, particularly in the open source community\". He's right, and I hope he will veto this. \n\nIf you agree, please like/retweet this to show your support for VETOing SB-1047!",
  "createdAt": "Thu Sep 26 19:51:06 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 480,
  "replyCount": 72,
  "likeCount": 1762,
  "quoteCount": 55,
  "viewCount": 629075,
  "bookmarkCount": 80,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1839338535519932886",
  "url": "https://x.com/AndrewYNg/status/1839338535519932886",
  "text": "Announcing Generative AI for Software Development, a new specialization on Coursera! Taught by my friend and longtime https://t.co/zpIxRSuky4 instructor @lmoroney. \n\nUsing GenAI for software development goes well beyond using chatbots for code generation. This 3-course series shares current best practices for AI use through the entire software development lifecycle: From design and architecture to coding, testing, deployment, and maintenance.\n\nYou'll learn to use LLMs as your thought partner, pair programmer, documentation specialist, security analyst, and performance optimization expert. There's a lot that anyone that writes software can gain from using GenAI, and this will show you how!\n\nPlease sign up here to get started! https://t.co/8nKgy32vIc",
  "createdAt": "Thu Sep 26 16:17:34 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 161,
  "replyCount": 24,
  "likeCount": 799,
  "quoteCount": 3,
  "viewCount": 68926,
  "bookmarkCount": 522,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1839016467767111829",
  "url": "https://x.com/AndrewYNg/status/1839016467767111829",
  "text": "The Llama 3.2 open multimodal model just dropped! https://t.co/R0m408f8CA has been working with Meta on a short course on how to use it. Please sign up for \"Introducing Llama 3.2\", taught by Meta's @asangani7, which will launch Oct 9!\nhttps://t.co/CC2uLNQ20W",
  "createdAt": "Wed Sep 25 18:57:47 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 292,
  "replyCount": 38,
  "likeCount": 1559,
  "quoteCount": 17,
  "viewCount": 111444,
  "bookmarkCount": 576,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1836433152803488119",
  "url": "https://x.com/AndrewYNg/status/1836433152803488119",
  "text": "We just launched a major new Data Engineering Professional Certificate on Coursera! Data underlies all modern AI systems, and engineers who know how to build systems to store and serve it are in high demand. If you're interested in learning this skill, please check out this 4-course sequence, which is designed to make you job-ready to be a Data Engineer. \n\nThis is a new specialization taught by Joe Reis, the co-author of the best-selling book ‚ÄúFundamentals of Data Engineering,\" in collaboration with AWS. (Disclosure, I serve on Amazon's board.) For many AI systems, data engineering is 80% of the work, and modeling is 20%. But people‚Äôs attention on these two topics is often flipped. This makes the job of the data engineer particularly important.\n\nIn this professional certificate, you'll learn foundational data engineering skills while implementing modern data architectures using open-source tools:\n- Learn the key steps of the data lifecycle, to generate, ingest, store, transform, and serve data.\n- Learn to align with organizational goals to design the data pipeline right for your business' needs.\n- Understand how to make necessary trade-offs between speed, scalability, security, and cost.\n\nJoe has distilled into this specialization decades of experience helping startups and large companies with data infrastructure. He is also joined by 17 other industry leaders in the data field, who will help you learn in-demand skills for the growing field of data engineering.\n\nPlease sign up here: https://t.co/2kTGSXrSHP",
  "createdAt": "Wed Sep 18 15:52:37 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 221,
  "replyCount": 39,
  "likeCount": 1352,
  "quoteCount": 12,
  "viewCount": 119315,
  "bookmarkCount": 1030,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1835028475436257705",
  "url": "https://x.com/AndrewYNg/status/1835028475436257705",
  "text": "Last weekend, my two kids colluded in a hilariously bad attempt to mislead me to look in the wrong place during a game of hide-and-seek. I was reminded that most capabilities ‚Äî in humans or in AI ‚Äî develop slowly.\n\nSome people fear that AI someday will learn to deceive humans deliberately. If that ever happens, I‚Äôm sure we will see it coming from far away and have plenty of time to stop it.\n\nWhile I was counting to 10 with my eyes closed, my daughter (age 5) recruited my son (age 3) to tell me she was hiding in the bathroom while she actually hid in the closet. But her stage whisper, interspersed with giggling, was so loud I heard her instructions clearly. And my son‚Äôs performance when he pointed to the bathroom was so hilariously overdramatic, I had to stifle a smile.\n\nPerhaps they will learn to trick me someday, but not yet! (In his awful performance, I think my son takes after me. To this day, I have a terrible poker face ‚Äî which is matched by my perfect lifetime record of losing every poker game I have ever played!)\n\nLast year, the paper ‚ÄúAre Emergent Abilities of Large Language Models a Mirage?‚Äù by Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo, which won a NeurIPS outstanding paper award, considered ‚Äúemergent‚Äù properties of LLMs, which refers to capabilities that seem to appear suddenly as model sizes increase. The authors point out that scaling laws imply that the per-token error rate decreases (improves) slowly with scale, and emergent properties might be an artifact of researchers studying nonlinear or discontinuous metrics that transform a gradually decreasing per-token error rate into something that looks more like a step function.\n\nConsider a ‚Äúcombination lock‚Äù metric that requires getting many items right. Say we‚Äôre measuring the likelihood that an LLM will get 10 independent digits of an answer right. If the odds of it getting each digit right improve gradually from 0 to 1, then the odds of it getting all 10 digits right will appear to jump suddenly. But if we look at continuous metrics, such as the total number of correct digits, we will see that the underlying performance actually improves gradually. (Public perception of a technology can also shift in a discontinuous way because of social dynamics.)\n\nThis is why many of us saw GPT-3 as a promising step in transforming text processing long before ChatGPT appeared: BERT, GPT, GPT-2, and GPT-3 represented points on a continuous spectrum of progress. Or, looking back further in AI history, even though AlphaGo‚Äôs victory over Lee Sedol in the game of Go took the public by surprise, it actually represented many years of gradual improvements in AI‚Äôs ability to play Go.\n\nWhile analogies between human and machine learning can be misleading, I think that just as a person‚Äôs ability to do math, to reason ‚Äî or to deceive ‚Äî grows gradually, so will AI‚Äôs. This means the capabilities of AI technology will grow gradually (I wish we could achieve AGI overnight!), and the ability of AI to be used in harmful applications, too, will grow gradually. As long as we keep performing red-teaming exercises and monitoring our systems‚Äô capabilities as they evolve, I‚Äôm confident that we will have plenty of time to spot issues in advance, and the science-fiction fears of AI-initiated doomsday will remain science fiction.\n\n[Original text (with links): https://t.co/HTnKpqqse5 ]",
  "createdAt": "Sat Sep 14 18:50:56 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 46,
  "replyCount": 53,
  "likeCount": 385,
  "quoteCount": 9,
  "viewCount": 86029,
  "bookmarkCount": 68,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1834268475243856342",
  "url": "https://x.com/AndrewYNg/status/1834268475243856342",
  "text": "New short course Multimodal RAG: Chat with Videos, developed with @intel and taught by @vasudev_lal!\n\nIn this course, you‚Äôll work with LLaVA (Large Language and Vision Assistant), a Large Vision Language Model (LVLM) that can process both images and text. For example, given an image of a person doing a handstand on a skateboard at the beach, LLaVA doesn't just caption the scene, it‚Äôs able to predict possible outcomes, like the person losing balance or falling off. By understanding not just what's in a video frame, but what might happen next, your application can provide more insightful answers to questions about video.\n\nYou'll build a full multimodal RAG pipeline that can chat about video content:\n- Use the BridgeTower model to create joint text-image embeddings in a 512-dimensional multimodal semantic space.\n- Learn video processing techniques to extract keyframes, generate transcripts using Whisper, and create captions.\n- Use the LanceDB vector database to store and retrieve high-dimensional multimodal embeddings.\n- Integrate the LLaVA model, combining CLIP's (Contrastive Language Image Pretraining) vision transformer with Llama, for advanced visual-textual reasoning.\n\nYour final system will ingest video data, generate embeddings for frames and text, perform similarity searches for relevant content, and use the retrieved multimodal context to inform LVLM-based response generation. The result is a system capable of answering nuanced questions about video content, effectively chatting about the video it has processed.\n\nPlease sign up here! https://t.co/cjUHPK3rK2",
  "createdAt": "Thu Sep 12 16:30:58 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 198,
  "replyCount": 33,
  "likeCount": 1190,
  "quoteCount": 12,
  "viewCount": 106733,
  "bookmarkCount": 683,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1831715072248545741",
  "url": "https://x.com/AndrewYNg/status/1831715072248545741",
  "text": "Recently I visited South Korea, where I spoke at length about AI with President Yoon Suk Yeol. Based on what I saw there in government, business, and academia, the nation is well positioned to become a strong AI hub. When he asked me if I would advise South Korea as a member of the Global AI Strategy Steering Group of the country‚Äôs National AI Committee, I agreed on the spot. I was delighted to learn this week that Yann LeCun has also joined. I‚Äôve been consistently impressed by the thoughtful approach the Korean government has taken toward AI, with an emphasis on investment and innovation and a realistic understanding of risks without being distracted by science-fiction scenarios of harm.\n\nI‚Äôve advised many countries to build AI for the sectors where they‚Äôre strong. For example, I wrote previously that by investing in sectors like tourism and certain industrial areas, Thailand can do projects more efficiently than I can in Silicon Valley. South Korea‚Äôs tech ecosystem gives it a foundation to move fast even across numerous sectors. This emphasizes the long-term value for countries to become good at tech, because tech is now pervasive and affects all industries.\n\nKorea has a very strong local software ecosystem. For example, the dominant search engine is not Google or Bing, but Naver (a Korean company). The dominant messaging system is not WhatsApp or WeChat, but KakaoTalk. With local tech giants Naver and Kakao offering email, mobile payment, cloud computing, ride sharing, and other services, the country has many sophisticated tech businesses. Additionally, SK hynix and Samsung are advanced semiconductor manufacturers. It also has a thriving entrepreneurship ecosystem, including Upstage, a language modeling startup, which taught a course with us on ‚ÄúPretraining LLMs.‚Äù Finally, the Korean institutions Seoul National University, which I visited last year, and KAIST have global reputations.\n\nKorea has a highly educated population, highly skilled software engineers, and a thriving set of software products. This gives it a fantastic foundation to embrace the next generation of AI. After meeting with businesses in retail, construction, insurance, cosmetics, telecoms, and other industries, I was delighted by the wide variety of opportunities many companies are pursuing across different industry sectors.\n\nLastly, Korea is known globally for its K-pop. Meeting Bang Si-Hyuk, the chairman of HYBE, which manages the superstar singing group BTS, and learning how the company operates was a real treat! \n\nThat‚Äôs why I‚Äôve traveled to South Korea four times since last year. My venture studio AI Fund, which collaborates with many Korean companies, has benefited tremendously from the advice of many South Koreans, including Taizo Son, Changmook Kang, Hyungjun Kim, Sung Kim, JP Lee, Ian Park, and Alice Oh. I look forward to doing more in, and with, South Korea!\n\n[Original text (with links): https://t.co/mVSVBCI49q ]",
  "createdAt": "Thu Sep 05 15:24:39 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 148,
  "replyCount": 57,
  "likeCount": 1282,
  "quoteCount": 30,
  "viewCount": 137016,
  "bookmarkCount": 155,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1831346457854771255",
  "url": "https://x.com/AndrewYNg/status/1831346457854771255",
  "text": "We just released the final two courses of AI Python for Beginners! The complete set of four courses is now available and remains free for a limited time.\n\nThey teach how to write code (a) Using AI-assistance, which is where the field is going, and (b) to take advantage of generative AI, which allows you to quickly do valuable things with code.\n\nIf you're considering learning to code, AI has made this a great time to jump in. Or if you know someone who is considering learning, please recommend these courses! \n\nhttps://t.co/lTupltSZkT",
  "createdAt": "Wed Sep 04 14:59:54 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 766,
  "replyCount": 86,
  "likeCount": 3784,
  "quoteCount": 47,
  "viewCount": 420025,
  "bookmarkCount": 4529,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1829218674806583779",
  "url": "https://x.com/AndrewYNg/status/1829218674806583779",
  "text": "There‚Äôs still time to stop California‚Äôs SB 1047 from becoming law. For @TIME, I wrote about why this bill would hinder developers and actually make AI less safe. We should be regulating harmful applications of AI, not  general-purpose AI models. https://t.co/dco1e65u9H",
  "createdAt": "Thu Aug 29 18:04:51 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 195,
  "replyCount": 80,
  "likeCount": 940,
  "quoteCount": 38,
  "viewCount": 237806,
  "bookmarkCount": 99,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1829190549842321758",
  "url": "https://x.com/AndrewYNg/status/1829190549842321758",
  "text": "After a recent price reduction by OpenAI, GPT-4o tokens now cost $4 per million tokens (using a blended rate that assumes 80% input and 20% output tokens). GPT-4 cost $36 per million tokens at its initial release in March 2023. This price reduction over 17 months corresponds to about a 79% drop in price per year. (4/36 = (1 - p)^{17/12}) \n\nAs you can see, token prices are falling rapidly! One force that‚Äôs driving prices down is the release of open weights models such as Llama 3.1. If API providers, including startups Anyscale, Fireworks, Together AI, and some large cloud companies, do not have to worry about recouping the cost of developing a model, they can compete directly on price and a few other factors such as speed. \n\nFurther, hardware innovations by companies such as Groq (a leading player in fast token generation), Samba Nova (which serves Llama 3.1 405B tokens at an impressive 114 tokens per second), and wafer-scale computation startup Cerebras (which just announced a new offering this week), as well as the semiconductor giants NVIDIA, AMD, Intel, and Qualcomm, will drive further price cuts. \n\nWhen building applications, I find it useful to design to where the technology is going rather than only where it has been. Based on the technology roadmaps of multiple software and hardware companies ‚Äî which include improved semiconductors, smaller models, and algorithmic innovation in inference architectures ‚Äî I‚Äôm confident that token prices will continue to fall rapidly.\n\nThis means that even if you build an agentic workload that isn‚Äôt entirely economical, falling token prices might make it economical at some point. As I wrote previously, being able to process many tokens is particularly important for agentic workloads, which must call a model many times before generating a result. Further, even agentic workloads are already quite affordable for many applications. Let's say you build an application to assist a human worker, and it uses 100 tokens per second continuously: At $4/million tokens, you'd be spending only $1.44/hour ‚Äì which is significantly lower than the minimum wage in the U.S. and many other countries.\n\nSo how can AI companies prepare?\n- First, I continue to hear from teams that are surprised to find out how cheap LLM usage is when they actually work through cost calculations. For many applications, it isn‚Äôt worth too much effort to optimize the cost. So first and foremost, I advise teams to focus on building a useful application rather than on optimizing LLM costs.\n- Second, even if an application is marginally too expensive to run today, it may be worth deploying in anticipation of lower prices.\n- Finally, as new models get released, it might be worthwhile to periodically examine an application to decide whether to switch to a new model either from the same provider (such as switching from GPT-4 to the latest GPT-4o-2024-08-06) or a different provider, to take advantage of falling prices and/or increased capabilities.\n\nBecause multiple providers now host Llama 3.1 and other open-weight models, if you use one of these models, it might be possible to switch between providers without too much testing (though implementation details ‚Äî specifically quantization, does mean that different offerings of the model do differ in performance). When switching between models, unfortunately, a major barrier is still the difficulty of implementing evals, so carrying out regression testing to make sure your application will still perform after you swap in a new model can be challenging. However, as the science of carrying out evals improves, I‚Äôm optimistic that this will become easier.\n\n[Original text (with links): https://t.co/txk7q32EXn ]",
  "createdAt": "Thu Aug 29 16:13:06 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 627,
  "replyCount": 114,
  "likeCount": 3583,
  "quoteCount": 114,
  "viewCount": 740702,
  "bookmarkCount": 2048,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1828844538712224137",
  "url": "https://x.com/AndrewYNg/status/1828844538712224137",
  "text": "Explore state-of-the-art multimodal prompting in our new short course Large Multimodal Model Prompting with Gemini, taught by Erwin Huizenga in collaboration with @googlecloud.\n\nOne interesting insight from this course: with multimodal models, prompt structure matters significantly. Placing text inputs, such as a patient's medical history, before image inputs, like an X-ray, can enhance the model's ability to contextualize and interpret visual data effectively. In other contexts, such as image captioning, you may get better results by putting the image first. Multimodal models behave differently than text-only LLMs, and effective prompting for models varies depending on the model you‚Äôre using. In this course you‚Äôll learn how to effectively prompt Gemini models.\n\nGemini's multimodal capabilities also enable new approaches in AI application development, for example:\n- The Gemini library handles various video formats (MP4, MOV, MPEG), streamlining applications using these formats.\n- Large context window (up to 1 million tokens) enables processing of extensive content, like analyzing multiple 50-minute videos simultaneously. \n- Function calling feature integrates real-time data (e.g., current exchange rates) into model responses.\n\nThe course demonstrates building multimodal applications with real-world examples including document analyzers that reason across text and graphs simultaneously, video content extractors that find and timestamp specific information from multiple hours of footage, and automated expense report systems processing receipt images while cross-referencing company policies.\n\nSign up here: https://t.co/4yI4DXcFpK",
  "createdAt": "Wed Aug 28 17:18:10 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 158,
  "replyCount": 36,
  "likeCount": 838,
  "quoteCount": 7,
  "viewCount": 73454,
  "bookmarkCount": 450,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1828552114123288835",
  "url": "https://x.com/AndrewYNg/status/1828552114123288835",
  "text": "Really fun hackathon, and it was great to see 30 creative Agentic AI projects, all built in a day. Well done to all the hackathon participants, and thank you @HenryYin_ , @AlexReibman and @agihouse_org for having me! \n¬∑",
  "createdAt": "Tue Aug 27 21:56:11 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 44,
  "replyCount": 26,
  "likeCount": 405,
  "quoteCount": 5,
  "viewCount": 64941,
  "bookmarkCount": 36,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1828504913158316075",
  "url": "https://x.com/AndrewYNg/status/1828504913158316075",
  "text": "I've been playing with @SambaNovaAI's API serving fast Llama 3.1 405B tokens. Really cool to see leading model running at speed. Congrats to Samba Nova for hitting a 114 tokens/sec speed record (and also thanks @KunleOlukotun for getting me an API key!) https://t.co/GuBfYsfizJ",
  "createdAt": "Tue Aug 27 18:48:37 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 65,
  "replyCount": 20,
  "likeCount": 346,
  "quoteCount": 20,
  "viewCount": 51087,
  "bookmarkCount": 73,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1827047221013180611",
  "url": "https://x.com/AndrewYNg/status/1827047221013180611",
  "text": "I am thrilled to announce that Dan Maloney is becoming the Chief Executive Officer (CEO) of LandingAI, and I will step into the company‚Äôs Executive Chairman role, where I will continue to focus on Visual AI deep tech with our team, including agentic vision.\n\nSince Dan joined two years ago as the company‚Äôs Chief Operating Officer (COO), his leadership across the company has been instrumental to LandingAI serving more customers and better than ever. Dan played an integral role in all of our key initiatives, including the launch of our self-service offering with LandingLens available for any vision application builder to use, establishing dozens of strategic and channel partnerships, increasing our focus on serving developers, and improving our operational excellence; even as we continue to innovate on Visual AI technology.\n\nI look forward to continuing LandingAI‚Äôs journey with Dan, with our fantastic team, and with all our wonderful partners and users!\n\nText of full announcement below. https://t.co/At9k1JhvBV",
  "createdAt": "Fri Aug 23 18:16:17 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 26,
  "replyCount": 27,
  "likeCount": 311,
  "quoteCount": 4,
  "viewCount": 42753,
  "bookmarkCount": 31,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1826635358848909737",
  "url": "https://x.com/AndrewYNg/status/1826635358848909737",
  "text": "I‚Äôm encouraged at the progress of the U.S. government at moving to stem harmful AI applications. Two examples are the new Federal Trade Commission (FTC) ban on fake product reviews and the DEFIANCE Act, which imposes punishments for creating and disseminating non-consensual deepfake porn. Both rules take a sensible approach to regulating AI insofar as they target harmful applications rather than general-purpose AI technology.\n\nThe best way to ensure AI safety is to regulate it at the application level rather than the technology level. This is important because the technology is general-purpose and its builders (such as a developer who releases an open-weights foundation model) cannot control how someone else might use it. If, however, someone applies AI in a nefarious way, we should stop that application.\n\nEven before generative AI, fake reviews were a problem on many websites, and many tech companies dedicate considerable resources to combating them. A telltale sign of old-school fake reviews is the use of similar wording in different reviews. AI‚Äôs ability to automatically paraphrase or rewrite is making fake reviews harder to detect.\n\nImportantly, the FTC is not going after the makers of foundation models for fake reviews. The provider of an open weights AI model, after all, can‚Äôt control what someone else uses it for. Even if one were to try to train a model to put up guardrails against writing reviews, I don‚Äôt know how it could distinguish between a real user of a product asking for help writing a legitimate review and a spammer who wanted a fake review. The FTC appropriately aims to ban the application of fake reviews along with other deceptive practices such as buying positive reviews.\n\nThe DEFIANCE Act, which passed unanimously in the Senate (and still requires passage in the House of Representatives before the President can sign it into law) imposes civil penalties for the creating and distributing non-consensual, deepfake porn. This disgusting application is harming many people including underage girls. While many image generation models do have guardrails against generating porn, these guardrails often can be circumvented via jailbreak prompts or fine-tuning (for models with open weights).\n\nAgain, DEFIANCE regulates an application, not the underlying technology. It aims to punish people who engage in the application of creating and distributing non-consensual intimate images, regardless of how they are generated ‚Äî whether the perpetrator uses a diffusion model, a generative adversarial network, or Microsoft Paint to create an image pixel by pixel.\n\nI hope DEFIANCE passes in the House and gets signed into law. Both rules guard against harmful AI applications without stifling AI technology itself (unlike California‚Äôs poorly designed SB-1047), and they offer a good model for how the U.S. and other nations can protect citizens against other potentially harmful applications.\n\n[Original text (with links): https://t.co/AA2x2KxCqW ]",
  "createdAt": "Thu Aug 22 14:59:41 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 63,
  "replyCount": 44,
  "likeCount": 320,
  "quoteCount": 14,
  "viewCount": 46353,
  "bookmarkCount": 47,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1826273022149587383",
  "url": "https://x.com/AndrewYNg/status/1826273022149587383",
  "text": "Build and customize complex AI applications with a flexible framework in this new short course, Building AI Applications with Haystack. Created in collaboration with @deepset_ai, and taught by @tuanacelik, who is the developer relations lead for Haystack at deepset.\n\nGenerative AI technology is changing rapidly and it can be challenging to integrate APIs from different LLMs, vector databases, and various tools such as web search. In this course, you will learn how to use the Haystack framework to make your development process more modular, allowing you to manage complexity and focus more on building your application.\n\nIn detail, you‚Äôll:\n- Build a RAG pipeline using Haystack‚Äôs main building blocks ‚Äì components, pipelines, and document stores.\n- Create custom components in your pipeline by building a Hacker News summarizer that extends your app‚Äôs ability to access APIs.\n- Use conditional routing to create a branching pipeline with a fallback to web search mechanism when the LLM does not have the necessary context to respond to the user's query.\n- Build a self-reflecting agent for named entity recognition that loops using an output validator custom component.\n- Create a chat agent using OpenAI's function-calling capabilities which allow you to provide Haystack pipelines as tools to the LLM, enhancing that agent's capabilities.\n\nBy the end of this course, you will learn a high-level orchestration framework that can help make your applications flexible, extendible, and maintainable, even as the technology stack changes, new user needs arise, and you add new features to your application.\n\nPlease sign up here: https://t.co/wCvf549cM0",
  "createdAt": "Wed Aug 21 14:59:53 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 101,
  "replyCount": 26,
  "likeCount": 526,
  "quoteCount": 7,
  "viewCount": 53542,
  "bookmarkCount": 232,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1825577904287395984",
  "url": "https://x.com/AndrewYNg/status/1825577904287395984",
  "text": "Thank you @RepZoeLofgren for speaking out against the anti-open source, anti-innovation bill SB-1047. \n\nShe is the ranking member of the House Science, Space and Technology Committee. Her staff concludes: \"the problematic core concerns remain: there is little evidentiary basis for the bill; the bill would negatively affect open-source development by applying liability to downstream use; it uses arbitrary thresholds not backed in science.\" [my boldface] \n\nLets all keep fighting to protect open source AI.\n\nhttps://t.co/ln7uOEzmgh",
  "createdAt": "Mon Aug 19 16:57:44 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 79,
  "replyCount": 19,
  "likeCount": 444,
  "quoteCount": 8,
  "viewCount": 116780,
  "bookmarkCount": 36,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1824106080106123638",
  "url": "https://x.com/AndrewYNg/status/1824106080106123638",
  "text": "When entrepreneurs build a startup, it is often their speed and momentum that gives them a shot at competing with the tech behemoths. This is true of countries as well.\n\nI was recently in Thailand, where I was delighted to see tremendous momentum building in AI (and sip the best Thai ice tea I‚Äôve ever tasted). Even though Thailand is not as advanced in AI technology or applications as leading tech countries, the enthusiasm for building AI throughout government, corporations, and academia was thrilling. I came away heartened that AI‚Äôs benefits will be spread among many countries and convinced that one's level of AI development right now matters less than your momentum toward increasing it.\n\nSeeing the momentum behind AI in Thailand ‚Äî where the per capita GDP is around one fifth that of Japan, and one tenth that of the United States ‚Äî left me feeling that any country, company, or person has a shot at doing meaningful work in the field. While advanced economies such as the U.S. and China are still in the lead, generative AI has made the playing field more level. Foundation models, especially those with open weights, are significantly lowering the barriers to building meaningful AI projects. In Thailand, a lot of people I met weren‚Äôt just talking about AI, they were rolling up their sleeves and building. That buys a nation a lot more momentum than just talk.\n\nI met with Prime Minister Srettha Thavisin and his Ministers of Higher Education and Education (primary/secondary) along with many staffers. It was delightful to hear the PM speak of his enthusiasm for AI. The ministers discussed how to (i) provide AI training and (ii) use AI to improve education in a variety of subjects. Happily, the focus was on creating value while thinking through realistic risks like AI‚Äôs potential to proliferate misinformation, and not a single person asked me about whether AI will lead to human extinction!\n\nI also met with many business leaders and enjoyed seeing a rapid pace of experimentation with AI. KBTG, an affiliate of the country‚Äôs leading digital bank KBank, is working on a financial chatbot advisor, AI-based identity verification for anti-fraud, AI for auto insurance, and a Thai-language financial large language model. These features are growing mobile banking and increasing financial access. Many business leaders in other sectors, too, have asked their teams to run experiments. There are many AI applications yet to be built in industrial sectors, tourism, trade, and more! (KBTG is an investor in AI Fund, which I lead.)\n\nI often visit universities in both developed and developing economies, and I‚Äôve been surprised to see that universities in developing economies sometimes adopt AI faster. At Chulalongkorn University (known as Chula), I met with the University President Bundhit Eua-arporn and Director of Chula AI Professor Proadpran Punyabukkana. Chula AI has rolled out campus-wide training in generative AI for faculty, staff, and students. In addition, it supports building AI applications such as AI screening for depression and gastrointestinal cancer. \n\nIt takes years to build up advanced technology. But momentum matters, and there will be many rewards along the journey. There‚Äôs no time like the present to start building!\n\n[Original text: https://t.co/xqFyYudIZ7 ]",
  "createdAt": "Thu Aug 15 15:29:14 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 72,
  "replyCount": 49,
  "likeCount": 466,
  "quoteCount": 12,
  "viewCount": 72121,
  "bookmarkCount": 97,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1823759268937650528",
  "url": "https://x.com/AndrewYNg/status/1823759268937650528",
  "text": "Learn a development pattern to systematically improve the accuracy and reliability of LLM applications in our new short course, Improving Accuracy of LLM Applications, built in partnership with @LaminiAI and @Meta, and taught by Lamini‚Äôs CEO @realSharonZhou, and Meta‚Äôs Senior Director of Partner Engineering,  @asangani7. (Disclosure: I am an investor in Lamini.)\n\nThe path to tuning an LLM application can be complex. In this course, you'll learn a systematic sequence of steps for improving accuracy by reducing hallucinations: \n- Create an evaluation dataset to measure model accuracy\n- Add prompt engineering and self-reflection\n- Fine-tune your model including \"memory-tuning\" which is a new method of embedding facts in an LLM\n\nUsing the Llama 3-8B parameter model, you will:\n- Build a text-to-SQL agent with a custom schema and simulate situations where it hallucinates\n- Understand the difference between instruction fine-tuning, which gives pre-trained LLMs instructions to follow, and memory fine-tuning\n- See how Performance-Efficient Fine-tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) reduce training time by 100x and Mixture of Memory Experts (MoME) reduces it even further\n\nI appreciate Meta releasing the Llama's family of open models -- this course gives an example of the unique type of work that developers can do with such models.\n\nPlease sign up here: https://t.co/FITZFVlzNk",
  "createdAt": "Wed Aug 14 16:31:08 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 126,
  "replyCount": 22,
  "likeCount": 635,
  "quoteCount": 12,
  "viewCount": 65971,
  "bookmarkCount": 312,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1823429929381585248",
  "url": "https://x.com/AndrewYNg/status/1823429929381585248",
  "text": "It is very rare for the U.S. Federal government to chime in  on state-level legislation. I'm glad that Congressman @RoKhanna is speaking out about why California's proposed SB 1047 is a bad idea. \n\nHe writes that he is \"concerned that the bill as currently written would be ineffective, punishing of individual entrepreneurs and small businesses, and hurt California‚Äôs spirit of innovation.\" I agree with him. \n\nhttps://t.co/V1yf7ERyxw",
  "createdAt": "Tue Aug 13 18:42:27 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 29,
  "replyCount": 15,
  "likeCount": 197,
  "quoteCount": 2,
  "viewCount": 38910,
  "bookmarkCount": 14,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1823388325409140946",
  "url": "https://x.com/AndrewYNg/status/1823388325409140946",
  "text": "I‚Äôm speaking on a panel this Thursday (3pm PT) about Stratospheric Aerosol Injection (SAI). SAI is a potential approach to reducing global warming, and AI climate modeling has an important role to play in understanding its impact. I look forward to discussing the atmospheric science and social/political factors of SAI with @chrfield @DKeithClimate @DougMacMartin and Simone Tilmes.\n\nPlease register here to join us! https://t.co/Algxy9Wxqp",
  "createdAt": "Tue Aug 13 15:57:08 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 33,
  "replyCount": 23,
  "likeCount": 169,
  "quoteCount": 6,
  "viewCount": 33538,
  "bookmarkCount": 24,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1821950054439252385",
  "url": "https://x.com/AndrewYNg/status/1821950054439252385",
  "text": "There is an chorus of voices across academia, business and even government concerned about California's proposed anti-open source, anti-innovation bill SB 1047. \n\n@martin_casado has a nice thread summarizing some of these. Thank you @russellwald, @vishalmisra, Ion Stoica, many people from the University of California community, @AnimaAnandkumar, @drfeifei, @garrytan, @AnjneyMidha, Zoe Lofgren and many many others for speaking out to explain why this bill will hurt AI innovation without actually increasing safety. \n\nIf anything, by hurting open source and thus hampering researchers' ability to study cutting-edge models to identify problems, I believe SB1047 is more likely to make AI less safe.",
  "createdAt": "Fri Aug 09 16:41:57 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 69,
  "replyCount": 11,
  "likeCount": 292,
  "quoteCount": 8,
  "viewCount": 72687,
  "bookmarkCount": 38,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1821206887913943110",
  "url": "https://x.com/AndrewYNg/status/1821206887913943110",
  "text": "I'm teaching a new course! AI Python for Beginners is a series of four short courses that teach anyone to code, regardless of current technical skill. We are offering these courses free for a limited time.\n\nGenerative AI is transforming coding. This course teaches coding in a way that‚Äôs aligned with where the field is going, rather than where it has been:\n\n(1) AI as a Coding Companion. Experienced coders are using AI to help write snippets of code, debug code, and the like. We embrace this approach and describe best-practices for coding with a chatbot. Throughout the course, you'll have access to an AI chatbot that will be your own coding companion that can assist you every step of the way as you code.\n\n(2) Learning by Building AI Applications. You'll write code that interacts with large language models to quickly create fun applications to customize poems, write recipes, and manage a to-do list. This hands-on approach helps you see how writing code that calls on powerful AI models will make you more effective in your work and personal projects.\n\nWith this approach, beginning programmers can learn to do useful things with code far faster than they could have even a year ago.\n\nKnowing a little bit of coding is increasingly helping people in job roles other than software engineers. For example, I've seen a marketing professional write code to download web pages and use generative AI to derive insights; a reporter write code to flag important stories; and an investor automate the initial drafts of contracts.\n\nWith this course you‚Äôll be equipped to automate repetitive tasks, analyze data more efficiently, and leverage AI to enhance your productivity.\n\nIf you are already an experienced developer, please help me spread the word and encourage your non-developer friends to learn a little bit of coding.\n\nI hope you'll check out the first two short courses here! https://t.co/lTupltSZkT",
  "createdAt": "Wed Aug 07 15:28:53 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 1779,
  "replyCount": 487,
  "likeCount": 8575,
  "quoteCount": 173,
  "viewCount": 1214911,
  "bookmarkCount": 7991,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1820863062993490137",
  "url": "https://x.com/AndrewYNg/status/1820863062993490137",
  "text": "I wrote last week about why working on a concrete startup or project idea ‚Äî meaning a specific product envisioned in enough detail that we can build it for a specific target user ‚Äî lets you go faster. In this letter, I‚Äôd like to share some best practices for identifying promising ideas. \n\nAI Fund, which I lead, works with many corporate partners to identify ideas, often involving applications of AI to the company‚Äôs domain. Because AI is applicable to numerous sectors such as retail, energy, logistics and finance, I‚Äôve found working with domain experts who know these areas well immensely helpful for identifying what applications are worth building in these areas.\n\nOur brainstorming process starts with recommending that a large number of key contributors at our partner corporation (at least 10 but sometimes well over 100) gain a non-technical, business-level understanding of AI and what it can and can‚Äôt do. Taking https://t.co/zpIxRSuky4‚Äôs ‚ÄúGenerative AI for Everyone‚Äù course is a popular option, after which a company is well positioned to assign a small team to coordinate a brainstorming process, followed by a prioritization exercise to pick what to work on. The brainstorming process can be supported by a task-based analysis of jobs in which we decompose employees‚Äô jobs into tasks to identify which ones might be automated or augmented using AI.\n\nHere are some best practices for these activities:\n\n(i) Trust the domain expert‚Äôs gut. A domain expert who has worked for years in a particular sector will have well honed instincts that let them make leaps that would take a non-expert weeks of research.\n\nLet‚Äôs say we‚Äôre working with a financial services expert and have developed a vague idea (‚Äúbuild a chatbot for financial advice‚Äù). To turn this into a concrete idea, we might need to answer questions such as what areas of finance to target (should we focus on budgeting, investing, or insurance?) and what types of user to serve (fresh graduates, mortgage applicants, new parents, or retirees?) Even a domain expert who has spent years giving financial advice might not know the best answer, but a choice made via their gut gives a quick way to get to one plausible concrete idea. Of course, if market-research data can be obtained quickly to support this decision, we should take advantage of it. But to avoid slowing down too much, we‚Äôve found that experts‚Äô gut reactions work well and are a quick way to make decisions.\n\nSo, if I‚Äôm handed a non-concrete idea, I often ask a domain expert to use their gut ‚Äî and nothing else ‚Äî to quickly make decisions as needed to make the idea concrete. The resulting idea is only a starting point to be tweaked over time. If, in the discussion, the domain expert picks one option but seems very hesitant to disregard a different option, then we can also keep the second option as a back-up that we can quickly pivot to if the initial one no longer looks promising.\n\n(ii) Generate many ideas. I usually suggest coming up with at least 10 ideas; some will come up with over 100, which is even better. The usual brainstorming advice to go for volume rather than quality applies here. Having many ideas is particularly important when it comes to prioritization. If only one idea is seriously considered ‚Äî sometimes this happens if a senior executive has an idea they really like and puts this forward as the ‚Äúmain‚Äù idea to be worked on ‚Äî there‚Äôs a lot of pressure to make this idea work. Even if further investigation discovers problems with it ‚Äî for example, market demand turns out to be weak or the technology is very expensive to build ‚Äî the team will want to keep trying to make it work so we don‚Äôt end up with nothing.\n\nIn contrast, when a company has many ideas to choose from, if one starts to look less interesting, it‚Äôs easy to shift attention to a different one. When many ideas are considered, it‚Äôs easier to compare them to pick the superior ones. As explained in the book Ideaflow, teams that generate more ideas for evaluation and prioritization end up with better solutions.\n\nBecause of this, I‚Äôve found it helpful to run a broad brainstorming process that involves many employees. Specifically, large companies have many people who collectively have a lot of wisdom regarding the business. Having a small core team coordinate the gathering of ideas from a large number of people lets us tap into this collective fountain of invention. Many times I‚Äôve seen a broad effort (involving, say, ~100 people who are knowledgeable about the domain and have a basic understanding of AI) end up with better ideas than a narrow one (involving, say, a handful of top executives).\n\n(iii) Make the evaluation criteria explicit. When evaluating and prioritizing, clear criteria for scoring and ranking ideas helps the team to judge ideas more consistently. Business value and technical feasibility are almost always included. Additionally, many companies will prioritize projects that can be a quick win (to build momentum for their overall AI efforts) or support certain strategic priorities such as growth in a particular part of the business. Making such criteria explicit can help during the idea-generation phase, and it‚Äôs critical when you evaluate and prioritize.\n\nIn large companies, it can take a few weeks to go through a process to gather and prioritize ideas, but this pays off well in identifying valuable, concrete ideas to pursue. AI isn‚Äôt useful unless we find appropriate ways to apply it, and I hope these best practices will help you to generate great AI application ideas to work on.\n\n[Original text: https://t.co/7pwXMGEbpI ]",
  "createdAt": "Tue Aug 06 16:42:38 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 194,
  "replyCount": 24,
  "likeCount": 947,
  "quoteCount": 11,
  "viewCount": 118669,
  "bookmarkCount": 1011,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1818666105340330260",
  "url": "https://x.com/AndrewYNg/status/1818666105340330260",
  "text": "Learn how embedding models are built, trained, and used in semantic search systems in this new short course, Embedding Models: From Architecture to Implementation, created with @vectara and taught by @ofermend.\n\nMany LLM apps use a single embedding model for both questions and answers. This can lead to issues such as retrieving results that are similar to the question itself rather than relevant answers. With a dual encoder architecture, you can use separate embedding models for questions and answers to better match questions with appropriate answers.\n\nIn this course, you will use, build, and train a dual encoder model.\n\nYou‚Äôll also learn:\n- What are word embeddings, and how they are used\n- The evolution of embeddings to BERT, where embeddings take into account each word's surrounding context  \n- How a contrastive loss is used to train a dual encoder model with one encoder trained to embed questions and the other responses.\n- How to analyze a dual encoder‚Äôs effect on search relevance and compare it to a retrieval process with a single encoder.\n\nPlease sign up here! https://t.co/4wuz0vAw6X",
  "createdAt": "Wed Jul 31 15:12:43 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 147,
  "replyCount": 23,
  "likeCount": 716,
  "quoteCount": 8,
  "viewCount": 71347,
  "bookmarkCount": 389,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1818320842654371918",
  "url": "https://x.com/AndrewYNg/status/1818320842654371918",
  "text": "AI‚Äôs usefulness in a wide variety of applications creates many opportunities for entrepreneurship. Here, I‚Äôd like to share what might be a counter-intuitive best practice that I‚Äôve learned from leading AI Fund, a venture studio that has built dozens of startups with extraordinary entrepreneurs. When it comes to building AI applications, we strongly prefer to work on a concrete idea, meaning a specific product envisioned in enough detail that we can build it for a specific target user.\n\nSome design philosophies say you shouldn‚Äôt envision a specific product from the start. Instead, they recommend starting with a problem to be solved and then carefully studying the market before you devise a concrete solution. There‚Äôs a reason for this: The more concrete or precise your product specification, the more likely it is to be off-target. However, I find that having something specific to execute toward lets you go much faster and discover and fix problems more rapidly along the way. If the idea turns out to be flawed, rapid execution will let you discover the flaws sooner, and this knowledge and experience will help you switch to a different concrete idea.\n\nOne test of concreteness is whether you‚Äôve specified the idea in enough detail that a product/engineering team could build an initial prototype. For example, ‚ÄúAI for livestock farming‚Äù is not concrete; it‚Äôs vague. If you were to ask an engineer to build this, they would have a hard time knowing what to build.  Similarly, ‚ÄúAI for livestock tracking in farming‚Äù is still vague. There are so many approaches to this that most reasonable engineers wouldn‚Äôt know what to build. But ‚ÄúApply face recognition to cows so as to recognize individual cows and monitor their movement on a farm‚Äù is specific enough that a good engineer could quickly choose from the available options (for example, what algorithm to try first, what camera resolution to use, and so on) to let us relatively efficiently assess:\n- Technical feasibility: For example, do face recognition algorithms developed for human faces work for cows? (It turns out that they do!)\n- Business feasibility: Does the idea add enough value to be worth building? (Talking to farmers might quickly reveal that solutions like RFID are easier and cheaper.)\n\nArticulating a concrete idea ‚Äî which is more likely than a vague idea to be wrong ‚Äî takes more courage. The more specific an idea, the more likely it is to be a bit off, especially in the details. The general area of AI for livestock farming seems promising, and surely there will be good ways to apply AI for livestock. In contrast, specifying a concrete idea, which is much easier to invalidate, is scary.\n\nThe benefit is that the clarity of a specific product vision lets a team execute much faster. One strong predictor of how likely a startup is to succeed is the speed with which it can get stuff done. This is why founders with clarity of vision tend to be desired; clarity helps drive a team in a specific direction. Of course, the vision has to be a good one, and there‚Äôs always a risk of efficiently building something that no one wants to buy! But a startup is unlikely to succeed if it meanders for too long without forming a clear, concrete vision.\n\nBuilding toward something concrete ‚Äî if you can do so in a responsible way that doesn‚Äôt harm others ‚Äî lets you get critical feedback more efficiently and, if necessary, switch directions sooner. (An earlier letter in The Batch also discussed when it‚Äôs better to go with a ‚ÄúReady, Fire, Aim‚Äù approach to projects.) One factor that favors this approach is the low cost of experimenting and iterating. This is increasingly the case for many AI applications, but perhaps less so for deep-tech AI projects.\n\nI realize that this advice runs counter to common practice in design thinking, which warns against leaping to a solution too quickly, and instead advocates spending time understanding end-users, deeply understanding their problems, and brainstorming a wide range of solutions. If you‚Äôre starting without any ideas, then such an extended process can be a good way to develop good ideas. Further, keeping ideas open-ended can be good for curiosity-driven research, where investing to pursue deep tech with only a vague direction in mind can pay huge dividends over the long term.\n\nIf you are thinking about starting a new AI project, consider whether you can come up with a concrete vision to execute toward. Even if the initial vision turns out not to be quite right, rapid iteration will let you discover this sooner, and the learnings will let you switch to a different concrete idea.\n\n[Original text: https://t.co/8CkePKyBJ4 ]",
  "createdAt": "Tue Jul 30 16:20:46 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 114,
  "replyCount": 90,
  "likeCount": 581,
  "quoteCount": 14,
  "viewCount": 58787,
  "bookmarkCount": 327,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1816171538275787145",
  "url": "https://x.com/AndrewYNg/status/1816171538275787145",
  "text": "Learn to train an LLM with distributed data while ensuring privacy using federated learning in a new two-part short course, Intro to Federated Learning and Federated Fine-tuning of LLMs with Private Data, created with @flwrlabs and taught by @daniel_janes and @niclane7.\n\nFederated learning allows a single model to be trained across multiple devices, such as phones, or multiple organizations, such as hospitals, without the need to share data to a central server.\n\nThis two-part course gives you an introduction to federated learning, and then teaches you how to fine-tune your large language model with distributed data using Flower Lab‚Äôs open source federated learning framework.\n\nYou‚Äôll learn:\n- How to use federated learning to train a variety of models, ranging from speech and vision models to LLMs, across distributed data while offering data privacy options to users and organizations.\n- Privacy Enhancing Technologies like differential privacy (DP), which obscures individual data by adding calibrated noise to query results.\n- Two variants of differential privacy - Central and Local - and how to choose depending on your use case.\n- How to measure and decrease bandwidth usage to make federated learning more practical and efficient with techniques like using pre-trained models and Parameter-Efficient Fine-Tuning\n- How federated LLM fine-tuning reduces the risk of leaking training data.\n\nSign up here! https://t.co/cuN1n0ylee",
  "createdAt": "Wed Jul 24 18:00:12 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 128,
  "replyCount": 20,
  "likeCount": 656,
  "quoteCount": 11,
  "viewCount": 63905,
  "bookmarkCount": 323,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1815792223411429741",
  "url": "https://x.com/AndrewYNg/status/1815792223411429741",
  "text": "Thank you Meta and the Llama team for your huge contributions to open-source! Llama 3.1 with increased context length and improved capabilities is a wonderful gift to everyone. \n\nI hope foolish regulations don't like California's proposed SB1047 don't stop such innovations.",
  "createdAt": "Tue Jul 23 16:52:56 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 215,
  "replyCount": 36,
  "likeCount": 1717,
  "quoteCount": 12,
  "viewCount": 104425,
  "bookmarkCount": 85,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1813591557511295234",
  "url": "https://x.com/AndrewYNg/status/1813591557511295234",
  "text": "New short course on Pretraining LLMs! Developed with @UpstageAI and taught by their CEO @hunkims and CSO @echojuliett.\n\nWhile prompting or fine-tuning existing models works well for many general language tasks, pretraining is  valuable for specialized domains or languages with limited representation in current models.\n\nThis course walks you through the LLM pretraining pipeline:\n1. Data preparation: Learn to source, clean, and prepare training data using HuggingFace.\n2. Model architecture: Configure transformer networks, including modifying existing models.\n3. Training: Set up and run training using open-source libraries.\n4. Evaluation: Benchmark performance using popular evaluation strategies.\n\nAs an example use case, you'll also compare the output of a base model with its fine-tuned and further pretrained variants, to see the impact of pretraining on a model's ability to write Python. \n\nThe course also explores an innovative technique called depth up-scaling, which Upstage used to train their Solar model family, reducing pretraining compute costs by up to 70%. This technique works by first duplicating layers of a smaller pretrained model to form a larger model, and then further pretraining the result.\n\nSign up here! https://t.co/IjYmNPR7sd",
  "createdAt": "Wed Jul 17 15:08:16 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 206,
  "replyCount": 30,
  "likeCount": 986,
  "quoteCount": 14,
  "viewCount": 85009,
  "bookmarkCount": 488,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1811425437048070328",
  "url": "https://x.com/AndrewYNg/status/1811425437048070328",
  "text": "I continue to be alarmed at the progress of proposed California regulation SB 1047 and the attack it represents on open source and more broadly on AI innovation. As I wrote previously, this proposed law makes a fundamental mistake of regulating AI technology instead of AI applications, and thus would fail to make AI meaningfully safer. I‚Äôd like to explain why the specific mechanisms of SB 1047 are so pernicious to open source.\n\nTo be clear, there are routes that regulators should pursue to improve safety. For example, I would welcome outlawing nonconsensual deepfake pornography, standardizing watermarking and fingerprinting to identify generated content, and investing more in red teaming and other safety research. Unfortunately, the proposed bill pursues a less beneficial and more harmful path.\n\nSB 1047‚Äôs purported goal is to ensure safety of AI models. It puts in place complex reporting requirements for developers who fine-tune models or develop models that cost more than $100 million to train. It is a vague, ambiguous law that imposes significant penalties for violations, creating a huge gray zone in which developers can‚Äôt be sure how to avoid breaking the law. This will paralyze many teams.\n\nYou can read the latest draft of the law online. I‚Äôve read through it carefully, and I find it ambiguous and very hard to follow.\n\nDevelopers who try to navigate the law‚Äôs complex requirements face what feels like a huge personal risk. It requires that developers submit, under penalty of perjury, a certification of compliance with the requirements of the law. But when the requirements are complex, hard to understand, and can even shift according to the whims of an unelected body (more on this below), how do we ensure we are in compliance?\n\nFor example, the certification must include many different sections. One is an analysis of ‚Äúthe nature and magnitude of critical harms ‚Ä¶ the model might reasonably cause or enable.‚Äù But given that even leading AI researchers aren‚Äôt sure what harms models might cause or enable, how is a team of developers supposed to figure this out and declare ‚Äî under penalty of perjury ‚Äî that they meet this requirement?\n\nFurther, some developers will be required to implement ‚Äúprotections to prevent ‚Ä¶ misuse of, or unsafe post-training modifications of, the covered model and all covered model derivatives ‚Ä¶ that are appropriate in light of the risks associated with the covered model, including from advanced persistent threats or other sophisticated actors.‚Äù Even leading AI researchers don‚Äôt agree on how best to ‚Äúprotect‚Äù AI models against these supposed risks, or what would be ‚Äúappropriate.‚Äù So how are developers supposed to figure out how to comply with this requirement?\n\nThis creates a scary situation for developers. Committing perjury could lead to fines and even jail time. Some developers will have to hire expensive lawyers or consultants to advise them on how to comply with these requirements. (I am not a lawyer and am not giving legal advice, but one way to try to avoid perjury is to show that you are relying on expert advice, to demonstrate that you had no intent to lie.) Others will simply refrain from releasing cutting-edge AI products.\n\nIf this law passes, the fear of a trial by a jury ‚Äî leading to a verdict that can be very unpredictable and with significant penalties in the event of a conviction ‚Äî will be very real. What if someone releases a model today after taking what they genuinely felt were reasonable safeguards, but a few years later, when views on AI technology might have shifted, some aggressive prosecutor manages to convince a jury that whatever they did was not, in hindsight, ‚Äúreasonable‚Äù? \n\nReasonableness is ambiguous and its legal interpretation can depend on case law, jury instructions, and common facts, among other things. This makes it very hard to ensure that what a developer does today will be deemed reasonable by a future jury. (For more on this, see Context Fund‚Äôs analysis of SB 1047. [URLs in article linked to below.])\n\nOne highly placed lawyer in the California government who studied this law carefully told me they found it hard to understand. I invite you to read it and judge for yourself ‚Äî if you find the requirements clear, you might have a brilliant future as a lawyer!\n\nAdding to the ambiguity, the bill would create a Frontier Model Division (FMD) with a five-person board that has the power to dictate standards to developers. This small board would be a great target for lobbying and regulatory capture. (Bill Gurley has a great video on regulatory capture.) The unelected FMD can levy fees on developers to cover its costs. It can arbitrarily change the computation threshold at which fine-tuning a model becomes subject to its oversight. This can lead to even small teams being required to hire an auditor to check for compliance with an ambiguous safety standard.\n\nThese provisions don‚Äôt ensure that AI is safe. They create regulatory uncertainty, and more opportunities for vested interests wishing to stifle open-source to lobby for shifts in the requirements that raise the cost of compliance. This would lock out many teams that don‚Äôt have a revenue stream ‚Äî specifically, many open-source contributors ‚Äî that would let them pay for lobbyists, auditors, and lawyers to help ensure they comply with these ambiguous and unreasonable requirements.\n\nOpen source is a wonderful force that is bringing knowledge and tools to many people, and is a key pillar of AI innovation. I am dismayed at the concerted attacks on it. Make no mistake, there is a fight in California right now for the future health of open source. I am committed to doing what I can to preserve open source, but I don‚Äôt assume that the pro-open source side will prevail. I hope you will join me in speaking out against SB 1047 and other laws that threaten to stifle open source.\n\n[Original text (with links): https://t.co/whAndl5C2g ]",
  "createdAt": "Thu Jul 11 15:40:53 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 502,
  "replyCount": 132,
  "likeCount": 2175,
  "quoteCount": 84,
  "viewCount": 455274,
  "bookmarkCount": 465,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1811065347841348052",
  "url": "https://x.com/AndrewYNg/status/1811065347841348052",
  "text": "Learn to optimize RAG for cost and performance in our new short course, Prompt Compression and Query Optimization, created with @MongoDB and taught by @richmondalake. \n\nThis course teaches you to combine traditional database capabilities with vector search using MongoDB for RAG. You'll learn these techniques:\n- Vector search: For semantic matching of user queries\n- Filtering using metadata: Pre- and post-filtering to narrow search results\n- Projections: Selecting only necessary fields to minimize data returned\n- Boosting: Reranking results to improve relevance\n- Prompt compression: Using a small LLM to compress context, significantly reducing token count and processing costs\n\nThese methods address scaling, performance, and security challenges in large-scale RAG applications. \n\nYou can sign up here: https://t.co/Z7KwOXlx7i",
  "createdAt": "Wed Jul 10 15:50:01 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 156,
  "replyCount": 18,
  "likeCount": 782,
  "quoteCount": 8,
  "viewCount": 71249,
  "bookmarkCount": 394,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1810338684270768464",
  "url": "https://x.com/AndrewYNg/status/1810338684270768464",
  "text": "As we reach the milestone of the 256th issue of The Batch, I‚Äôm reflecting on how AI has changed over the years and how society continues to change with it. As AI becomes more widely available, it‚Äôs clear that many people ‚Äî developers and non-developers ‚Äî will benefit from high-quality training to keep up with the changes and gain useful AI skills.\n\nIn my years of working in education, I‚Äôve felt that the world has enough low-quality courses, newsletters, social media posts, and other forms of content.  It‚Äôs possible to build a business churning out mediocre content in sufficient volume to attract a meaningful amount of attention, but I have no interest in doing that.\n\nAt https://t.co/zpIxRSuky4, our core philosophy is to put learners first. Our team obsesses about how to create quality training or other programs that benefit people who want to learn about AI. We have intense debates about what tools to teach, which examples to include, even which partners to work with, based on what we think is best for learners.\n\nFor example, I recall vividly how, when working on the Machine Learning Specialization, our team spent ages debating whether to use row or column matrices. Both sides showed up with deep analysis of the pros and cons, made Powerpoint presentations to argue their case, and we spent hours debating over what was better for learners in terms of both ease of picking up the concepts as well as subsequently being able to use these skills with third-party machine learning libraries.\n\nWe don‚Äôt release a course unless we think it‚Äôs a good use of a learner‚Äôs time and we‚Äôd be proud to recommend it to our own friends and family members. Quality, of course, can mean a lot of things. I expect what we do to be technically accurate, useful, up to date, clear, and time-efficient for learners. And, if possible, fun!\n\nWe don‚Äôt always get it right, but we scrutinize learner feedback (one of my most important weekly routines is to study a dashboard that summarizes learner ratings of our courses) and work to make sure our courses serve learners well. And yes, we have a large-language model powered application that reads learner reviews to flag important issues quickly.\n\nEarlier this year, we realized that some of the paid content we had launched was below our quality standard, and that I wouldn‚Äôt in good conscience recommend it to my friends or family members. Despite this content being profitable, we did what we felt was the right thing for learners. So we decided to retire that content and forgo the revenues, but we feel much better now for having done the right thing for learners.\n\nWhen we teach courses with partners, we tell them our priorities are ‚Äúlearners first, partners second, ourselves last.‚Äù I‚Äôm grateful to the many wonderful companies and individuals that work with us to teach cutting-edge techniques, and given an opportunity we try to support our partners‚Äô goals as well. But we never prioritize the interest of our educational partners over that of learners. Fortunately, our partners are onboard with this as well. We have a common goal to serve learners. Without their help, it would be difficult to teach many of the topics we do with high-quality content.\n\nQuite a few companies have tried to offer to pay us to teach a course with them, but we‚Äôve always said no. We work only with the companies that we think help us serve learners best, and are not interested in being paid to teach lower quality courses.\n\nOne reason I obsess about building quality training materials is that I think learning must be a habit. Learning a little every week is important to get through the volume of learning we all need, and additionally to keep up with changing technology. High-quality training that‚Äôs also fun supports a healthy learning habit!\n\nFun fact: In addition to taking online courses, I also read a lot. Recently I noticed that my digital reading app says I‚Äôve been on a reading streak for 170 weeks. I‚Äôve used the app for many years, but apparently I had broken and restarted my streak 170 weeks ago. What happened then? That was the week that my son was born, Coursera became a public company, and my grandfather died. While my life has had disruptions since then, I was happy to find that it takes a disruption of this magnitude to make me pause my learning habit for a week.\n\n[Original text: https://t.co/OXdmYHgrR1 ]",
  "createdAt": "Mon Jul 08 15:42:31 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 64,
  "replyCount": 59,
  "likeCount": 427,
  "quoteCount": 11,
  "viewCount": 64056,
  "bookmarkCount": 90,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1809658174724796583",
  "url": "https://x.com/AndrewYNg/status/1809658174724796583",
  "text": "Shoutout to the team that built https://t.co/sJnTRDfHGF . Really neat site that benchmarks the speed of different LLM API providers to help developers pick which models to use. This nicely complements the LMSYS Chatbot Arena, Hugging Face open LLM leaderboards and Stanford's HELM that focus more on the quality of the outputs. \n\nI hope benchmarks like this encourage more providers to work on fast token generation, which is critical for agentic workflows!",
  "createdAt": "Sat Jul 06 18:38:25 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 482,
  "replyCount": 80,
  "likeCount": 2063,
  "quoteCount": 47,
  "viewCount": 546962,
  "bookmarkCount": 1389,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1806383866472730749",
  "url": "https://x.com/AndrewYNg/status/1806383866472730749",
  "text": "An often overlooked part of the AI supply chain is electricity to power our data centers. \n\n@TheAESCorp is the leading provider of renewable energy to data centers, and is a also global leader in building technology to efficiently scale renewal energy projects. At AI Fund, we're thrilled to work with the visionary @AndresGluski and the AES team to co-build new AI companies that will help with the energy transition!",
  "createdAt": "Thu Jun 27 17:47:29 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 24,
  "replyCount": 15,
  "likeCount": 158,
  "quoteCount": 3,
  "viewCount": 48625,
  "bookmarkCount": 22,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1806008133862805840",
  "url": "https://x.com/AndrewYNg/status/1806008133862805840",
  "text": "As machine learning models grow in size, so too does their carbon footprint. As AI scales, it's important that we quantify and mitigate these emissions. \nIn our new short course Carbon Aware Computing for GenAI Developers you'll learn from @googlecloud  Developer Advocate Nikita Namjoshi how to:\n- Query the ElectricityMaps API for real-time data on regional electricity grid carbon intensity\n- Route your model training jobs to data centers in regions primarily powered by low-carbon sources like wind, solar, hydro and nuclear\n- Measure the carbon footprint of your ML training, inference, storage and API usage with the Google Cloud's Carbon Footprint tool\n- Optimize training job scheduling to run when clean energy is most abundant in a given region.\n\nYou can sign up here: https://t.co/CW3dBUJYfe",
  "createdAt": "Wed Jun 26 16:54:27 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 158,
  "replyCount": 30,
  "likeCount": 787,
  "quoteCount": 10,
  "viewCount": 102839,
  "bookmarkCount": 205,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1803835964604977663",
  "url": "https://x.com/AndrewYNg/status/1803835964604977663",
  "text": "On Father‚Äôs Day last weekend, I sat with my daughter to help her practice solving arithmetic problems. To give her practice problems, I used OpenDevin, an open-source agentic coding framework, to write a Python script that generated questions that she enjoyed answering at her own pace. OpenDevin wrote the code much faster than I could have and genuinely improved my and my daughter‚Äôs day.\n\nSix months ago, coding agents were a novelty. They still frequently fail to deliver, but I find that they‚Äôre now working well enough that they might be genuinely useful to more and more people!\n\nGiven a coding problem that‚Äôs specified in a prompt, the workflow for a coding agent typically goes something like this: Use a large language model (LLM) to analyze the problem and potentially break it into steps to write code for, generate the code, test it, and iteratively use any errors discovered to ask the coding agent to refine its answer. But within this broad framework, a huge design space and numerous innovations are available to experiment with. I‚Äôd like to highlight a few papers that I find notable:\n- ‚ÄúAgentCoder: Multiagent-Code Generation with Iterative Testing and Optimisation,‚Äù Huang et al. (2024).\n- ‚ÄúLDB: A Large Language Model Debugger via Verifying Runtime Execution Step by Step,‚Äù Zhong et al., (2024).\n- ‚ÄúSWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering,‚Äù Yang et al. (2024).\n\nHow can we test the code without requiring the user to write test cases? In a multi-agent system, each ‚Äúagent‚Äù is an LLM prompted to play a particular role. An interesting result from AgentCoder shows that having separate agents for writing code and generating tests results in better performance than letting a single agent do both tasks. This is presumably because, if the agent writing the code is also responsible for writing the tests, the tests might be influenced by the code and fail to consider corner cases that the code does not cover.\n\nWhen people think of testing code, many initially think of output testing, in which we see if the code generates the correct outputs to a specific set of test inputs. If the code fails a test, an LLM can be prompted to reflect on why the code failed and then to try to fix it. In addition to testing the output, the LDB method is helpful. LDB steps through the code and presents to the LLM values of the variables during intermediate steps of execution, to see if the LLM can spot exactly where the error is. This mimics how a human developer might step through the code to see where one of the computational steps went wrong, and so pinpoint and fix the problem.\n\nA lot of agentic workflows mimic human workflows. Similar to other work in machine learning, if humans can do a task, then trying to mimic humans makes development much easier compared to inventing a new process. However, the authors of SWE-agent noticed that many tools that humans use for coding are very inefficient for agents. For example, giving an agent access to a bash shell and having it find a piece of code by executing numerous cd, ls, and cat commands is inefficient, even though humans can do this rapidly. Similarly, visual coding editors like VSCode, emacs, and vim are easy for humans to use, but hard for LLMs (or LMMs) to navigate. Because agents interact with computers differently than humans do, the authors found that building special-purpose tools (functions) to let an agent search, view, and edit codebases resulted in better performance.\n\nOne reason research into coding agents is making rapid progress is that their performance can be evaluated automatically and reliably. With benchmarks like HumanEval, MBPP, and SWE-bench, researchers can try out an idea and automatically test how often it generates correct code. In contrast, even though there‚Äôs considerable activity on AI research agents that search the web and synthesize an article (I‚Äôve enjoyed using the open-source STORM system by Stanford's Yijia Shao et al.), they are hard to evaluate and this makes progress harder.\n\nGithub Copilot was released in 2021, and many developers have been getting coding help by prompting LLMs. The rapid evolution from that to more sophisticated coding agents is expanding how computers can help us with coding tasks, and the pace of progress is rapid. With these tools, I expect programming to become even more fun and more productive.\n\n[Original text (with links): https://t.co/QP1JRomjrZ ]",
  "createdAt": "Thu Jun 20 17:03:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 314,
  "replyCount": 56,
  "likeCount": 1604,
  "quoteCount": 18,
  "viewCount": 217527,
  "bookmarkCount": 1341,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1803812309460189479",
  "url": "https://x.com/AndrewYNg/status/1803812309460189479",
  "text": "Function calling is a powerful way to extend the capabilities of LLMs and AI agents by letting them use external tools. Our new short course Function calling and Data Extraction with LLMs, created with @NexusflowX and taught by @JiantaoJ  and @VenkatKSrini, demonstrates how to prompt LLMs to form calls to external functions. \n\nYou'll work with NexusRavenV2-13B, a 13B parameter open-source model that excels in function calling tasks while still being small enough to host locally. Learn to use function calling to extract structured data from unstructured text and access web APIs, and build an end-to-end application that processes customer service transcripts. You'll learn how to build LLM-powered applications that can analyze feedback, automate data entry, and enhance search. \n\nPlease get started here: https://t.co/FxSp3jv36w",
  "createdAt": "Thu Jun 20 15:29:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 200,
  "replyCount": 98,
  "likeCount": 846,
  "quoteCount": 14,
  "viewCount": 110186,
  "bookmarkCount": 513,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1801295202788983136",
  "url": "https://x.com/AndrewYNg/status/1801295202788983136",
  "text": "One reason for machine learning‚Äôs success is that our field welcomes a wide range of work. I can‚Äôt think of even one example where someone developed what they called a machine learning algorithm and senior members of our community criticized it saying, ‚Äúthat‚Äôs not machine learning!‚Äù Indeed, linear regression using a least-squares cost function was used by mathematicians Legendre and Gauss in the early 1800s ‚Äî long before the invention of computers ‚Äî yet machine learning has embraced these algorithms, and we routinely call them ‚Äúmachine learning‚Äù in introductory courses!\n\nIn contrast, about 20 years ago, I saw statistics departments at a number of universities look at developments in machine learning and say, ‚Äúthat‚Äôs not really statistics.‚Äù This is one reason why machine learning grew much more in computer science than statistics departments. (Fortunately, since then, most statistics departments have become much more open to machine learning.)\n\nThis contrast came to mind a few months ago, as I thought about how to talk about agentic systems that use design patterns such as reflection, tool use, planning, and multi-agent collaboration to produce better results than zero-shot prompting. I had been involved in conversations about whether certain systems should count as ‚Äúagents.‚Äù Rather than having to choose whether or not something is an agent in a binary way, I thought, it would be more useful to think of systems as being agent-like to different degrees. Unlike the noun ‚Äúagent,‚Äù the adjective ‚Äúagentic‚Äù allows us to contemplate such systems and include all of them in this growing movement.\n\nMore and more people are building systems that prompt a large language model multiple times using agent-like design patterns. But there‚Äôs a gray zone between what clearly is not an agent (prompting a model once) and what clearly is (say, an autonomous agent that, given high-level instructions, plans, uses tools, and carries out multiple, iterative steps of processing).\n\nRather than arguing over which work to include or exclude as being a true agent, we can acknowledge that there are different degrees to which systems can be agentic. Then we can more easily include everyone who wants to work on agentic systems. We can also encourage newcomers to start by building simple agentic workflows and iteratively make their systems more sophisticated.\n\nIn the past few weeks, I‚Äôve noticed that, while technical people and non-technical people alike sometimes use the word ‚Äúagent,‚Äù mainly only technical people use the word ‚Äúagentic‚Äù (for now!). So when I see an article that talks about ‚Äúagentic‚Äù workflows, I‚Äôm more likely to read it, since it‚Äôs less likely to be marketing fluff and more likely to have been written by someone who understands the technology.\n\nLet‚Äôs keep working on agentic systems and keep welcoming anyone who wants to join our field!\n\n[Original text: https://t.co/4izf1hsv9P ]",
  "createdAt": "Thu Jun 13 16:46:57 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 204,
  "replyCount": 59,
  "likeCount": 1130,
  "quoteCount": 23,
  "viewCount": 178391,
  "bookmarkCount": 430,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1801277928740970619",
  "url": "https://x.com/AndrewYNg/status/1801277928740970619",
  "text": "New short course: Building Your Own Database Agent, created with Microsoft @Azure's @adriangs86. You'll learn to build an AI assistant that translates natural language questions into SQL queries. Querying with natural language empowers everyone in your organization, from business leaders to developers, to access data insights directly, using plain English.\n\nYou‚Äôll use the Azure OpenAI Service and LangChain to implement retrieval augmented generation and function calling, and leverage the Assistants API. You'll gain hands-on experience building an agent that can reason over both CSV files and SQL databases.\n\nPlease sign up here! https://t.co/9NxN8pJYKC",
  "createdAt": "Thu Jun 13 15:38:18 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 216,
  "replyCount": 20,
  "likeCount": 1052,
  "quoteCount": 21,
  "viewCount": 109282,
  "bookmarkCount": 781,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1800582171259982289",
  "url": "https://x.com/AndrewYNg/status/1800582171259982289",
  "text": "I think AI agentic machine translation has huge potential for improving over traditional neural machine translation, and am releasing as open-source a demonstration I'd been playing with as a fun weekend project.\n\nUsing an agentic workflow, this demonstration (i) Prompts an LLM to translate from one language to another, (ii) Reflects on the translation to come up with constructive suggestions, (iii) Uses the suggestions to refine the translation. In our limited testing, this is sometimes competitive with, and sometimes worse than, leading commercial providers.\n\nBut it gives a highly steerable translation system where by simply changing the prompt, you can specify the tone (formal/informal), regional variation (do you want Spanish as spoken in Spain or as spoken in Latin America?), and ensure consistent translation of terms (by providing a glossary).\n\nThis is not mature software. But I hope the open-source community can make agentic translation work much better. Given how a simple reflection workflow already gives decent results, I think there's significant headroom to make agentic translation much better.\n\nReleasing an early software prototype like this is something new I decided to try to see if it is helpful to the developer community. I'd love any feedback on this.\n\nThanks to Joaquin Dominguez, @nedteneva, @JohnSanterre for help with this.\n\nhttps://t.co/nghC3wN3Id",
  "createdAt": "Tue Jun 11 17:33:37 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 348,
  "replyCount": 87,
  "likeCount": 1736,
  "quoteCount": 52,
  "viewCount": 544536,
  "bookmarkCount": 1363,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1798753608974139779",
  "url": "https://x.com/AndrewYNg/status/1798753608974139779",
  "text": "The effort to protect innovation and open source continues. I believe we‚Äôre all better off if anyone can carry out basic AI research and share their innovations. Right now, I‚Äôm deeply concerned about California's proposed law SB-1047. It‚Äôs a long, complex bill with many parts that require safety assessments, shutdown capability for models, and so on.\n\nThere are many things wrong with this bill, but I‚Äôd like to focus here on just one: It defines an unreasonable ‚Äúhazardous capability‚Äù designation that may make builders of large AI models liable if someone uses their models to do something that exceeds the bill‚Äôs definition of harm (such as causing $500 million in damage). That is practically impossible for any AI builder to ensure. If the bill is passed in its present form, it will stifle AI model builders, especially open source developers.\n\nSome AI applications, for example in healthcare, are risky. But as I wrote previously, regulators should regulate applications rather than technology.\n- Technology refers to tools that can be applied in many ways to solve various problems.\n- Applications are specific implementations of technologies designed to meet particular customer needs.\n\nFor example, an electric motor is a technology. When we put it in a blender, an electric vehicle, dialysis machine, or guided bomb, it becomes an application. Imagine if we passed laws saying, if anyone uses a motor in a harmful way, the motor manufacturer is liable. Motor makers would either shut down or make motors so tiny as to be useless for most applications. If we pass such a law, sure, we might stop people from building guided bombs, but we‚Äôd also lose blenders, electric vehicles, and dialysis machines. In contrast, if we look at specific applications, like blenders, we can more rationally assess risks and figure out how to make sure they‚Äôre safe, and even ban classes of applications, like certain types of munitions.\n\nSafety is a property of the application, not a property of the technology (or model), as @random_walker and @sayashk have pointed out. Whether a blender is a safe one can‚Äôt be determined by examining the electric motor. A similar argument holds for AI.\n\nSB-1047 doesn‚Äôt account for this distinction. It ignores the reality that the number of beneficial uses of AI models is, like electric motors, vastly greater than the number of harmful ones. But, just as no one knows how to build a motor that can‚Äôt be used to cause harm, no one has figured out how to make sure an AI model can‚Äôt be adapted to harmful uses. In the case of open source models, there‚Äôs no known defense to fine-tuning to remove RLHF alignment. And jailbreaking work has shown that even closed-source, proprietary models that have been properly aligned can be attacked in ways that make them give harmful responses. Indeed, the sharp-witted @elder_plinius regularly tweets about jailbreaks for closed models. Kudos also to Anthropic‚Äôs @cem__anil and collaborators for publishing their work on many-shot jailbreaking, an attack that can get leading large language models to give inappropriate responses and is hard to defend against.\n\nCalifornia has been home to a lot of innovation in AI. I‚Äôm worried that this anti-competitive, anti-innovation proposal has gotten so much traction in the legislature. Worse, other jurisdictions often follow California, and it would be awful if they were to do so in this instance.\n\nSB-1047 passed in a key vote in the State Senate in May, but it still has additional steps before it becomes law. I hope you will speak out against it if you get a chance to do so.\n\n[Original text (with links): https://t.co/MOQqFF6cID ]",
  "createdAt": "Thu Jun 06 16:27:34 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 782,
  "replyCount": 163,
  "likeCount": 3331,
  "quoteCount": 136,
  "viewCount": 1119357,
  "bookmarkCount": 881,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1798378861337723039",
  "url": "https://x.com/AndrewYNg/status/1798378861337723039",
  "text": "New AI Agentic course! Learn to use LangGraph to build single and multi-agent LLM applications in AI Agents in LangGraph. This short course, taught by LangChain @LangChainAI  founder Harrison Chase @hwchase17 and @tavilyai founder @weiss_rotem, shows how to integrate agentic search to enhance an agent's knowledge with query-focused answers in predictable formats. Also learn to implement agentic memory to save state for reasoning and debugging, and see how human-in-the-loop input can guide agents at key junctures. \n\nYou'll build an agent from scratch, then reconstruct it with LangGraph to thoroughly understand the framework. Finally, you'll build a sophisticated essay-writing agent that incorporates all the learnings from the course.\n\nSign up here! https://t.co/ZDpjLmdyDL",
  "createdAt": "Wed Jun 05 15:38:27 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 215,
  "replyCount": 57,
  "likeCount": 1134,
  "quoteCount": 16,
  "viewCount": 151103,
  "bookmarkCount": 826,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1798063159787577843",
  "url": "https://x.com/AndrewYNg/status/1798063159787577843",
  "text": "We just released a new climate emulator to explore the application of Stratospheric Aerosol Injection (SAI) to mitigate global warming!\n\nSAI uses reflective particles in the atmosphere to reflect sunlight and thereby cool Earth‚Äôs surface. Our emulator lets you explore how different ways to apply SAI might affect average global temperature.\n\nPlease check out the emulator at https://t.co/OxtaQMyDuL.\n\nSAI is a promising direction, but we still need more research to better understand its impact and potential implementation.\n\nBig thanks to collaborators @jeremy_irvin16 @DanVisioni Ben Kravitz @dakotagruener @chrisroadmap and @DWatsonParris",
  "createdAt": "Tue Jun 04 18:43:58 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 163,
  "replyCount": 95,
  "likeCount": 987,
  "quoteCount": 37,
  "viewCount": 140885,
  "bookmarkCount": 268,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1796206876805489105",
  "url": "https://x.com/AndrewYNg/status/1796206876805489105",
  "text": "A barrier to faster progress in generative AI is evaluations (evals), particularly of custom AI  applications that generate free-form text. Let‚Äôs say you have a multi-agent research system that includes a researcher agent and a writer agent. Would adding a fact-checking agent improve the results? If we can‚Äôt efficiently evaluate the impact of such changes, it‚Äôs hard to know which changes to keep.\n\nFor evaluating general-purpose foundation models such as large language models (LLMs) ‚Äî which are trained to respond to a large variety of prompts ‚Äî we have standardized tests like MMLU (multiple-choice questions that cover 57 disciplines like math, philosophy, and medicine) and HumanEval (testing code generation); the LMSYS Chatbot arena, which pits two LLMs‚Äô responses against each other and asks a human to judge which response is superior; and large-scale benchmarking like HELM. These evaluation tools took considerable effort to build, and they are invaluable for giving LLM users a sense of different models' relative performance. Nonetheless, they have limitations: For example, leakage of benchmarks datasets‚Äô questions and answers into training data is a constant worry, and human preference for certain answers does not mean those answers are more accurate.\n\nIn contrast, our current options for evaluating specific applications built using LLMs are far more limited. Here, I see two major types of applications.\n- For applications designed to deliver unambiguous, right-or-wrong responses, we have reasonable options. Let‚Äôs say we want an LLM to read a resume and extract the candidate's most recent job title, or read a customer email and route it to the right department. We can create a test set that comprises ground-truth labeled examples with the right responses, and measure the percentage of times the LLM generates the right output. The main bottleneck is creating the labeled test set, which is expensive but surmountable.\n- But many LLM-based applications generate free-text output with no single right response. For example, if we ask an LLM to summarize customer emails, there‚Äôs a multitude of possible good (and bad) responses. The same holds for an agentic system to do web research and write an article about a topic, or a RAG system for answering questions. It‚Äôs impractical to hire an army of human experts to read the LLM‚Äôs outputs every time we tweak the algorithm and evaluate if the answers have improved ‚Äî we need an automated way to test the outputs. Thus, many teams use an advanced language model to evaluate outputs. In the customer email summarization example, we might design an evaluation rubric (scoring criteria) for what makes a good summary. Given an email summary generated by our system, we might prompt an advanced LLM to read it and score it according to our rubric. I‚Äôve found that the results of such a procedure, while better than nothing, can also be noisy ‚Äî sometimes too noisy to reliably tell me if the way I‚Äôve tweaked an algorithm is good or bad.\n\nThe cost of running evals poses an additional challenge. Let‚Äôs say you‚Äôre using an LLM that costs $10 per million input tokens, and a typical query has 1000 tokens. Each user query therefore costs only $0.01. However, if you iteratively work to improve your algorithm based on 1000 test examples, and if in a single day you evaluate 20 ideas, then your cost will be 20*1000*0.01 = $200. For many projects I‚Äôve worked on, the development costs were fairly negligible until we started doing evals, whereupon the costs suddenly increased. (If the product turned out to be successful, then costs increased even more at deployment, but that was something we were happy to see!)\n\nIn addition to the dollar cost, evals also have a significant time cost. Running evals on 1000 examples might take tens of minutes or even hours. Time spent waiting for eval jobs to finish also slows down the speed with which we can experiment and iterate over new ideas. Previously I wrote that fast, inexpensive token generation is critical for agentic workflows. This will also be useful for evals, which involve nested for-loops that iterate over a test set and different model/hyperparameter/prompt choices and therefore consume large numbers of tokens.\n\nDespite the limitations of today's eval methodologies, I‚Äôm optimistic that our community will invent better techniques (maybe involving agentic workflows like reflection?) for getting LLMs to evaluate such output.\n\nIf you‚Äôre a developer or researcher and have ideas along these lines, I hope you‚Äôll keep working on them and consider open sourcing or publishing your findings!\n\n[Original text: https://t.co/HXtzJH7eP8 ]",
  "createdAt": "Thu May 30 15:47:45 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 162,
  "replyCount": 60,
  "likeCount": 881,
  "quoteCount": 26,
  "viewCount": 186788,
  "bookmarkCount": 597,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1795845101979406490",
  "url": "https://x.com/AndrewYNg/status/1795845101979406490",
  "text": "New Agentic AI short course! AI Agentic Design Patterns with AutoGen, taught by @MSFTResearch's @Chi_Wang_ and @penn_state's @qingyun_wu, shows you how to use AutoGen to implement agentic design patterns like multi-agent collaboration, sequential and nested chat, reflection, tool use, and planning. Learn how to build and combine multiple specialized agents ‚Äì such as researchers, planners, coders, writers, and critics ‚Äì that interact to execute complex workflows, like generating detailed financial reports, that would otherwise have taken extensive manual effort.\n\nThis course illustrates key agentic design principles with many fun demonstrations. For example, you'll build a conversational chess game using two player agents, each of which can use a tool to validate moves and update the board state, while engaging in lively banter about the game!\n\nI've enjoyed using AutoGen, and think you will too. Sign up to get started here:  https://t.co/C0tsLuilMf",
  "createdAt": "Wed May 29 15:50:12 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 294,
  "replyCount": 60,
  "likeCount": 1496,
  "quoteCount": 26,
  "viewCount": 215378,
  "bookmarkCount": 1143,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1793760961343701045",
  "url": "https://x.com/AndrewYNg/status/1793760961343701045",
  "text": "@greg_karsten Thank you for sharing this -- I think a lot of people feel similarly as you. Please keep tinkering, and from your description it sounds to me like you're making good progress! üéâ",
  "createdAt": "Thu May 23 21:48:34 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 0,
  "likeCount": 12,
  "quoteCount": 0,
  "viewCount": 2132,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1793673520863715396",
  "url": "https://x.com/AndrewYNg/status/1793673520863715396",
  "text": "A good way to get started in AI is to start with coursework, which gives a systematic way to gain knowledge, and then to work on projects. For many who hear this advice, ‚Äúprojects‚Äù may evoke a significant undertaking that delivers value to users. But I encourage you to set a lower bar and relish small, weekend tinkering projects that let you learn, even if they don‚Äôt result in a meaningful deliverable.\n\nRecently, my son and daughter (ages 3 and 5) were building Lego vehicles. They built a beautiful ice-cream truck as well as a . . . umm . . . colorful and asymmetric dinosaur car, shown in the picture below. While most observers would judge the ice-cream truck as the superior creation, my kids built it by following Lego‚Äôs instructions, and it is likely identical to thousands of ice-cream trucks built by others. In contrast, building the dinosaur car required creativity and novel thinking. The exercise helped them hone their ability to pick and assemble Lego building blocks.\n\nThere is, of course, room for both mimicking others‚Äô designs (with permission) and coming up with your own. As a parent, I try to celebrate both. (To be honest, I celebrated the dinosaur car more.) When learning to build Lego, it‚Äôs helpful to start by following a template. But eventually, building your own unique projects enriches your skills.\n\nAs a developer, too, I try to celebrate unique creations. Yes, it is nice to have beautiful software, and the impact of the output does matter. But good software is often written by people who spend many hours tinkering and building things. By building unique projects, you master key software building blocks. Then, using those blocks, you can go on to build bigger projects\n\nI routinely tinker with building AI applications, and a lot of my tinkering doesn‚Äôt result in anything useful. My latest example: I built a Streamlit app that would authenticate to Google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. I didn‚Äôt find it useful in the end because of friction in the user interface, and I‚Äôm sure a commercial provider will soon, if they haven‚Äôt already, build a better product than I was able to throw together in a couple of hours on a weekend. But such tinkering helps me hone my intuition and master software components (I now know how to programmatically interface with Google docs) that might be useful in future projects.\n\nIf you have an idea for a project, I encourage you to build it! Often, working on a project will also help you decide what additional skills to learn, perhaps through coursework. To sustain momentum, it helps to find friends with whom to talk about ideas and celebrate projects ‚Äî large or small.\n\n[Original text: https://t.co/i21oCaQpDc ]",
  "createdAt": "Thu May 23 16:01:06 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 141,
  "replyCount": 38,
  "likeCount": 897,
  "quoteCount": 12,
  "viewCount": 91383,
  "bookmarkCount": 281,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1792986667852374386",
  "url": "https://x.com/AndrewYNg/status/1792986667852374386",
  "text": "Thank you! It‚Äôs been a privilege for Landing AI to work with you @RamaswmySridhar, \n@jeffhollan, and the whole Snowflake team. Many businesses have a lot of image data, and integrating our vision software to run as a native app in Snowpark Container Services makes it easy for users to get insights out of their vision data stored in Snowflake. @danmaloney and I also look forward to seeing you at the Summit!",
  "createdAt": "Tue May 21 18:31:48 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 2,
  "replyCount": 2,
  "likeCount": 11,
  "quoteCount": 0,
  "viewCount": 2076,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1792919935691214899",
  "url": "https://x.com/AndrewYNg/status/1792919935691214899",
  "text": "Learn to deploy AI models to edge devices in our new short course Introduction to On-Device AI, created with @Qualcomm and taught by Senior Director of Engineering @krishna_srd.\n\nI think on-device (edge) AI is an important technology trend that's enabling new low latency, privacy-preserving applications. In this course, you'll deploy a real-time image segmentation model on-device, and through this learn key steps for on-device deployment: Neural Network graph capture, on-device compilation, hardware acceleration, and validating on-device numerical correctness. You'll also see how quantization can make your model up to 4x faster and 4x smaller, and thereby improve its performance on resource-constrained edge devices.\n\nThe techniques covered are used to deploy models on numerous device types including smartphones, drones, and robots, and are enabling many new, creative applications. \n\nPlease sign up here: https://t.co/V4mS6E7dXx",
  "createdAt": "Tue May 21 14:06:38 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 199,
  "replyCount": 34,
  "likeCount": 981,
  "quoteCount": 14,
  "viewCount": 112667,
  "bookmarkCount": 508,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1791134037178020308",
  "url": "https://x.com/AndrewYNg/status/1791134037178020308",
  "text": "This week, Google announced a doubling of Gemini Pro 1.5's input context window from 1 million to 2 million tokens, and OpenAI released GPT-4o, which generates tokens 2x faster and 50% cheaper than GPT-4 Turbo and natively accepts and generates multimodal tokens. I view these developments as the latest in an 18-month trend. Given the improvements we've seen, best practices for developers have changed as well.\n\nSince the launch of ChatGPT in Nov 2022, with key milestones that include the releases of GPT-4, Gemini 1.5 Pro, Claude 3 Opus, and Llama 3-70B, many model providers have improved their capabilities in two important ways: (i) reasoning, which allows LLMs to think through complex concepts and and follow complex instructions; and (ii) longer input context windows. \n\nThe reasoning capability of GPT-4 and other advanced models makes them quite good at interpreting complex prompts with detailed instructions. Many people are used to dashing off a quick, 1- to 2-sentence query to an LLM. In contrast, when building applications, I see sophisticated teams frequently writing prompts that might be 1 to 2 pages long (my teams call them ‚Äúmega-prompts‚Äù) that provide complex instructions to specify in detail how we‚Äôd like an LLM to perform a task. I still see teams not going far enough in terms of writing detailed instructions. For an example of a moderately lengthy prompt, take a look at Claude 3‚Äôs system prompt. It‚Äôs detailed and gives clear guidance on how Claude should behave. \n\nThis is a very different style of prompting than we typically use with LLMs‚Äô web user interfaces, where we might dash off a quick query and, if the response is unsatisfactory, clarify what we want through repeated conversational turns with the chatbot.\n\nFurther, the increasing length of input context windows has added another technique to the developer‚Äôs toolkit. GPT-3 kicked off a lot of research on few-shot in-context learning. For example, if you‚Äôre using an LLM for text classification, you might give a handful ‚Äî say 1 to 5 examples ‚Äî of text snippets and their class labels, so that it can use those examples to generalize to additional texts. However, with longer input context windows ‚Äî GPT-4o accepts 128,000 input tokens, Claude 3 Opus 200,000 tokens, and Gemini 1.5 Pro 1 million tokens (2 million just announced in a limited preview) ‚Äî LLMs aren‚Äôt limited to a handful of examples. With many-shot learning, developers can give dozens, even hundreds of examples in the prompt, and this works better than few-shot learning. \n\nWhen building complex workflows, I see developers getting good results with this process: \n- Write quick, simple prompts and see how it does.\n- Based on where the output falls short, flesh out the prompt iteratively. This often leads to a longer, more detailed, prompt, perhaps even a mega-prompt.\n- If that‚Äôs still insufficient, consider few-shot or many-shot learning (if applicable) or, less frequently, fine-tuning.\n- If that still doesn‚Äôt yield the results you need, break down the task into subtasks and apply an agentic workflow.\n\nI hope a process like this will help you build applications more easily. If you‚Äôre interested in taking a deeper dive into prompting strategies, I recommend Microsoft's Medprompt paper (Nori et al., 2023), which lays out a complex set of prompting strategies that can lead to very good results.\n\n[Original text (with links): https://t.co/UOtLDza1Vh ]",
  "createdAt": "Thu May 16 15:50:06 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 553,
  "replyCount": 74,
  "likeCount": 2873,
  "quoteCount": 66,
  "viewCount": 505103,
  "bookmarkCount": 2067,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1790769732146307308",
  "url": "https://x.com/AndrewYNg/status/1790769732146307308",
  "text": "New agentic short course! Multi AI Agent Systems with crewAI, built with @crewAIInc's founder and CEO @joaomdmoura. In this course, you'll learn how to break down complex tasks into subtasks for multiple AI agents, each playing a specialized role, to execute. \n\nFor example, to generate a research report, you might have researcher, writer, and quality assurance agents collaborate. You'll define the roles, expectations, and interactions between the agents‚Äîlike a manager organizing a team.\n\nYou'll work with key agentic AI techniques like role-playing, tool use, memory, guardrails, and cross-agent collaboration. And you'll build your own multi-agent systems that can tackle complex tasks. I think you'll find it both productive and fun to design agents and watch them collaborate to get things done.\n\nLet me know what you think! I believe multi-agent architectures will drive significant progress in AI systems.\n\nPlease sign up here! https://t.co/0rObe4feBz",
  "createdAt": "Wed May 15 15:42:29 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 278,
  "replyCount": 85,
  "likeCount": 1490,
  "quoteCount": 46,
  "viewCount": 348115,
  "bookmarkCount": 1255,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1790500978279776450",
  "url": "https://x.com/AndrewYNg/status/1790500978279776450",
  "text": "Congratulations to all my Google friends for the cool announcements at I/O! \n\nI'm personally looking forward to Gemini with 2 million token input context window and better support for on-device AI -- should open up new opportunities for application builders!",
  "createdAt": "Tue May 14 21:54:33 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 72,
  "replyCount": 36,
  "likeCount": 1061,
  "quoteCount": 6,
  "viewCount": 112230,
  "bookmarkCount": 68,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1790088683259048120",
  "url": "https://x.com/AndrewYNg/status/1790088683259048120",
  "text": "Congrats to OpenAI for the release of GPT-4o! 2x faster and 50% cheaper tokens will be great for everyone using agentic AI workflows. \n\nWhen an agentic job that used to take 10min now takes 5min just by switching APIs, that's great progress!",
  "createdAt": "Mon May 13 18:36:14 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 222,
  "replyCount": 46,
  "likeCount": 1934,
  "quoteCount": 7,
  "viewCount": 184926,
  "bookmarkCount": 158,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1790050852776112439",
  "url": "https://x.com/AndrewYNg/status/1790050852776112439",
  "text": "New short course: Building Multimodal Search and RAG\", by @weaviate_io's  @sebawita.\n\nContrastive learning is used to train models to map vectors into an embedding space by pulling similar concepts closer together and pushing dissimilar concepts away from each other. This technique is also used to train multimodal embedding models that capture semantic similarity across different modalities like text, images, and audio. These multimodal embeddings can be used to build multimodal search and RAG systems.\n\nIn this course, you'll learn how contrastive learning works, and how to add multimodality to RAG ‚Äì so your models can draw on diverse, relevant context to answer questions. For example, a query about a financial report might synthesize information from text snippets, graphs, tables, and slides. You will also learn how visual instruction tuning lets you integrate image understanding into language models, and build a multi-vector recommender system using Weaviate‚Äôs open-source vector database.\n\nPlease sign up here: https://t.co/IVULLqbdOD",
  "createdAt": "Mon May 13 16:05:55 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 179,
  "replyCount": 19,
  "likeCount": 836,
  "quoteCount": 9,
  "viewCount": 103888,
  "bookmarkCount": 531,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1788648531873628607",
  "url": "https://x.com/AndrewYNg/status/1788648531873628607",
  "text": "Last week, I spoke about AI and regulations at an event at the U.S. Capitol attended by legislative and business leaders. I‚Äôm encouraged by the progress the open source community has made fending off regulations that would have stifled innovation. But opponents of open source are continuing to shift their arguments, with the latest worries centering on open source's impact on national security. I hope we‚Äôll all keep protecting open source!\n\nBased on my conversations with legislators, I‚Äôm encouraged by the progress the U.S. federal government has made getting a realistic grasp of AI‚Äôs risks. To be clear, guardrails are needed. But they should be applied to AI applications, not to general-purpose AI technology.\n\nNonetheless, some companies are eager to limit open source, possibly to protect the value of massive investments they‚Äôve made in proprietary models and to deter competitors. It has been fascinating to watch their arguments change over time.\n\nFor instance, about 12 months ago, the Center For AI Safety‚Äôs ‚ÄúStatement on AI Risk‚Äù warned that AI could cause human extinction and stoked fears of AI taking over. This alarmed leaders in Washington. But many people in AI pointed out that this dystopian science-fiction scenario had little basis in reality. About six months later, when I testified at the U.S. Senate‚Äôs AI Insight forum, legislators no longer worried much about an AI takeover.\n\nThen the opponents of open source shifted gears. Their leading argument shifted to the risk of AI helping to create bioweapons. Soon afterward, OpenAI and RAND showed that current AI does not significantly increase the ability of malefactors to build bioweapons. This fear of AI-enabled bioweapons has diminished. To be sure, the possibility that bad actors could use bioweapons ‚Äî with or without AI ‚Äî remains a topic of great international concern.\n\nThe latest argument for blocking open source AI has shifted to national security. AI is useful for both economic competition and warfare, and open source opponents say the U.S. should make sure its adversaries don‚Äôt have access to the latest foundation models. While I don‚Äôt want authoritarian governments to use AI, particularly to wage unjust wars, the LLM cat is out of the bag, and authoritarian countries will fill the vacuum if democratic nations limit access. When, some day, a child asks an AI system questions about democracy, the role of a free press, or the function of an independent judiciary in preserving the rule of law, I would like the AI to reflect democratic values rather than favor authoritarian leaders‚Äô goals over, say, human rights.\n\nI came away from Washington optimistic about the progress we‚Äôve made. A  year ago, legislators seemed to me to spend 80% of their time talking about guardrails for AI and 20% about investing in innovation. I was delighted that the ratio has flipped, and there was far more talk of investing in innovation.\nLooking beyond the U.S. federal government, there are many jurisdictions globally. Unfortunately, arguments in favor of  regulations that would stifle AI development continue to proliferate. But I‚Äôve learned from my trips to Washington and other nations‚Äô capitals that talking to regulators does have an impact. If you have a chance to talk to a regulator at any level, I hope you‚Äôll do what you can to help governments better understand AI.\n\n[Original text (with links): https://t.co/tw2iT0ooLT ]",
  "createdAt": "Thu May 09 19:13:36 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 253,
  "replyCount": 81,
  "likeCount": 1137,
  "quoteCount": 51,
  "viewCount": 290058,
  "bookmarkCount": 244,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1788246239517282795",
  "url": "https://x.com/AndrewYNg/status/1788246239517282795",
  "text": "I‚Äôm excited to kick off the first of our short courses focused on agents, starting with Building Agentic RAG with LlamaIndex, taught by @jerryjliu0, CEO of @llama_index.\n\nThis covers an important shift in RAG (retrieval augmented generation), in which rather than having the developer write explicit routines to retrieve information to feed into the LLM context, we instead build a RAG agent that that has access to tools for retrieving information. This lets the agent decide what information to fetch, and enables it to answer more complex questions using multi-step reasoning.\n\nIn detail, you'll learn about:\n- Routing: Where your agent will use decision-making to route requests to multiple tools.\n- Tool Use: Where you'll create an interface for agents to select what tool (function call) to use as well as generate the right arguments.\n- Multi-step reasoning with tool use: Where you'll use an LLM to carry out multiple steps of reasoning, while retaining memory throughout the process.\n\nYou‚Äôll also learn how to step through what your agent is doing to debug and improve it iteratively.\n\nIt‚Äôs an exciting time to build agents. Sign up and get started here! https://t.co/sHhzRRJG0l",
  "createdAt": "Wed May 08 16:35:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 227,
  "replyCount": 24,
  "likeCount": 1247,
  "quoteCount": 21,
  "viewCount": 296183,
  "bookmarkCount": 969,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1787525611864695148",
  "url": "https://x.com/AndrewYNg/status/1787525611864695148",
  "text": "Have you used quantization with an open source machine learning library, and wondered how quantization works? How can you preserve model accuracy as you compress from 32 bits to 16, 8, or even 2 bits? In our new short course, Quantization in Depth, taught by @huggingface's @_marcsun and @younesbelkada, you'll learn to implement variants of linear quantization, such as asymmetric and symmetric modes, from scratch. You'll also quantize at different granularities (per-tensor, per-channel, per-group) to maintain performance. You‚Äôll then construct a quantizer to compress any open source deep learning model‚Äôs dense layers to 8-bit precision. Finally, you‚Äôll practice quantizing weights into 2 bits by packing four 2-bit weights into a single 8-bit integer.\n\nIf you've ever run a large open source model on your laptop, you've likely benefited from someone's work in quantization. Come learn how this key technique works under the hood!\n\nPlease sign up here: https://t.co/lPfRY0LdFI",
  "createdAt": "Mon May 06 16:51:31 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 191,
  "replyCount": 22,
  "likeCount": 1174,
  "quoteCount": 15,
  "viewCount": 197942,
  "bookmarkCount": 745,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1787219405916828001",
  "url": "https://x.com/AndrewYNg/status/1787219405916828001",
  "text": "Back then, the idea that scaling deep learning would lead to significant performance gains was controversial. Several senior academic colleagues were advising me not to waste time trying to scale deep learning, but to just focus on inventing new algorithms, which was where they thought the action was! \n\nFortunately, I already had small scale data from my Stanford group (thanks to @adampaulcoates, @honglaklee, and many others) that gave me conviction that scaling was going to work, and we just kept pushing in that direction.",
  "createdAt": "Sun May 05 20:34:45 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 4,
  "quoteCount": 0,
  "viewCount": 798,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1787216859621958078",
  "url": "https://x.com/AndrewYNg/status/1787216859621958078",
  "text": "@quocleix @ylecun @JeffDean @deliprao @karpathy Yup. By the way @quocleix, I still remember the time we were both in the office, and you waved at me to come over to your desk to look at your new result. And right there on your monitor was the now-famous Google Cat image. That was a defining moment for Google Brain!",
  "createdAt": "Sun May 05 20:24:38 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 3,
  "replyCount": 3,
  "likeCount": 52,
  "quoteCount": 1,
  "viewCount": 14228,
  "bookmarkCount": 6,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1787209990023082105",
  "url": "https://x.com/AndrewYNg/status/1787209990023082105",
  "text": "@unJADded @JeffDean @ylecun @deliprao @karpathy You were a real pioneer @unJADded ! ‚ù§Ô∏è\n\nLooking back, honestly I feel a bit bad at how hard DistBelief was to use, and the complexity of the C++ interface we built.... Nonetheless, I'm glad it turned out to be a useful product of its time!",
  "createdAt": "Sun May 05 19:57:21 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 19,
  "quoteCount": 3,
  "viewCount": 4677,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1787204296590987630",
  "url": "https://x.com/AndrewYNg/status/1787204296590987630",
  "text": "I'm with @JeffDean on this. DistBelief taught us early important lessons about scaling up deep learning, and it was general enough for many algorithms including supervised backprop. \n\nObviously, we got a lot of software and hardware architecture details \"wrong\" back in 2012 -- but who didn't? But DistBelief was used by numerous teams within Google, taught early lessons about scaling deep learning, and became the precursor to TensorFlow. \n\nFun fact: Back then, \"Deep Belief Networks\" (by Geoff Hinton) was a popular algorithm. When we built our Distributed training system, it was Jeff that came up with the name DistBelief, which I thought was a real groaner. I guess we were hoping our results would be so good that people would look at them with disbelief. üòÄ",
  "createdAt": "Sun May 05 19:34:43 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 10,
  "replyCount": 3,
  "likeCount": 230,
  "quoteCount": 0,
  "viewCount": 46321,
  "bookmarkCount": 51,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1787199044194070776",
  "url": "https://x.com/AndrewYNg/status/1787199044194070776",
  "text": "Link to original article:  https://t.co/9CEoDsWYgh",
  "createdAt": "Sun May 05 19:13:51 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 5,
  "replyCount": 6,
  "likeCount": 45,
  "quoteCount": 0,
  "viewCount": 26866,
  "bookmarkCount": 5,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1787198521747308637",
  "url": "https://x.com/AndrewYNg/status/1787198521747308637",
  "text": "I'm glad the Washington Post's editorial board is pushing for governments to engage in exploring climate geoengineering. I believe AI climate modeling has an important role to play. \n\nHere's the situation as I see it:\n- Earth is on track to a catastrophic 2-4 degrees Celsius of warming. The article references the UN Environment Program's estimate of 2.9 degrees on the current trajectory.\n- We have a high degree of confidence that geoengineering via stratospheric aerosol injection (SAI) will significantly lower Earth's average surface temperature. The science here is really solid: Use aerosols in the atmosphere to reflect more sunlight away from earth, and we become cooler.\n- So, why don't we just do it? Multiple reasons, but the biggest is that we still don't have good models for estimating how it will affect local climate and weather patterns, even though we're confident global average surface temperature will go down. \n\nThat's why better AI climate modeling is crucial for reducing uncertainty and better understanding the impacts of various geoengineering strategies.\n\nThere're other issues to consider too, like governance, moral hazard (disincentivizing decarbonization), equity,  pollution from the aerosols, and the challenging implementation engineering. But many of these  problems become easier if we can make progress on the core problem of better understanding what impact SAI will have.",
  "createdAt": "Sun May 05 19:11:46 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 96,
  "replyCount": 63,
  "likeCount": 452,
  "quoteCount": 15,
  "viewCount": 112120,
  "bookmarkCount": 134,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1786057567178834328",
  "url": "https://x.com/AndrewYNg/status/1786057567178834328",
  "text": "Inexpensive token generation and agentic workflows for large language models (LLMs) open up intriguing new possibilities for training LLMs on synthetic data. Pretraining an LLM on its own directly generated responses to prompts doesn't help. But if an agentic workflow implemented with the LLM results in higher quality output than the LLM can generate directly, then training on that output becomes potentially useful.\n\nJust as humans can learn from their own thinking, perhaps LLMs can, too. For example, imagine a math student who is learning to write mathematical proofs. By solving a few problems ‚Äî even without external input ‚Äî they can reflect on what does and doesn‚Äôt work and, through practice, learn how to more quickly generate good proofs.\n\nBroadly, LLM training involves (i) pretraining (learning from unlabeled text data to predict the next word) followed by (ii) instruction fine-tuning (learning to follow instructions) and (iii) RLHF/DPO tuning to align the LLM‚Äôs output to human values. Step (i) requires many orders of magnitude more data than the other steps. For example, Llama 3 was pretrained on over 15 trillion tokens, and LLM developers are still hungry for more data. Where can we get more text to train on?\n\nMany developers train smaller models directly on the output of larger models, so a smaller model learns to mimic a larger model‚Äôs behavior on a particular task. However, an LLM can‚Äôt learn much by training on data it generated directly, just like a supervised learning algorithm can‚Äôt learn from trying to predict labels it generated by itself. Indeed, training a model repeatedly on the output of an earlier version of itself can result in model collapse.\n\nHowever, an LLM wrapped in an agentic workflow may produce higher-quality output than it can generate directly. In this case, the LLM‚Äôs higher-quality output might be useful as pretraining data for the LLM itself.\nEfforts like these have precedents:\n- When using  reinforcement learning to play a game like chess, a model might learn a function that evaluates board positions. If we apply game tree search along with a low-accuracy evaluation function, the model can come up with more accurate evaluations. Then we can train that evaluation function to mimic these more accurate values.\n- In the alignment step, Anthropic‚Äôs constitutional AI method uses RLAIF (RL from AI Feedback) to judge the quality of LLM outputs, substituting feedback generated by an AI model for human feedback.\n\nA significant barrier to using LLMs prompted via agentic workflows to produce their own training data is the cost of generating tokens. Say we want to generate 1 trillion tokens to extend a pre-existing training dataset. Currently, at publicly announced prices, generating 1 trillion tokens using GPT-4-turbo ($30 per million output tokens), Claude 3 Opus ($75), Gemini 1.5 Pro ($21), and Llama-3-70B on Groq ($0.79) would cost, respectively, $30M, $75M, $21M and $790K. Of course, an agentic workflow that uses a design pattern like Reflection would require generating more than one token per token that we would use as training data. But budgets for training cutting-edge LLMs easily surpass $100M, so spending a few million dollars more for data to boost performance is quite feasible.\n\nThat‚Äôs why I believe agentic workflows will open up intriguing new opportunities for high-quality synthetic data generation.\n\n[Original text: https://t.co/zOiuUFtmo3 ]",
  "createdAt": "Thu May 02 15:38:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 236,
  "replyCount": 32,
  "likeCount": 1267,
  "quoteCount": 26,
  "viewCount": 203693,
  "bookmarkCount": 706,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1785152969304068405",
  "url": "https://x.com/AndrewYNg/status/1785152969304068405",
  "text": "Chatting with @GroqInc‚Äôs CEO @JonathanRoss321. Groq has super fast token generation capabilities now. And,  I was excited also to hear about his plans to scale up capacity aggressively and also expand this to other models than just LLMs! This is a good time to be building AI applications.",
  "createdAt": "Tue Apr 30 03:43:29 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 104,
  "replyCount": 52,
  "likeCount": 1205,
  "quoteCount": 12,
  "viewCount": 147202,
  "bookmarkCount": 122,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1784977075176374704",
  "url": "https://x.com/AndrewYNg/status/1784977075176374704",
  "text": "In Prompt Engineering for Vision Models, taught by @anmorgan2414 @JacquesVerre and @KaiserFrose of @Cometml , you‚Äôll learn how to prompt and fine-tune vision models for personalized image generation, image editing, object detection and segmentation. The prompts you'll use for vision models could be text, point coordinates, or bounding boxes, depending on the model. You'll also learn to tune hyperparameters to shape the output.\n\nModels you'll use include Segment-Anything Model (SAM), OWL-ViT, and Stable Diffusion. You'll also learn to fine-tune Stable Diffusion to generate personalized images (say, an image of a specific person), using a handful of images for training. As an example of a multi-step workflow, you'll use OWL-ViT to detect an object based on a text prompt, then pass the bounding box to SAM to create a segmentation mask, and input that mask into Stable Diffusion to replace the original object with a new one based on a text prompt.\n\nControlling vision models can be tricky; this course will teach prompting and fine-tuning techniques to get precise control over their output. Get started here: https://t.co/7JerpCG9l9",
  "createdAt": "Mon Apr 29 16:04:32 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 124,
  "replyCount": 23,
  "likeCount": 651,
  "quoteCount": 14,
  "viewCount": 151458,
  "bookmarkCount": 382,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1783588055770886652",
  "url": "https://x.com/AndrewYNg/status/1783588055770886652",
  "text": "I've really enjoyed using @crewAIInc 's tools to build multiagent AI systems -- in addition to being productive, it's also fun to use! It was great hanging out with its creator @joaomdmoura to chat about best practices for building agentic workflows. https://t.co/IxVeqTqWWj",
  "createdAt": "Thu Apr 25 20:05:04 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 37,
  "replyCount": 43,
  "likeCount": 637,
  "quoteCount": 7,
  "viewCount": 115255,
  "bookmarkCount": 122,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1783521818093195277",
  "url": "https://x.com/AndrewYNg/status/1783521818093195277",
  "text": "Much has been said about many companies‚Äô desire for more compute (as well as data) to train larger foundation models. I think it‚Äôs under-appreciated that we have nowhere near enough compute available for inference on foundation models as well.\n\nYears ago, when I was leading teams at Google, Baidu, and Stanford that focused on scaling up deep learning algorithms, many semiconductor manufacturers, data center operators, and academic researchers asked me whether I felt that AI technology would continue to make good use of more compute if they kept on delivering it. For many normal desktop processing workloads, like running a web browser or a text editor, having a faster CPU doesn‚Äôt help that much beyond a certain point. So do we really need faster and faster AI processors to train larger and larger models? Each time, I confidently replied ‚Äúyes!‚Äù and encouraged them to keep scaling up compute. (Sometimes, I added half-jokingly that I had never met a machine learning engineer who felt like they had enough compute. üòÄ)\n\nFortunately, this prediction has been right so far. However, beyond training, I believe we are also far from exhausting the benefits of faster and higher volumes of inference.\n\nToday, a lot of LLM output is primarily for human consumption. A human might read around 250 words per minute, which is around 6 tokens per second (250 words/min / (0.75 words/token) / (60 secs/min)). So it might initially seem like there‚Äôs little value to generating tokens much faster than this.\n\nBut in an agentic workflow, an LLM might be prompted repeatedly to reflect on and improve its output, use tools, plan and execute sequences of steps, or implement multiple agents that collaborate with each other. In such settings, we might easily generate hundreds of thousands of tokens or more before showing any output to a user. This makes fast token generation very desirable and makes slower generation a bottleneck to taking better advantage of existing foundation models.\n\nThat‚Äôs why I‚Äôm excited about the work of companies like @GroqInc, which can generate hundreds of tokens per second. Recently, @SambaNovaAI also published an impressive demo that hit hundreds of tokens per second.\n\nIncidentally, faster, cheaper token generation will also help make running evaluations (evals), a step that can be slow and expensive today since it typically involves iterating over many examples, more palatable. Having better evals will help many developers with the process of tuning models to improve their performance.\n\nFortunately, it appears that both training and inference are rapidly becoming cheaper. I recently spoke with @CathieDWood and @CCRobertsARK of the investment firm ARK, which is famous for its bullish predictions on tech. They estimate that AI training costs are falling at 75% a year. If they are right, a foundation model that costs $100M to train this year might cost only $25M to train next year. Further, they report that for ‚Äúenterprise scale use cases, inference costs seem to be falling at an annual rate of ~86%, even faster than training costs.‚Äù\n\nI don‚Äôt know how accurate these specific predictions will turn out to be, but with improvements in both semiconductors and algorithms, I do see training and inference costs falling rapidly. This will be good for application builders and help AI agentic workflows lift off.\n\n[Original text: https://t.co/BaH6bqZDds ]",
  "createdAt": "Thu Apr 25 15:41:52 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 236,
  "replyCount": 52,
  "likeCount": 1303,
  "quoteCount": 39,
  "viewCount": 251741,
  "bookmarkCount": 499,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1782442718960230688",
  "url": "https://x.com/AndrewYNg/status/1782442718960230688",
  "text": "New short course with @MistralAI !\n\nMistral's open-source Mixtral 8x7B model uses a \"mixture of experts\" (MoE) architecture. Unlike a standard transformer, an MoE model has multiple expert feed-forward networks (8 in this case), with a gating network selecting two experts at inference time. This enables MoE to match the performance of a large model but faster inference. Mixtral 8x7B has 46.7B parameters but activates only 12.9B at inference to predict the next token.\n\nIn Getting Started with Mistral, you‚Äôll learn from Mistral‚Äôs @sophiamyang to:\n- Explore Mistral's open-source models (Mistral 7B, Mixtral 8x7B) and commercial models via API calls and Mistral AI's Le Chat website\n- Implement JSON mode to generate structured outputs to integrate directly into larger software systems.\n- Use function calling for Tool Use, such as calling custom Python code that queries tabular data\n- Ground your LLM's response with external knowledge sources using RAG\n- Build a Mistral-powered chat interface that can reference external documents\n\nThis course will help deepen your prompt engineering skills. Please sign up here: https://t.co/weYwGmPlLA",
  "createdAt": "Mon Apr 22 16:13:55 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 278,
  "replyCount": 40,
  "likeCount": 1630,
  "quoteCount": 17,
  "viewCount": 385624,
  "bookmarkCount": 1030,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1781021135339164147",
  "url": "https://x.com/AndrewYNg/status/1781021135339164147",
  "text": "Meta released Llama 3 on my birthday! üéÇ Best present ever, thanks Meta! üòÄ",
  "createdAt": "Thu Apr 18 18:05:03 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 166,
  "replyCount": 288,
  "likeCount": 4171,
  "quoteCount": 27,
  "viewCount": 293078,
  "bookmarkCount": 113,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1780991671855161506",
  "url": "https://x.com/AndrewYNg/status/1780991671855161506",
  "text": "Multi-agent collaboration has emerged as a key AI agentic design pattern. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles -- such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on -- and have different agents accomplish different subtasks.\n\nDifferent agents might be built by prompting one LLM (or, if you prefer, different LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: \"You are an expert in writing clear, efficient code. Write code to perform the task ‚Ä¶\".\n\nIt might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I'd like to offer a few reasons:\n- It works! Many teams are getting good results with this method, and there's nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent.\n- Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An  agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that  subtask: For example, the prompt above emphasized clear, efficient code as opposed to, say, scalable and highly secure code. By decomposing the overall task into subtasks, we can optimize the subtasks better.\n- Perhaps most important, the multi-agent design pattern gives us, as developers, a framework for breaking down complex tasks into subtasks. When writing code to run on a single CPU, we often break our program up into different processes or threads. This is a useful abstraction that lets us decompose a task -- like implementing a web browser -- into subtasks that are easier to code. I find thinking through multi-agents roles to be a useful abstraction.\n\nIn many companies, managers routinely decide what roles to hire, and then how to split complex projects -- like writing a large piece of software or preparing a research report -- into smaller tasks to assign to employees with different specialties. Using multiple agents is analogous. Each agent implements its own workflow, has its own memory (itself a rapidly evolving area in agentic technologies -- how can an agent remember enough of its past interactions to perform better on upcoming ones?), and may ask other agents for help. Agents themselves can also engage in Planning and Tool Use. This results in a cacophony of LLM calls and message passing between agents that can result in very complex workflows.\n\nWhile managing people is hard, it's a sufficiently familiar idea that it gives us a mental framework for how to \"hire\" and assign tasks to our AI agents. Fortunately, the damage from mismanaging an AI agent is much lower than that from mismanaging humans!\n\nEmerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to build multi-agent solutions to problems. If you're interested in playing with a fun multi-agent system, also check out ChatDev, an open source implementation of a set of agents that run a virtual software company. I encourage you to check out their github repo and perhaps even clone the repo and run  the system yourself. While it may not always produce what you want, you might be amazed at how well it does!\n\nLike the design pattern of Planning, I find the output quality of multi-agent collaboration hard to predict. The more mature patterns of Reflection and Tool use are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you!\n\nIf you're interested in learning more, I recommend:\n- Communicative Agents for Software Development, Qian et al. (2023) (the ChatDev paper)\n- AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, Wu et al. (2023)\n- MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework, Hong et al. (2023)\n\n[Original text: https://t.co/4gTbcQfikx ]",
  "createdAt": "Thu Apr 18 16:07:58 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 516,
  "replyCount": 89,
  "likeCount": 2402,
  "quoteCount": 87,
  "viewCount": 413335,
  "bookmarkCount": 1894,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1779905922602782752",
  "url": "https://x.com/AndrewYNg/status/1779905922602782752",
  "text": "LLMs can take gigabytes of memory to store, which limits what can be run on consumer hardware. But quantization can dramatically compress models, making a wider selection of models available to developers. You can often reduce model size by 4x or more while maintaining reasonable performance. In our new short course Quantization Fundamentals taught by @huggingface's @younesbelkada and @_marcsun, you'll: \n- Learn how to quantize nearly any open source model\n- Use int8 and bfloat16 (Brain float 16) data types to load and run LLMs using PyTorch and the Hugging Face Transformers library\n- Dive into the technical details of linear quantization to map 32-bit floats to 8-bit integers\n\nAs models get bigger and bigger, quantization becomes more important for making models practical and accessible. Please check out the course here:  https://t.co/i8trQdOIOh",
  "createdAt": "Mon Apr 15 16:13:35 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 451,
  "replyCount": 41,
  "likeCount": 2355,
  "quoteCount": 28,
  "viewCount": 287673,
  "bookmarkCount": 1351,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1779606380665803144",
  "url": "https://x.com/AndrewYNg/status/1779606380665803144",
  "text": "Planning is a key agentic AI design pattern in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report.\n\nMany people had a ‚ÄúChatGPT moment‚Äù shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar ‚ÄúAI Agentic moment,‚Äù I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools.\n\nI had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool ‚Äî which I had forgotten I‚Äôd given it ‚Äî and completed the task using Wikipedia instead of web search.\n\nThis was an AI Agentic moment of surprise for me. I think many people who haven‚Äôt experienced such a moment yet will do so in the coming months. It‚Äôs a beautiful thing when you see an agent autonomously decide to do things in ways that you had not anticipated, and succeed as a result!\n\nMany tasks can‚Äôt be done in a single step or with a single tool invocation, but an agent can decide what steps to take. For example, to simplify an example from the HuggingGPT paper (cited below), if you want an agent to consider a picture of a boy and draw a picture of a girl in the same pose, the task might be decomposed into two distinct steps: (i) detect the pose in the picture of the boy and (ii) render a picture of a girl in the detected pose. An LLM might be fine-tuned or prompted (with few-shot prompting) to specify a plan by outputting a string like \"{tool: pose-detection, input: image.jpg, output: temp1 } {tool: pose-to-image, input: temp1, output: final.jpg}\".\n\nThis structured output, which specifies two steps to take, then triggers software to invoke a pose detection tool followed by a pose-to-image tool to complete the task. (This example is for illustrative purposes only; HuggingGPT uses a different format.)\n\nAdmittedly, many agentic workflows do not need planning. For example, you might have an agent reflect on, and improve, its output a fixed number of times. In this case, the sequence of steps the agent takes is fixed and deterministic. But for complex tasks in which you aren‚Äôt able to specify a decomposition of the task into a set of steps ahead of time, Planning allows the agent to decide dynamically what steps to take.\n\nOn one hand, Planning is a very powerful capability; on the other, it leads to less predictable results. In my experience, while I can get the agentic design patterns of Reflection and Tool use to work reliably and improve my applications‚Äô performance, Planning is a less mature technology, and I find it hard to predict in advance what it will do. But the field continues to evolve rapidly, and I'm confident that Planning abilities will improve quickly.\n\nIf you‚Äôre interested in learning more about Planning with LLMs, I recommend:\n- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Wei et al. (2022)\n- HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, Shen et al. (2023)\n- Understanding the planning of LLM agents: A survey, by Huang et al. (2024)\n\n[Original text: https://t.co/pWmIR9wEki ]",
  "createdAt": "Sun Apr 14 20:23:19 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 457,
  "replyCount": 83,
  "likeCount": 2431,
  "quoteCount": 58,
  "viewCount": 389654,
  "bookmarkCount": 2022,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1778075380886495676",
  "url": "https://x.com/AndrewYNg/status/1778075380886495676",
  "text": "Data preprocessing is critical for building effective RAG systems. Our new short course, Preprocessing Unstructured Data for LLM Applications, taught by @mrobinson0623 of @UnstructuredIO, demonstrates important but sometimes overlooked aspects of RAG systems:\n\n- How to extract and normalize content from diverse formats like PDF, Powerpoint, and HTML to expand your LLM's knowledge\n- Enriching data with metadata to enable more powerful retrieval and reasoning\n- Applying document layout analysis and vision transforms to process embedded images and tables\n\nThen you‚Äôll use all these skills and build a RAG bot that draws from a corpus that includes PDF, PowerPoint, and Markdown documents.\n\nPlease sign up here: https://t.co/AM3rmZJmNF",
  "createdAt": "Wed Apr 10 14:59:40 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 235,
  "replyCount": 24,
  "likeCount": 1190,
  "quoteCount": 21,
  "viewCount": 150023,
  "bookmarkCount": 814,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1776737961243218134",
  "url": "https://x.com/AndrewYNg/status/1776737961243218134",
  "text": "The Financial Times has a great article on Renate Nyborg @renate's work on @meeno_official , written by @madhumita29. \n\nThe article is paywalled, but I appreciate Renate (as well as Harvard's @ronivey)'s leadership speaking about the dangers of the AI fake girlfriend/boyfriend industry and the risk of this leading to greater loneliness. Renate says  \"Men didn‚Äôt want to meet girls because they had virtual girlfriends who said exactly what they wanted to hear.\" To regulators wondering what are the risky applications of AI, I would urge taking a look at the fake gf/bf industry! \n\nIn contrast, Meeno gives advice for human relationships, and is working to bring people together. Working to reduce human loneliness is a wonderful goal! \n\nhttps://t.co/nB6dfkHaHn",
  "createdAt": "Sat Apr 06 22:25:14 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 56,
  "replyCount": 43,
  "likeCount": 279,
  "quoteCount": 7,
  "viewCount": 110658,
  "bookmarkCount": 97,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1776363779141628369",
  "url": "https://x.com/AndrewYNg/status/1776363779141628369",
  "text": "The task-based analysis of how AI affects jobs is a powerful technique for creating business value. It was pioneered by Workhelix‚Äôs @erikbryn et al. Now, Workhelix has developed technology to apply this at scale, by automatically examining a company‚Äôs job descriptions, professional social data, and other information, to give CEOs and Boards a roadmap to creating value. \n\nAI Fund is thrilled to support Workhelix‚Äôs launch, coming Tuesday April 9th. To learn more, please join the conversation with @erikbryn, @amcafee, @danielrock and @JamesMilin and me at the webinar below! https://t.co/I6sVHhEGmV",
  "createdAt": "Fri Apr 05 21:38:22 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 90,
  "replyCount": 33,
  "likeCount": 519,
  "quoteCount": 8,
  "viewCount": 172487,
  "bookmarkCount": 316,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1775951610059141147",
  "url": "https://x.com/AndrewYNg/status/1775951610059141147",
  "text": "Tool use, in which an LLM is given functions it can request to call for gathering information, taking action, or manipulating data, is a key design pattern of AI agentic workflows. You may be familiar with LLM-based systems that can perform a web search or execute code. Some of the large, consumer-facing LLMs already incorporate these features. But tool use goes well beyond these examples. \n\nIf you prompt an online LLM-based chat system, ‚ÄúWhat is the best coffee maker according to reviewers?‚Äù, it might decide to carry out a web search and download one or more web pages to gain context. Early on, LLM developers realized that relying only on a pre-trained transformer to generate output tokens is limiting, and that giving an LLM a tool for web search lets it do much more. With such a tool, an LLM is either fine-tuned or prompted (perhaps with few-shot prompting) to generate a special string like {tool: web-search, query: \"coffee maker reviews\"} to request calling a search engine. (The exact format of the string depends on the implementation.) A post-processing step then looks for strings like these, calls the web search function with the relevant parameters when it finds one, and passes the result back to the LLM as additional input context for further processing.\n\nSimilarly, if you ask, ‚ÄúIf I invest $100 at compound 7% interest for 12 years, what do I have at the end?‚Äù, rather than trying to generate the answer directly using a transformer network ‚Äî which is unlikely to result in the right answer ‚Äî the LLM might use a code execution tool to run a Python command to compute 100 * (1+0.07)**12 to get the right answer. The LLM might generate a string like this: {tool: python-interpreter, code: \"100 * (1+0.07)**12\"}.\n\nBut tool use in agentic workflows now goes much further. Developers are using functions to search different sources (web, Wikipedia, arXiv, etc.), to interface with productivity tools (send email, read/write calendar entries, etc.), generate or interpret images, and much more. We can prompt an LLM using context that gives detailed descriptions of many functions. These descriptions might include a text description of what the function does plus details of what arguments the function expects. And we‚Äôd expect the LLM to automatically choose the right function to call to do a job.\n\nFurther, systems are being built in which the LLM has access to hundreds of tools. In such settings, there might be too many functions at your disposal to put all of them into the LLM context, so you might use heuristics to pick the most relevant subset to include in the LLM context at the current step of processing. This technique, which is described in the Gorilla paper cited below, is reminiscent of how, if there is too much text to include as context, retrieval augmented generation (RAG) systems offer heuristics for picking a subset of the text to include.\n\nEarly in the history of LLMs, before widespread availability of large multimodal models (LMMs)  like LLaVa, GPT-4V, and Gemini, LLMs could not process images directly, so a lot of work on tool use was carried out by the computer vision community. At that time, the only way for an LLM-based system to manipulate an image was by calling a function to, say, carry out object recognition or some other function on it. Since then, practices for tool use have exploded. GPT-4‚Äôs function calling capability, released in the middle of last year, was a significant step toward general-purpose tool use. Since then, more and more LLMs are being developed to similarly be facile with tool use.\n\nIf you‚Äôre interested in learning more about tool use, I recommend:\n- Gorilla: Large Language Model Connected with Massive APIs, Patil et al. (2023)\n- MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action, Yang et al. (2023)\n- Efficient Tool Use with Chain-of-Abstraction Reasoning, Gao et al. (2024)\n\nBoth Tool Use and Reflection, which I posted about last week, are design patterns that I can get to work fairly reliably on my applications ‚Äî both are capabilities well worth learning about. In the future, I‚Äôll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable ‚Äî albeit very exciting ‚Äî technologies.\n\n[Original text:  https://t.co/gHCOYSsKQO ]",
  "createdAt": "Thu Apr 04 18:20:34 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 301,
  "replyCount": 80,
  "likeCount": 1588,
  "quoteCount": 33,
  "viewCount": 255763,
  "bookmarkCount": 1270,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1775569875639116148",
  "url": "https://x.com/AndrewYNg/status/1775569875639116148",
  "text": "Learn to carry out red teaming attacks against your own LLM-based applications to spot and patch vulnerabilities! In our new short course, Red Teaming LLM Applications, Matteo Dora & Luca Martial of LLM testing company @giskard_ai teach how to simulate malicious actions to discover vulnerabilities, and improve security. We start with prompt injection, where you can trick an LLM into bypassing safeguards to reveal private information, or say something inappropriate. There is no one-size-fits-all approach to security, but this course will help you identify some scenarios to protect against.\n\nWe believe having red teaming capabilities widely known will result in greater transparency and safer LLM-based systems. However, we ask you to use the skills you gain from this course ethically.\n\nPlease sign up here: https://t.co/Y9ZANSldhG",
  "createdAt": "Wed Apr 03 17:03:41 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 129,
  "replyCount": 27,
  "likeCount": 734,
  "quoteCount": 7,
  "viewCount": 109324,
  "bookmarkCount": 319,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1775324778624397657",
  "url": "https://x.com/AndrewYNg/status/1775324778624397657",
  "text": "I hope everyone in Taiwan üáπüáº is okay after the earthquake. My thoughts are with everyone affected. ‚ù§Ô∏è",
  "createdAt": "Wed Apr 03 00:49:45 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 127,
  "replyCount": 47,
  "likeCount": 1866,
  "quoteCount": 5,
  "viewCount": 188744,
  "bookmarkCount": 28,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1773393357022298617",
  "url": "https://x.com/AndrewYNg/status/1773393357022298617",
  "text": "Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality output. Here, I'd like to discuss Reflection. For a design pattern that‚Äôs relatively quick to implement, I've seen it lead to surprising performance gains. \n\nYou may have had the experience of prompting ChatGPT/Claude/Gemini, receiving unsatisfactory output, delivering critical feedback to help the LLM improve its response, and then getting a better response. What if you automate the step of delivering critical feedback, so the model automatically criticizes its own output and improves its response? This is the crux of Reflection. \n\nTake the task of asking an LLM to write code. We can prompt it to generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:\n\nHere‚Äôs code intended for task X:\n[previously generated code]\nCheck the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.\n\nSometimes this causes the LLM to spot problems and come up with constructive suggestions. Next, we can prompt the LLM with context including (i) the previously generated code and (ii) the constructive feedback, and ask it to use the feedback to rewrite the code. This can lead to a better response. Repeating the criticism/rewrite process might yield further improvements. This self-reflection process allows the LLM to spot gaps and improve its output on a variety of tasks including producing code, writing text, and answering questions.\n\nAnd we can go beyond self-reflection by giving the LLM tools that help evaluate its output; for example, running its code through a few unit tests to check whether it generates correct results on test cases or searching the web to double-check text output. Then it can reflect on any errors it found and come up with ideas for improvement.\n\nFurther, we can implement Reflection using a multi-agent framework. I've found it convenient to create two different agents, one prompted to generate good outputs and the other prompted to give constructive criticism of the first agent's output. The resulting discussion between the two agents leads to improved responses.\n\nReflection is a relatively basic type of agentic workflow, but I've been delighted by how much it improved my applications‚Äô results in a few cases. I hope you will try it in your own work. If you‚Äôre interested in learning more about reflection, I recommend these papers:\n- Self-Refine: Iterative Refinement with Self-Feedback, by Madaan et al. (2023)\n- Reflexion: Language Agents with Verbal Reinforcement Learning, by Shinn et al. (2023)\n- CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, by Gou et al. (2024)\n\nI‚Äôll discuss the other agentic design patterns as well in the future.\n\n[Original text: https://t.co/FtM2zOT2Lx ]",
  "createdAt": "Thu Mar 28 16:54:59 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 576,
  "replyCount": 101,
  "likeCount": 2801,
  "quoteCount": 76,
  "viewCount": 488223,
  "bookmarkCount": 2649,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1773006786058219889",
  "url": "https://x.com/AndrewYNg/status/1773006786058219889",
  "text": "New JavaScript short course: Build a full-stack web application that uses RAG in JavaScript RAG Web Apps with LlamaIndex, taught by @seldo, VP of Developer Relations at @llama_index and npm co-founder.\n- Build a RAG application for querying your own data\n- Develop tools to interact with multiple data sources using an agent that intelligently selects the right tool for your queries\n- Create a full-stack web app that can chat with your data\n- Dig further into production-ready techniques, like how to persist your data so you aren‚Äôt constantly reindexing, and try the create-llama command line tool from LlamaIndex\nYou can sign up here: https://t.co/w2j0Mq2df1",
  "createdAt": "Wed Mar 27 15:18:53 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 241,
  "replyCount": 35,
  "likeCount": 1277,
  "quoteCount": 25,
  "viewCount": 218147,
  "bookmarkCount": 1046,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1771297451506622741",
  "url": "https://x.com/AndrewYNg/status/1771297451506622741",
  "text": "I‚Äôve been a fan of ‚Å¶@pyautogen‚Å© as a multiagent programming framework for awhile. It was great hosting two of its leaders ‚Å¶@Chi_Wang_‚Å© and ‚Å¶@qingyun_wu‚Å© to discuss agent design patterns! https://t.co/1c2SrjLtj1",
  "createdAt": "Fri Mar 22 22:06:36 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 61,
  "replyCount": 28,
  "likeCount": 669,
  "quoteCount": 9,
  "viewCount": 107590,
  "bookmarkCount": 129,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1770969902452822519",
  "url": "https://x.com/AndrewYNg/status/1770969902452822519",
  "text": "Yes, with agentic workflows, super fast token generation (like @groq) becomes very important to overall system speed. \n\nIf an LLM were generating tokens only for human consumption, then there's not much value to generating much faster than human reading speed. But with agentic workflows, most of the generated tokens are consumed not by humans but instead by another part of the AI system, so very very high token generation throughput is a big help to speeding up the overall system.",
  "createdAt": "Fri Mar 22 00:25:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 16,
  "replyCount": 7,
  "likeCount": 117,
  "quoteCount": 10,
  "viewCount": 28549,
  "bookmarkCount": 32,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1770967731753677266",
  "url": "https://x.com/AndrewYNg/status/1770967731753677266",
  "text": "@erikbryn Yes, and the outcomes of task-based analysis of jobs is also changing, since the set of tasks that agentic workflows can do is much larger than the set of tasks that non-agentic LLMs can do!",
  "createdAt": "Fri Mar 22 00:16:24 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 6,
  "replyCount": 1,
  "likeCount": 35,
  "quoteCount": 0,
  "viewCount": 13692,
  "bookmarkCount": 7,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1770897666702233815",
  "url": "https://x.com/AndrewYNg/status/1770897666702233815",
  "text": "I think AI agentic workflows will drive massive AI progress this year ‚Äî perhaps even more than the next generation of foundation models. This is an important trend, and I urge everyone who works in AI to pay attention to it.\n\nToday, we mostly use LLMs in zero-shot mode, prompting a model to generate final output token by token without revising its work. This is akin to asking someone to compose an essay from start to finish, typing straight through with no backspacing allowed, and expecting a high-quality result. Despite the difficulty, LLMs do amazingly well at this task!\n\nWith an agentic workflow, however, we can ask the LLM to iterate over a document many times. For example, it might take a sequence of steps such as:\n- Plan an outline.\n- Decide what, if any, web searches are needed to gather more information.\n- Write a first draft.\n- Read over the first draft to spot unjustified arguments or extraneous information.\n- Revise the draft taking into account any weaknesses spotted.\n- And so on.\n\nThis iterative process is critical for most human writers to write good text. With AI, such an iterative workflow yields much better results than writing in a single pass.\n\nDevin‚Äôs splashy demo recently received a lot of social media buzz. My team has been closely following the evolution of AI that writes code. We analyzed results from a number of research teams, focusing on an algorithm‚Äôs ability to do well on the widely used HumanEval coding benchmark. You can see our findings in the diagram below. \n\nGPT-3.5 (zero shot) was 48.1% correct. GPT-4 (zero shot) does better at 67.0%. However, the improvement from GPT-3.5 to GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. \n\nOpen source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I‚Äôd like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.\n\n- Reflection: The LLM examines its own work to come up with ways to improve it.\n- Tool use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.\n- Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).\n- Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.\n\nI‚Äôll elaborate on these design patterns and offer suggested readings for each next week. \n\n[Original text: https://t.co/y4McIAjD2m]",
  "createdAt": "Thu Mar 21 19:38:00 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 1251,
  "replyCount": 217,
  "likeCount": 5257,
  "quoteCount": 196,
  "viewCount": 832515,
  "bookmarkCount": 3902,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1769761666143814122",
  "url": "https://x.com/AndrewYNg/status/1769761666143814122",
  "text": "Learn how to build an optimized LLM inference system from the ground up in our new short course, Efficiently Serving LLMs, built in collaboration with @predibase and taught by @TravisAddair.\n\nWhether you're serving your own LLM or using a model hosting service, this course will give you a deep understanding of the optimizations required to efficiently serve many users at once.\n- Learn how LLMs generate text one token at a time, and how techniques like KV caching, continuous batching, and quantization speed things up and optimize memory usage for serving multiple users.\n- Benchmark the performance of these LLM optimizations to explore the trade-offs between quickly responding to an individual user‚Äôs request vs. serving many users at once.\n- Use techniques like low-rank adaptation (LoRA) to efficiently serve hundreds of unique, custom fine-tuned models on a single device, without sacrificing throughput.\n- Use Predibase's LoRAX framework to see optimization techniques in action on a real LLM server.\n\nSign up here: https://t.co/JgIvrJGf8G",
  "createdAt": "Mon Mar 18 16:23:56 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 130,
  "replyCount": 21,
  "likeCount": 756,
  "quoteCount": 14,
  "viewCount": 104458,
  "bookmarkCount": 413,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1767941813820862655",
  "url": "https://x.com/AndrewYNg/status/1767941813820862655",
  "text": "Our new short course, Knowledge Graphs for RAG, is now available! Knowledge graphs are a data structure that is great at capturing complex relationships between data of multiple types. By enabling more sophisticated retrieval of text than similarity search alone, knowledge graphs can improve the context you pass to the LLM and the performance of your RAG applications. \n\nIn this course, taught by @akollegger of @neo4j, you‚Äôll\n- Explore how knowledge graphs work by building a graph of public financial documents from scratch\n- Learn to write queries that retrieve text and data from the graph and use it to enhance the context you pass to an LLM chatbot\n- Combine a knowledge graph with a question-answer chain to build better RAG-powered chat systems\n\nSign up here! https://t.co/N3gceKrvib",
  "createdAt": "Wed Mar 13 15:52:29 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 408,
  "replyCount": 50,
  "likeCount": 2104,
  "quoteCount": 38,
  "viewCount": 242392,
  "bookmarkCount": 1660,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1766589035781202423",
  "url": "https://x.com/AndrewYNg/status/1766589035781202423",
  "text": "@SiVola @RylanSchaeffer @BrandoHablando @sanmikoyejo The definition of AGI I use is \"AI that can perform any intellectual task that a human can.\"\n\nBut a few teams have come up with alternative definitions, so its meaning has become muddied and confusing, I now less it less esp in discussions that need technical precision.",
  "createdAt": "Sat Mar 09 22:17:02 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 15,
  "replyCount": 20,
  "likeCount": 103,
  "quoteCount": 4,
  "viewCount": 11793,
  "bookmarkCount": 21,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1766554536192446957",
  "url": "https://x.com/AndrewYNg/status/1766554536192446957",
  "text": "When we get to AGI, it will have come slowly, not overnight. \n\nA NeurIPS Outstanding Paper award recipient, Are Emergent Abilities of Large Language Models a Mirage? (by @RylanSchaeffer, @BrandoHablando, @sanmikoyejo) studies emergent properties of LLMs, and concludes: \n\"... emergent abilities appear due the researcher‚Äôs choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance.\" \n\nPublic perception goes through discontinuities when lots of people suddenly become aware of a technology -- maybe one that's been developing for a long time --  leading to a surprise. But growth in AI capabilities is  more continuous than one might think. \n\nThat's why I expect the path to AGI to be one involving numerous steps forward, leading to step-by-step improvements in how intelligent our systems are.",
  "createdAt": "Sat Mar 09 19:59:57 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 259,
  "replyCount": 91,
  "likeCount": 1598,
  "quoteCount": 37,
  "viewCount": 240773,
  "bookmarkCount": 648,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1765426218847949236",
  "url": "https://x.com/AndrewYNg/status/1765426218847949236",
  "text": "New short course: Open Source Models with Hugging Face ü§ó, taught by @mariaKhalusova, @_marcsun, and Younes Belkada! @huggingface has been a game changer by letting you quickly grab any of hundreds of thousands of already-trained open source models to assemble into new applications. This course teaches you best practices for building this way, including how to search and choose among models.\n\nYou‚Äôll learn to use the Transformers library and walk through multiple models for text, audio, and image processing, including zero-shot image segmentation, zero-shot audio classification, and speech recognition. You'll also learn to use multimodal models for visual question answering, image search, and image captioning. Finally, you‚Äôll learn how to demo what you build locally, on the cloud, or via an API using Gradio and Hugging Face Spaces. \n\nYou can sign up here: https://t.co/KavDNQHCCY",
  "createdAt": "Wed Mar 06 17:16:25 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 185,
  "replyCount": 40,
  "likeCount": 1140,
  "quoteCount": 28,
  "viewCount": 224100,
  "bookmarkCount": 707,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1765059128190173202",
  "url": "https://x.com/AndrewYNg/status/1765059128190173202",
  "text": "There're now multiple, very well resourced companies that \"can't afford to lose\" spending billions to compete to build better LLMs. I expect this competition to go on for years. This is going to great for innovation, and also for everyone building applications on top of LLMs.",
  "createdAt": "Tue Mar 05 16:57:44 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 131,
  "replyCount": 67,
  "likeCount": 1306,
  "quoteCount": 25,
  "viewCount": 157065,
  "bookmarkCount": 145,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1762879627633287477",
  "url": "https://x.com/AndrewYNg/status/1762879627633287477",
  "text": "New short course: Prompt Engineering with Llama 2, built in collaboration with Meta @AIatMeta, and taught by @asangani7! Meta's Llama 2 has been game-changing for AI. Building with open source lets you control your own data, scrutinize errors, update (or not) the models as you please, and work alongside the global community advancing open models.\n\nLlama isn't a single model, it's a collection of models. In this course, you'll:\n- Learn the differences between different Llama 2 flavors, and when to use each.\n- Prompt the Llama chat models -- you'll also see how Llama's instruction tags work -- so they can help you with day-to-day tasks, like writing or summarization.\n- Use advanced prompting, like few-shot prompting for classification, and chain-of-thought prompting for solving logic problems.\n- Use specialized models in the Llama collection for specific tasks, like Code Llama to help you write, analyze, and improve code, and Llama Guard, which checks prompts and model responses for harmful content. \n\nThe course also touches on how to run Llama 2 locally on your own computer.\n\nI hope you‚Äôll take this course and try out these powerful, open models!\nhttps://t.co/kas7jmeCkj",
  "createdAt": "Wed Feb 28 16:37:10 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 260,
  "replyCount": 86,
  "likeCount": 1287,
  "quoteCount": 18,
  "viewCount": 162302,
  "bookmarkCount": 805,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1761912346153521374",
  "url": "https://x.com/AndrewYNg/status/1761912346153521374",
  "text": "To all my Google friends: I know this week has been tough with a lot of criticism about Gemini's gaffes. \n\nJust wanted to say I love all of you and am rooting for you. I know everyone means well, and am grateful for your work &amp; eager to see where you next take this amazing tech!",
  "createdAt": "Mon Feb 26 00:33:32 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 204,
  "replyCount": 267,
  "likeCount": 3456,
  "quoteCount": 70,
  "viewCount": 445641,
  "bookmarkCount": 172,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1759430085957111932",
  "url": "https://x.com/AndrewYNg/status/1759430085957111932",
  "text": "@roelofbotha @sequoia It's wonderful that @sequoia is putting this together to support Open Source to benefit everyone. Thank you @roelofbotha! \n\nAm also a huge fan of your first Open Source Fellow @tiangolo -- I was literally using his FastAPI framework today to deploy an app!",
  "createdAt": "Mon Feb 19 04:09:55 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 3,
  "replyCount": 5,
  "likeCount": 98,
  "quoteCount": 1,
  "viewCount": 33479,
  "bookmarkCount": 6,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1758633108654711008",
  "url": "https://x.com/AndrewYNg/status/1758633108654711008",
  "text": "Congratulations @hwchase17 and the whole @LangChainAI team! Love the work you're doing to make it easy for others to build LLM apps.",
  "createdAt": "Fri Feb 16 23:23:01 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 40,
  "replyCount": 58,
  "likeCount": 393,
  "quoteCount": 0,
  "viewCount": 102764,
  "bookmarkCount": 63,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1757826615147704345",
  "url": "https://x.com/AndrewYNg/status/1757826615147704345",
  "text": "@fahadaziz It is our privilege at AI Fund to be working with you!",
  "createdAt": "Wed Feb 14 17:58:18 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 0,
  "replyCount": 0,
  "likeCount": 9,
  "quoteCount": 0,
  "viewCount": 7199,
  "bookmarkCount": 2,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1757821916843552842",
  "url": "https://x.com/AndrewYNg/status/1757821916843552842",
  "text": "New short course on Serverless LLM apps with Amazon Bedrock, taught by @AWS' @mikegchambers! A serverless architecture enables you to quickly deploy your applications without needing to set up and manage compute servers to run your applications on, the maintenance of which can be another full-time job. In this course, you‚Äôll learn how to do this by using an event-driven architecture to build complex AI workflows.\n\nMike illustrate these concepts by building a cool application that automatically detects incoming customer inquiries, transcribes them with ASR (automatic speech recognition), summarizes them with an LLM using Bedrock, and deploys serverless with AWS Lambda.\n\nI hope this course makes it much easier for you to build and deploy LLM applications requiring multi-step AI workflows.  Please sign up here: https://t.co/37hE71j3pT",
  "createdAt": "Wed Feb 14 17:39:38 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 181,
  "replyCount": 68,
  "likeCount": 900,
  "quoteCount": 8,
  "viewCount": 107782,
  "bookmarkCount": 406,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1752763171042165183",
  "url": "https://x.com/AndrewYNg/status/1752763171042165183",
  "text": "New short course on Building Applications with Vector Databases, taught by @pinecone‚Äôs @timt! At the heart of a vector database is the ability to store a collection of vectors and then query against that, meaning input a new vector and find similar ones. This is useful for many AI applications. In this course, you'll learn how to use vector databases to build:\n\n(i) Semantic Search: Create a text search tool that goes beyond keyword matching, and instead focuses on the meaning of content.\n(ii) RAG (retrieval augmented generation): Enhance your LLM output by incorporating context from sources the model wasn't trained on.\n(iii) Recommender System: Combine semantic search and RAG to recommend topics, and demonstrate it with a news article recommender.\n(iv) Hybrid Search: Build an application that finds items using both images and descriptive text -- by combining both sparse and dense vector representations of the data -- using an eCommerce dataset as an example.\n(v) Image Similarity: Use image vector embeddings to create an app to compare facial features, using a database of public figures to determine the likeness between them.\n(vi) Anomaly Detection: Build an anomaly detection app that identifies unusual patterns in network communication logs.\n\nI hope you‚Äôll enjoy learning how to build all these types of applications! Please sign up here: https://t.co/nginq45FAf",
  "createdAt": "Wed Jan 31 18:37:59 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 206,
  "replyCount": 65,
  "likeCount": 1150,
  "quoteCount": 14,
  "viewCount": 136721,
  "bookmarkCount": 654,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1750985019789873244",
  "url": "https://x.com/AndrewYNg/status/1750985019789873244",
  "text": "My takeaways from attending WEF at Davos last week:\n- There were lots of discussions on business implementation of AI. My top two tips: (i) Pretty much all knowledge workers can benefit from using GenAI now, but most will need training. (ii) Task-based analysis of jobs is helping businesses identify opportunities. \n- Also lots of AI regulation conversations. I'm happy to report that the conversation is much more sensible than 6 months ago. For example, the unnecessary fears and discussion on AI extinction risk is fading away. But some big companies are still pushing for stifling, anti-competitive regulations, and the fight to protect open-source is still far from won. \n- Attending climate sessions made me even more worried about the lack of action to change our planet's trajectory. Rather than 1.5 degrees Celsius of warming as the optimistic case and 2 degrees as the pessimistic case, I think 2 degrees is an optimistic case, and 4 degrees a more realistic pessimistic case. Decarbonization remains critical; and unfortunately, that we're talking about 1.5-2 degrees rather than 2-4 degrees means we're underinvesting in resilience, adaptation, and potentially game-changing technologies like geo-engineering.\n\nLonger writeup below in The Batch: https://t.co/ZkdsgeF6WU",
  "createdAt": "Fri Jan 26 20:52:15 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 264,
  "replyCount": 105,
  "likeCount": 1439,
  "quoteCount": 32,
  "viewCount": 723896,
  "bookmarkCount": 451,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1750200384600309872",
  "url": "https://x.com/AndrewYNg/status/1750200384600309872",
  "text": "New short course on Automated Testing for LLMOps, by @CircleCI's CTO Rob Zuber! This teaches you how to adapt some key ideas from CI (continuous integration), which has been a pillar of efficient software engineering, to building LLM-based applications.\n\nTweaking an LLM-based app to improve it -- say by modifying a prompt -- can have unexpected side effects. For example, what if a teammate updates a prompt to try to make the LLM output sound more interesting, but this causes it to hallucinate more? Automated testing, as part of your approach to LLMOps (LLM Operations), helps avoid these problems and lets you ship faster and with greater confidence.\n\nIn this course, you‚Äôll learn to:\n(i) Write LLM evaluations to cover common problems like hallucinations, data drift, and harmful or offensive output.\n(ii) Build a CI workflow to automatically evaluate each change to your application.\n(iii) Orchestrate your CI workflow to run specific evaluations at different stages of development.\n\nCI is especially important for AI applications given the iterative nature of AI development, which means we often want to make many incremental changes. \n\nPlease sign up for this course here!  https://t.co/eTfzX2dPjD",
  "createdAt": "Wed Jan 24 16:54:23 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 148,
  "replyCount": 67,
  "likeCount": 789,
  "quoteCount": 9,
  "viewCount": 100885,
  "bookmarkCount": 356,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1748005715237654528",
  "url": "https://x.com/AndrewYNg/status/1748005715237654528",
  "text": "New short course on LLMOps!\n\nLLMOps (large language model operations) is a rapidly developing field that takes ideas from MLOps (machine learning operations) and specializes them to building and deploying LLM-based applications. In this course, taught by @googlecloud's Erwin Huizenga, you'll learn to use automation to make building, tuning and deploying an LLM-based application less manual and more efficient. \n\nYou'll learn how to:\n- Apply supervised fine-tuning to tune an LLM to a specific task\n- Automate and orchestrate LLM-tuning and deployment by customizing a pre-built tuning pipeline\n- Apply best practices for preparing training data for supervised fine-tuning of an LLM\n- Create an LLMOps workflow you can adapt to other LLM-tuning jobs\n\nThis course doesn't assume any prior MLOps or LLMOps experience. Sign up here to learn about this emerging field! https://t.co/UlDEbI0DbK",
  "createdAt": "Thu Jan 18 15:33:33 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 281,
  "replyCount": 78,
  "likeCount": 1461,
  "quoteCount": 25,
  "viewCount": 221278,
  "bookmarkCount": 1014,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1745516258697863259",
  "url": "https://x.com/AndrewYNg/status/1745516258697863259",
  "text": "It is only rarely that, after reading a research paper, I feel like giving the authors a standing ovation. But I felt that way after finishing Direct Preference Optimization (DPO) by @rm_rafailov @archit_sharma97 @ericmitchellai @StefanoErmon @chrmanning and @chelseabfinn. This beautiful paper proposes a much simpler alternative to RLHF (reinforcement learning from human feedback) for aligning language models to human preferences. \n\nRLHF has been a key technique for training LLMs. In brief, RLHF (i) Gets humans to specify their preferences by ranking LLM outputs, (ii) Trains a reward model (used to score LLM outputs) -- typically represented using a transformer network -- to be consistent with the human rankings, (iii) Uses reinforcement learning to tune an LLM, also represented as a transformer, to maximize rewards. This requires two transformer networks, and RLHF is also finicky to the choice of hyperparameters.\n\nDPO simplifies the whole thing. Via clever mathematical insight, the authors show that given an LLM, there is a specific reward function for which that LLM is optimal. DPO then trains the LLM directly to make the reward function (that‚Äôs now implicitly defined by the LLM) consistent with the human rankings. So you no longer need to deal with a separately represented reward function, and you can train the LLM directly to optimize the same objective as RLHF. \n\nAlthough it‚Äôs still too early to be sure, I am cautiously optimistic that DPO will have a huge impact on LLMs and beyond in the next few years.\n\nYou can read the paper here: https://t.co/m14qRYszVa I also write more about this in The Batch (linked to below).  \nhttps://t.co/8h2ag2plIa",
  "createdAt": "Thu Jan 11 18:41:20 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 764,
  "replyCount": 90,
  "likeCount": 5099,
  "quoteCount": 78,
  "viewCount": 694998,
  "bookmarkCount": 3694,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1745127613742657887",
  "url": "https://x.com/AndrewYNg/status/1745127613742657887",
  "text": "Our first Generative AI short course in JavaScript!\n\nGitHub recently reported that JavaScript is again the world‚Äôs most popular programming language. To support web developers exploring and developing with generative AI, we just launched a new short course in JavaScript taught by @Hacubu, founding engineer at @LangChainAI. In ‚Äã‚ÄãBuild LLM Apps with LangChain.js you‚Äôll learn elements common in AI development, including:\n\n(i) Using data loaders to pull data from common sources such as PDFs, websites, and databases\n(ii) Prompts, which are used to provide the LLM context\n(iii) Modules to support RAG such as text splitters and integrations with vector stores\n(iv) Working with different models to write applications that are not vendor-specific\n(v) Parsers, which extract and format the output for your downstream code to process\n\nYou‚Äôll also build with the LangChain Expression Language, which lets you easily compose  sequences (also called chains) of modules to perform complex tasks using LLMs. \n\nPutting all this together, you‚Äôll also work on a conversational question-answering LLM application capable of using external data as context.\n\nPlease sign up here: https://t.co/oVoFPKRBbW",
  "createdAt": "Wed Jan 10 16:57:00 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 332,
  "replyCount": 78,
  "likeCount": 1709,
  "quoteCount": 40,
  "viewCount": 283672,
  "bookmarkCount": 1126,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1744433663969022090",
  "url": "https://x.com/AndrewYNg/status/1744433663969022090",
  "text": "I said some things poorly in my previous tweet, so let me elaborate/clarify.\n\n1. I don't think it's okay for any company to regurgitate others' copyrighted content at scale without permission or a viable fair-use rationale. I should have said this more explicitly.\n\nAnd... I still think the link between training an LLM on someone's content to having the LLM regurgitate that content to users at scale is weaker than many would have thought from looking at the NYT lawsuit. It is possible that an LLM will regurgitate text using only the pre-trained weights (no RAG), but I believe only in very rare, corner cases, in response to particular prompts (that in practice are hardly ever used by normal users).\n\n2. When I try to replicate the \"worst\" looking examples of copyright violations in the lawsuit -- such as a user trying to use ChatGPT to get around a paywall, or get Wirecutter (an NYT property) results -- I end up triggering GPT-4's web browsing capability. (See two examples in attached screenshots.) That's why I said I suspect RAG was involved in the examples used in the NYT lawsuit.\n\nSpecifically, one of the cool features of GPT-4 is that it can browse the web to download additional information to generate its response. For example, one can prompt it to do a web search, or sometimes even to download a specific article. While it's not great that GPT-4 apparently used to be willing to download and display an article (nearly) verbatim, to OpenAI's credit, this loophole appears to have been closed.\n\nI believe the prominence given to these examples in the lawsuit made people think that it was training an LLM on NYT text that led directly to some of these examples of NYT text being regurgitated. But if RAG was involved, then the root cause of these examples of regurgitation is not that the LLM was trained on NYT text -- that's why I said I found the presentation of issues in the lawsuit muddied.\n\n3. It is also true that the NYT shows that you can get GPT-4 to regurgitate NYT text, by prompting it in a particular way. (I should have said this in my last tweet as well.) The prompts used seem to typically involve giving a large chunk of an article, and then getting the LLM to complete it.\n\nWhile it's not great that an LLM does this, I am skeptical that practically anyone uses an LLM this way.  That‚Äôs why I said that given the rarity of such generations resulting in text regurgitation, I question how much harm to NYT this has actually caused. I'm also not sure if this works only on articles that have been syndicated and are all over the internet anyway (so that the article appears numerous times in the LLM training set). Further, it looks like the newer versions of ChatGPT have closed this loophole. \n\n@TonyW also makes this point well: https://t.co/KKYir0eFVV\n\nThanks for reading!",
  "createdAt": "Mon Jan 08 18:59:30 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 114,
  "replyCount": 112,
  "likeCount": 667,
  "quoteCount": 17,
  "viewCount": 265186,
  "bookmarkCount": 186,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1744145064115446040",
  "url": "https://x.com/AndrewYNg/status/1744145064115446040",
  "text": "After reading the @nytimes lawsuit against @OpenAI and @Microsoft, I find my sympathies more with OpenAI and Microsoft than with the NYT. \n\nThe suit:\n(1) Claims, among other things, that OpenAI and Microsoft used millions of copyrighted NYT articles to train their models\n(2) Gives examples in which OpenAI models regurgitated NYT articles almost verbatim\n\nBut the presentation muddies (1) and (2), and I saw a lot of commentary on social media that -- because of what I believe is a muddied presentation -- draws a link between them that I'm not sure is what people think it is.\n\nOn (1): I understand why media companies don't like people training on their documents, but believe that just as humans are allowed to read documents on the open internet, learn from them, and synthesize brand new ideas, AI should be allowed to do so too. I would like to see training on the public internet covered under fair use -- society will be better off this way -- though whether it actually is will ultimately be up to legislators and the courts. \n\nOn (2): I suspect a lot of the examples of ChatGPT regurgitating articles nearly verbatim were due to a RAG-like mechanism where the user prompt causes the system to browse the web, retrieve a specific article and then print it out. (If my statement here isn't accurate, I would love to see the @nytimes clarify this.) If this is the case, then (i) To OpenAI's credit, they seem to have already updated their software to make this much less likely, and (ii) This is also a much easier problem to fix than if an LLM were to regurgitate text using only the pre-trained weights, which AFAIK very rarely happens (and which, given its rarity, also raises the question of how much harm to NYT this has actually caused). \n\nTo be clear, I believe independent media is important for democracy and must be protected. I also sympathize with media businesses worried about Generative AI disrupting their businesses. But I'm not convinced the NYT lawsuit is the right way to do this. \n\nUsual caveat: I am not a lawyer and am not giving legal advice or any other form of advice here. \n\nYou can also read more details of my take on this below. https://t.co/wkZSMHsvNA",
  "createdAt": "Sun Jan 07 23:52:42 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 550,
  "replyCount": 299,
  "likeCount": 3448,
  "quoteCount": 101,
  "viewCount": 945113,
  "bookmarkCount": 842,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1742943594242249023",
  "url": "https://x.com/AndrewYNg/status/1742943594242249023",
  "text": "New short course on advanced retrieval for RAG (retrieval augmented generation)! \n\nRAG fetches relevant documents to give context to an LLM. In Advanced Retrieval for AI with Chroma, taught by @trychroma founder @atroyn, you‚Äôll learn:\n(i) Query expansion using an LLM to rewrite and improve a query, by either generating either additional relevant queries or a hypothetical answer to the query.\n(ii) Reranking using a cross-encoder - a model trained to measure similarity between two inputs presented simultaneously. Reranking reorders retrieved documents based on the cross-encoder similarity measure. \n(iii) Constructing and training an Embedding Adaptor, which is a model that adapts the embedding values to be more relevant to your use case.\n\nEach of these techniques can help you build much better RAG systems. Please sign up for the course here: https://t.co/6N1H8agcYC",
  "createdAt": "Thu Jan 04 16:18:29 +0000 2024",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
  "retweetCount": 258,
  "replyCount": 77,
  "likeCount": 1467,
  "quoteCount": 22,
  "viewCount": 190628,
  "bookmarkCount": 987,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": -1,
  "text": "Since you are a free user, you can only access a maximum of 15 tweets. Please upgrade to a paid user to unlock access to all tweets."
}]