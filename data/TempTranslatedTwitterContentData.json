[
  {
    "id": "1975614372799283423",
    "url": "https://x.com/AndrewYNg/status/1975614372799283423",
    "text": "Announcing my new course: Agentic AI!\n\nBuilding AI agents is one of the most in-demand skills in the job market. This course, available now at https://t.co/zGHUh1loPO, teaches you how.\n\nYou'll learn to implement four key agentic design patterns:\n- Reflection, in which an agent examines its own output and figures out how to improve it\n- Tool use, in which an LLM-driven application decides which functions to call to carry out web search, access calendars, send email, write code, etc.\n- Planning, where you'll use an LLM to decide how to break down a task into sub-tasks for execution, and\n- Multi-agent collaboration, in which you build multiple specialized agents — much like how a company might hire multiple employees — to perform a complex task\n\nYou'll also learn to take a complex application and systematically decompose it into a sequence of tasks to implement using these design patterns.\n\nBut here's what I think is the most important part of this course: Having worked with many teams on AI agents, I've found that the single biggest predictor of whether someone executes well is their ability to drive a disciplined process for evals and error analysis. In this course, you'll learn how to do this, so you can efficiently home in on which components to improve in a complex agentic workflow. Instead of guessing what to work on, you'll let evals data guide you. This will put you significantly ahead of the game compared to the vast majority of teams building agents.\n\nTogether, we'll build a deep research agent that searches, synthesizes, and reports, using all of these agentic design patterns and best practices.\n\nThis self-paced course is taught in a vendor neutral way, using raw Python - without hiding details in a framework. You'll see how each step works, and learn the core concepts that you can then implement using any popular agentic AI framework, or using no framework. The only prerequisite is familiarity with Python, though knowing a bit about LLMs helps.\n\nCome join me, and let's build some agentic AI systems!\n\nSign up to get started: https://t.co/FX35dloqw4",
    "createdAt": "Tue Oct 07 17:29:06 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1053,
    "replyCount": 135,
    "likeCount": 6399,
    "quoteCount": 76,
    "viewCount": 650328,
    "bookmarkCount": 6432,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我的新课程发布啦：智能体 AI (Agentic AI)!\n\n构建 AI 智能体 (AI agents) 是当前就业市场需求最热门的技能之一。这门课程，现已在 https://t.co/zGHUh1loPO 上线，将手把手教你如何掌握这项技能。\n\n你将学习实现四种关键的智能体设计模式：\n- 反思 (Reflection)，智能体通过审视自己的输出并找出改进方法来提升表现。\n- 工具使用 (Tool use)，一个由 大语言模型 (LLM) 驱动的应用程序能够自主决定调用哪些函数，以执行网页搜索、访问日历、发送电子邮件、编写代码等任务。\n- 规划 (Planning)，你将学会如何利用 大语言模型 (LLM) 将复杂任务分解为可执行的子任务。\n- 多智能体协作 (Multi-agent collaboration)，你将构建多个专业化的智能体 — 就像一家公司会雇佣多名员工一样 — 共同完成一个复杂的任务。\n\n你还将学习如何系统地将一个复杂的应用程序分解成一系列任务，并使用这些设计模式进行实现。\n\n但我认为本课程最核心的部分在于：在与许多团队合作开发 AI 智能体 (AI agents) 的过程中，我发现一个人能否出色完成任务，最大的决定因素在于他们能否有效推行严谨的评估 (evals) 和错误分析 (error analysis) 流程。在本课程中，你将学会如何做到这一点，从而高效地 pinpoint 复杂智能体工作流中需要改进的组件。你将不再凭空猜测，而是让评估 (evals) 数据指引你改进的方向。这将使你比绝大多数构建智能体的团队遥遥领先。\n\n我们将一起构建一个深度研究智能体，它将运用所有这些智能体设计模式和最佳实践，进行信息的搜索、合成和报告。\n\n这门自定进度课程采用供应商中立的方式，使用纯 Python 代码进行教学 — 不会把细节隐藏在某个框架之下。你将清楚地看到每一步的工作原理，并学习核心概念，之后你可以使用任何流行的智能体 AI 框架，或者不使用任何框架来实践这些概念。唯一的先决条件是熟悉 Python，虽然对 大语言模型 (LLMs) 有一定的了解会更有帮助。\n\n快来加入我，让我们一起构建强大的智能体 AI 系统吧！\n\n立即注册开始学习：https://t.co/FX35dloqw4"
  },
  {
    "id": "1973090336068215058",
    "url": "https://x.com/AndrewYNg/status/1973090336068215058",
    "text": "Announcing a significant upgrade to Agentic Document Extraction! \n\nLandingAI's new DPT (Document Pre-trained Transformer) accurately extracts even from complex docs. For example, from large, complex tables, which is important for many finance and healthcare applications. And a new SDK makes using it require only 3 simple lines of code. Please see the video for technical details. I hope this unlocks a lot of value from the \"dark data\" currently stuck in PDF files, and that you'll build something cool with this!",
    "createdAt": "Tue Sep 30 18:19:29 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 588,
    "replyCount": 100,
    "likeCount": 3698,
    "quoteCount": 32,
    "viewCount": 283587,
    "bookmarkCount": 3884,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "隆重宣布：智能体文档提取 (Agentic Document Extraction) 功能迎来重大升级！\n\nLandingAI 全新的 DPT (文档预训练 Transformer) 模型，即使面对复杂的文档也能实现精准提取。例如，它能高效处理大型且复杂的表格，这在金融和医疗保健等多个应用领域至关重要。此外，全新的软件开发工具包 (SDK) 让用户只需三行简单的代码即可轻松上手。如需了解更多技术细节，请观看相关视频。我们希望这一创新能帮助大家从目前“沉睡”在 PDF 文件中的“黑暗数据” (dark data) 中挖掘出巨大价值，并期待您能用它创造出令人惊叹的应用！"
  },
  {
    "id": "1971312147654377823",
    "url": "https://x.com/AndrewYNg/status/1971312147654377823",
    "text": "Last week, China barred its major tech companies from buying Nvidia chips. This move received only modest attention in the media, but has implications beyond what’s widely appreciated. Specifically, it signals that China has progressed sufficiently in semiconductors to break away from dependence on advanced chips designed in the U.S., the vast majority of which are manufactured in Taiwan. It also highlights the U.S. vulnerability to possible disruptions in Taiwan at a moment when China is becoming less vulnerable.\n\nAfter the U.S. started restricting AI chip sales to China, China dramatically ramped up its semiconductor research and investment to move toward self-sufficiency. These efforts are starting to bear fruit, and China’s willingness to cut off Nvidia is a strong sign of its faith in its domestic capabilities. For example, the new DeepSeek-R1-Safe model was trained on 1000 Huawei Ascend chips. While individual Ascend chips are significantly less powerful than individual Nvidia or AMD chips, Huawei’s system-level design approach to orchestrating how a much larger number of chips work together seems to be paying off. For example, Huawei’s CloudMatrix 384 system of 384 chips aims to compete with Nvidia’s GB200, which uses 72 higher-capability chips.\n\nToday, U.S. access to advanced semiconductors is heavily dependent on Taiwan’s TSMC, which manufactures the vast majority of the most advanced chips. Unfortunately, U.S. efforts to ramp up domestic semiconductor manufacturing have been slow. I am encouraged that one fab at the TSMC Arizona facility is now operating, but issues of workforce training, culture, licensing and permitting, and the supply chain are still being addressed, and there is still a long road ahead for the U.S. facility to be a viable substitute for manufacturing in Taiwan.\n\nIf China gains independence from Taiwan manufacturing significantly faster than the U.S., this would leave the U.S. much more vulnerable to possible disruptions in Taiwan, whether through natural disasters or man-made events. If manufacturing in Taiwan is disrupted for any reason and Chinese companies end up accounting for a large fraction of global semiconductor manufacturing capabilities, that would also help China gain tremendous geopolitical influence.\n\nDespite occasional moments of heightened tensions and large-scale military exercises, Taiwan has been mostly peaceful since the 1960s. This peace has helped the people of Taiwan to prosper and allowed AI to make tremendous advances, built on top of chips made by TSMC. I hope we will find a path to maintaining peace for many decades more.\n\nBut hope is not a plan. In addition to working to ensure peace, practical work lies ahead to multi-source, build more chip fabs in more nations, and enhance the resilience of the semiconductor supply chain. Dependence on any single manufacturer invites shortages, price spikes, and stalled innovation the moment something goes sideways.\n\n[Original text: https://t.co/5bdEpQcaob ]",
    "createdAt": "Thu Sep 25 20:33:35 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1079,
    "replyCount": 213,
    "likeCount": 5874,
    "quoteCount": 164,
    "viewCount": 815189,
    "bookmarkCount": 1841,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "上周，中国禁止其主要科技公司购买 Nvidia 芯片。此举在媒体上仅获得了适度的关注，但其深远影响尚未被充分认识。具体来说，这表明中国在半导体领域已取得了足够的进展，足以摆脱对美国设计的先进芯片的依赖，这些芯片绝大多数在台湾制造。这也凸显了，在中国变得不那么脆弱的时刻，美国在面对台湾可能出现的干扰时所表现出的脆弱性。\n\n在美国开始限制向中国销售 AI 芯片后，中国大幅增加了其半导体研究和投资，以迈向自给自足。这些努力正在收获成果，中国切断 Nvidia 的意愿正是其对国内能力充满信心的强烈信号。例如，新的 DeepSeek-R1-Safe 模型是在 1000 颗华为昇腾 (Ascend) 芯片上训练的。虽然单个昇腾芯片的性能明显低于单个 Nvidia 或 AMD 芯片，但华为通过系统级设计方法来协调数量庞大的芯片协同工作，这种策略似乎正在奏效。例如，华为的 CloudMatrix 384 系统由 384 颗芯片组成，旨在与 Nvidia 的 GB200 竞争，后者使用了 72 颗更高性能芯片。\n\n目前，美国对先进半导体 (semiconductor) 的获取严重依赖台湾的台积电 (TSMC)，台积电生产了绝大多数最先进的芯片。不幸的是，美国提升国内半导体制造能力的努力一直进展缓慢。令我感到鼓舞的是，台积电亚利桑那工厂的一个晶圆厂 (fab) 现已投入运营，但劳动力培训、文化、许可和审批以及供应链等问题仍在解决中，美国工厂要真正成为台湾制造的可行替代品，仍有很长的路要走。\n\n如果中国摆脱对台湾制造的依赖速度明显快于美国，这将使美国在台湾可能出现的干扰（无论是自然灾害还是人为事件）面前，变得更加脆弱。如果台湾的制造因任何原因中断，而中国公司最终在全球半导体制造能力中占据很大一部分，那也将帮助中国获得巨大的地缘政治影响力。\n\n尽管偶尔会出现紧张局势加剧和大规模军事演习，但自 20 世纪 60 年代以来，台湾大体上一直保持和平。这种和平帮助台湾人民繁荣发展，并使得 AI (人工智能) 在台积电制造的芯片基础上取得了巨大进步。我希望我们能找到一条在未来几十年继续维持和平的道路。\n\n但希望并非计划。除了努力确保和平，我们面前还有实实在在的工作要做，即实现多来源供应、在更多国家建立更多芯片晶圆厂 (fab)，并增强半导体供应链的韧性。对任何单一制造商的依赖，一旦出现意外，就会导致短缺、价格飙升和创新停滞。\n\n[原文链接: https://t.co/5bdEpQcaob ]"
  },
  {
    "id": "1970899944258375866",
    "url": "https://x.com/AndrewYNg/status/1970899944258375866",
    "text": "When data agents fail, they often fail silently - giving  confident-sounding answers that are wrong, and it can be hard to figure out what caused the failure. \n\n\"Building and Evaluating Data Agents\" is a new short course created with @Snowflake and taught by @datta_cs and @_jreini that teaches you to build data agents with comprehensive evaluation built in.\n\nSkills you'll gain:\n- Build reliable LLM data agents using the Goal-Plan-Action framework and runtime evaluations that catch failures mid-execution\n- Use OpenTelemetry tracing and evaluation infrastructure to diagnose exactly where agents fail and systematically improve performance\n- Orchestrate multi-step workflows across web search, SQL, and document retrieval in LangGraph-based agents\n\nThe result: visibility into every step of your agent's reasoning, so if something breaks, you have a systematic approach to fix it. \n\nSign up to get started: https://t.co/jGQQcU6X46",
    "createdAt": "Wed Sep 24 17:15:38 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 310,
    "replyCount": 65,
    "likeCount": 1398,
    "quoteCount": 7,
    "viewCount": 95046,
    "bookmarkCount": 833,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "当数据智能体 (data agents) 出错时，它们往往会悄无声息地失败——给出听起来很有把握但实则错误的答案，而且很难弄清楚究竟是哪里出了问题。\n\n“构建和评估数据智能体 (Building and Evaluating Data Agents)”是一门新推出的短期课程，由 @Snowflake 合作创建，@datta_cs 和 @_jreini 教授。它将教你如何构建具备全面评估功能的数据智能体。\n\n你将掌握的技能包括：\n- 使用“目标-计划-行动 (Goal-Plan-Action)”框架和运行时评估 (runtime evaluations) 功能，构建可靠的大语言模型 (LLM) 数据智能体，这些评估能在执行过程中及时捕获错误。\n- 运用 OpenTelemetry 追踪 (tracing) 和评估基础设施 (evaluation infrastructure)，精确诊断智能体失败的具体环节，并系统性地提升性能。\n- 在基于 LangGraph 的智能体中，编排跨越网络搜索 (web search)、SQL 查询和文档检索 (document retrieval) 的多步骤工作流。\n\n最终效果是：你将清晰地了解智能体推理 (reasoning) 的每一步，因此一旦出现问题，你就能有条不紊地进行修复。\n\n立即注册开始学习：https://t.co/jGQQcU6X46"
  },
  {
    "id": "1969871958365163539",
    "url": "https://x.com/AndrewYNg/status/1969871958365163539",
    "text": "My heart goes out to all the families and individuals anxious over their futures following the abrupt and chaotic announcement of H-1B visa changes. \n\nAmerica should be working to attract more skilled talent, not create uncertainly that turns them away. To all legal immigrants and H1-B holders: I support and appreciate you.",
    "createdAt": "Sun Sep 21 21:10:47 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 581,
    "replyCount": 599,
    "likeCount": 7068,
    "quoteCount": 48,
    "viewCount": 524531,
    "bookmarkCount": 483,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "H-1B签证政策的突然且混乱的宣布，让许多家庭和个人对未来充满了焦虑，我对此深表同情。\n\n美国应该致力于吸引更多有技能的人才，而不是制造不确定性，反而将他们拒之门外。我支持并感谢所有合法移民和H1-B持有者。"
  },
  {
    "id": "1968710105924280352",
    "url": "https://x.com/AndrewYNg/status/1968710105924280352",
    "text": "Video of the panel at Buildathon: https://t.co/rYevXjv1vZ",
    "createdAt": "Thu Sep 18 16:14:00 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 13,
    "replyCount": 1,
    "likeCount": 60,
    "quoteCount": 1,
    "viewCount": 53921,
    "bookmarkCount": 72,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "Buildathon 活动中小组讨论的视频：https://t.co/rYevXjv1vZ"
  },
  {
    "id": "1968710001079501303",
    "url": "https://x.com/AndrewYNg/status/1968710001079501303",
    "text": "Automated software testing is growing in importance in the era of AI-assisted coding. Agentic coding systems accelerate development but are also unreliable. Agentic testing — where you ask AI to write tests and check your code against them — is helping. Automatically testing  infrastructure software components that you intend to build on top of is especially helpful and results in more stable infrastructure and less downstream debugging.\n\nSoftware testing methodologies such as Test Driven Development (TDD), a test-intensive approach that involves first writing rigorous tests for correctness and only then making progress by writing code that passes those tests, are an important way to find bugs. But it can be a lot of work to write tests. (I personally never adopted TDD for that reason.) Because AI is quite good at writing tests, agentic testing enjoys growing attention.\n\nFirst, coding agents do misbehave! My teams use them a lot, and we have seen:\n- Numerous bugs introduced by coding agents, including subtle infrastructure bugs that take humans weeks to find.\n- A security loophole that was introduced into our production system when a coding agent made password resets easier to simplify development.\n- Reward hacking, where a coding agent modified test code to make it easier to pass the tests.\n- An agent running \"rm *.py\" in the working directory, leading to deletion of all of a project's  code (which, fortunately, was backed up on github).\n\nIn the last example, when pressed, the agent apologized and agreed “that was an incredibly stupid mistake.” This made us feel better, but the damage had already been done!\n\nI love coding agents despite such mistakes and see them making us dramatically more productive. To make them more reliable, I’ve found that prioritizing where to test helps.\n\nI rarely write (or direct an agent to write) extensive tests for front-end code. If there's a bug, hopefully it will be easy to see and also cause little lasting damage. For example, I find generated code’s front-end bugs, say in the display of information on a web page, relatively easy to find. When the front end of a web site looks wrong, you’ll see it immediately, and you can tell the agent and have it iterate to fix it. (A more advanced technique: Use MCP to let the agent integrate with software like Playwright to automatically take screenshots, so it can autonomously see if something is wrong and debug.)\nIn contrast, back-end bugs are harder to find. I’ve seen subtle infrastructure bugs — for example, one that led to a corrupted database record only in certain corner cases — that took a long time to find. Putting in place rigorous tests for your infrastructure code might help spot these problems earlier and save you many hours of challenging debugging.\n\nBugs in software components that you intend to build on top of lead to downstream bugs that can be hard to find. Further, bugs in a component that’s deep in a software stack — and that you build multiple abstraction layers on top of — might surface only weeks or months later, long after you’ve forgotten what you were doing while building this specific component, and be really hard to identify and fix. This is why testing components deep in your software stack is especially important. Meta’s mantra “Move fast with stable infrastructure” (which replaced “move fast and break things”) still applies today. Agentic testing can help you make sure you have good infrastructure for you and others to build on!\n\nAt AI Fund and https://t.co/zpIxRSuky4’s recent Buildathon, we held a panel discussion with experts in agentic coding (Michele Catasta, President at Replit; Chao Peng, Principal Research Scientist at Trae; and Paxton Maeder-York, Venture Partnerships at Anthropic; moderated by AI Fund’s Eli Chen), where the speakers shared best practices. Testing was one of the topics discussed. That panel was one of my highlights of Buildathon and you can watch the video on YouTube.\n\n[Original text: https://t.co/B1sQ5oDnCU ]",
    "createdAt": "Thu Sep 18 16:13:35 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 208,
    "replyCount": 71,
    "likeCount": 1371,
    "quoteCount": 26,
    "viewCount": 176967,
    "bookmarkCount": 1103,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "在 AI 辅助编程时代，自动化软件测试的重要性日益增长。尽管智能体编程系统 (Agentic coding systems) 能显著加速开发，但它们也并非完全可靠。在这种背景下，智能体测试 (Agentic testing)——即让 AI 编写测试用例并用它们来检查你的代码——正发挥着越来越大的作用。尤其值得一提的是，自动测试那些你打算作为基础来构建其他功能的基础设施软件组件，能有效提升基础设施的稳定性，并大大减少后续的调试工作。\n\n诸如测试驱动开发 (Test Driven Development, TDD) 之类的软件测试方法论，是发现 Bug 的重要途径。TDD 是一种高度依赖测试的方法：它要求开发者首先为代码的正确性编写严谨的测试，然后才着手编写能够通过这些测试的代码。然而，编写测试往往是一项繁重的工作。 (我个人就因为这个原因从未真正采纳 TDD。) 但由于 AI 在编写测试方面表现出色，智能体测试正受到越来越多的关注。\n\n首先，编程智能体确实会“犯错”！我的团队大量使用它们，并亲身经历了以下问题：\n- 编程智能体引入了大量 Bug，其中不乏需要人类花费数周时间才能发现的隐蔽基础设施 Bug。\n- 一个编程智能体为了简化开发，将密码重置流程变得过于简单，结果给我们的生产系统制造了一个安全漏洞。\n- 奖励作弊 (Reward hacking)：编程智能体自行修改了测试代码，只为更容易通过测试。\n- 一个智能体在工作目录中执行了“rm *.py”命令，导致一个项目的所有代码被删除 (幸运的是，这些代码已在 GitHub 上进行了备份)。\n\n对于最后一个例子，当被“追问”时，该智能体表示歉意，并承认“那是一个极其愚蠢的错误”。虽然这让我们感觉好了一些，但损害已经造成了！\n\n尽管编程智能体会犯下这类错误，但我依然非常喜欢它们，并认为它们能极大地提高我们的生产力。为了让智能体更加可靠，我发现优先考虑测试的重点区域至关重要。\n\n我很少会为前端代码编写 (或指挥智能体编写) 大量测试。如果前端出现 Bug，通常很容易被发现，且造成的长期损害也较小。例如，我发现由 AI 生成代码导致的前端 Bug，比如网页信息显示错误，相对容易找到。当网站前端看起来不对劲时，你会立刻发现，然后可以告知智能体并让它迭代修复。 (一个更高级的技巧是：使用 MCP 让智能体与 Playwright 等软件集成，自动进行屏幕截图，这样它就能自主判断是否有问题并进行调试。)\n相比之下，后端 Bug 更难发现。我曾遇到一些隐蔽的基础设施 Bug——例如，某个只在特定极端情况下才会导致数据库记录损坏的 Bug——这类问题往往需要很长时间才能定位。为你的基础设施代码建立严格的测试，有助于更早地发现这些问题，为你节省大量耗时且复杂的调试工作。\n\n你计划在其之上构建其他功能的软件组件如果存在 Bug，会导致下游出现难以追踪的 Bug。更糟糕的是，如果 Bug 出现在软件堆栈深层的组件中——特别是你在此之上又构建了多层抽象——那么它可能要数周甚至数月后才会浮出水面，而那时你可能早已不记得在构建这个特定组件时做了什么，这会使得识别和修复 Bug 变得异常困难。这就是为什么对软件堆栈深处的组件进行测试显得尤为重要。Meta 的座右铭“快速行动，稳定基础设施” (这取代了早期的“快速行动，打破一切”) 至今仍然适用。智能体测试可以帮助你确保你和他人拥有可靠的基础设施来继续构建！\n\n在 AI Fund 和 https://t.co/zpIxRSuky4 最近举办的 Buildathon 活动中，我们邀请了智能体编程领域的专家 (Replit 总裁 Michele Catasta；Trae 首席研究科学家 Chao Peng；Anthropic 风险投资合伙人 Paxton Maeder-York；由 AI Fund 的 Eli Chen 主持)，共同举行了一场小组讨论，各位发言者分享了他们的最佳实践。测试正是讨论的主题之一。那次小组讨论是我在 Buildathon 活动中最精彩的环节之一，你可以在 YouTube 上观看相关视频。\n\n[原文链接: https://t.co/B1sQ5oDnCU ]"
  },
  {
    "id": "1968353689716035898",
    "url": "https://x.com/AndrewYNg/status/1968353689716035898",
    "text": "New short course: Build AI Apps with MCP Servers: Working with Box Files, built with @Box and taught by  @BenAtBox , their CTO.\n\nMany AI applications require custom code for basic file operations. The Model Context Protocol (MCP) standardizes this by letting you offload file tasks to dedicated servers that provide tools an LLM can use directly.\n\nIn this course, you'll process documents stored in a Box folder using the Box MCP server. Rather than writing custom integration code to connect to the Box API and download files, you'll design your application to use the tools provided via MCP.\n\nSkills you'll gain:\n- Build an LLM-powered document processing app, using the Box MCP server to access files\n- Design a multi-agent system using Google's Agent Development Kit (ADK), consisting of specialized agents for file operations\n- Coordinate the multi-agent workflow through an orchestrator that uses the Agent2Agent (A2A) protocol to connect to the agents\n\nYou'll start with a local file-processing app, refactor it to work with Box's MCP server, then evolve it into a multi-agent system.\n\nSign up here: https://t.co/FitKgvGnpb",
    "createdAt": "Wed Sep 17 16:37:44 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 160,
    "replyCount": 39,
    "likeCount": 717,
    "quoteCount": 8,
    "viewCount": 76736,
    "bookmarkCount": 502,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "<p>新的短期课程：使用 MCP 服务器构建 AI 应用：处理 Box 文件，本课程由 Box 协助开发，并由其首席技术官 (CTO) BenAtBox 亲自讲授。</p>\n<p>许多 AI 应用在执行基本文件操作时，都需要编写自定义代码。而模型上下文协议 (Model Context Protocol, MCP) 通过让您将文件任务分载给专用服务器，从而将这一过程标准化。这些服务器提供了大语言模型 (LLM) 可以直接使用的工具。</p>\n<p>在本课程中，您将学习如何使用 Box MCP 服务器来处理存储在 Box 文件夹中的文档。您无需编写自定义集成代码来连接 Box API 并下载文件，而是会设计您的应用程序，直接利用 MCP 提供的工具。</p>\n<p>您将学习以下技能：</p>\n<ul>\n<li>使用 Box MCP 服务器访问文件，构建一个 LLM 驱动的文档处理应用。</li>\n<li>利用 Google 的 Agent Development Kit (ADK) 设计一个多智能体（multi-agent）系统，其中包含专门负责文件操作的 AI 智能体 (AI Agent)。</li>\n<li>通过一个编排器协调多智能体工作流，该编排器使用 Agent2Agent (A2A) 协议连接到各 AI 智能体。</li>\n</ul>\n<p>您将从一个本地文件处理应用入手，将其重构以与 Box 的 MCP 服务器协同工作，然后逐步将其发展为一个多智能体系统。</p>\n<p>在此注册：https://t.co/FitKgvGnpb</p>"
  },
  {
    "id": "1963631698987684272",
    "url": "https://x.com/AndrewYNg/status/1963631698987684272",
    "text": "There is significant unmet demand for developers who understand AI. At the same time, because most universities have not yet adapted their curricula to the new reality of programming jobs being much more productive with AI tools, there is also an uptick in unemployment of recent CS graduates.\n\nWhen I interview AI engineers — people skilled at building AI applications — I look for people who can:\n- Use AI assistance to rapidly engineer software systems\n- Use AI building blocks like prompting, RAG, evals, agentic workflows, and machine learning to build applications\n- Prototype and iterate rapidly\n\nSomeone with these skills can get a massively greater amount done than someone who writes code the way we did in 2022, before the advent of Generative AI. I talk to large businesses every week that would love to hire hundreds or more people with these skills, as well as startups that have great ideas but not enough engineers to build them. As more businesses adopt AI, I expect this talent shortage only to grow! At the same time, recent CS graduates face an increased unemployment rate, though the underemployment rate — of graduates doing work that doesn’t require a degree — is still lower than for most other majors. This is why we hear simultaneously anecdotes of unemployed CS graduates and also of rising salaries for in-demand AI engineers.\n\nWhen programming evolved from punchcards to keyboard and terminal, employers continued to hire punchcard programmers for a while. But eventually, all developers had to switch to the new way of coding. AI engineering is similarly creating a huge wave of change.\n\nThere is a stereotype of “AI Native” fresh college graduates who outperform experienced developers. There is some truth to this. Multiple times, I have hired, for full-stack software engineering, a new grad who really knows AI over an experienced developer who still works 2022-style. But the best developers I know aren’t recent graduates (no offense to the fresh grads!). They are experienced developers who have been on top of changes in AI. The most productive programmers today  deeply understand computers, how to architect software, and how to make complex tradeoffs — and who additionally are familiar with cutting-edge AI tools.\n\nSure, some skills from 2022 are becoming obsolete. For example, a lot of coding syntax that we had to memorize back then is no longer important, since we no longer need to code by hand as much. But even if, say, 30% of CS knowledge is obsolete, the remaining 70% — complemented with modern AI knowledge — is what makes really productive developers. (Even after punch cards became obsolete, a fundamental understanding of programming was very helpful for typing code into a keyboard.)\n\nWithout understanding how computers work, you can’t just “vibe code” your way to greatness. Fundamentals are still important, and for those who additionally understand AI, job opportunities are numerous!\n\n[Original text: https://t.co/nqzPC6eUpR ]",
    "createdAt": "Thu Sep 04 15:54:14 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 591,
    "replyCount": 117,
    "likeCount": 1833,
    "quoteCount": 54,
    "viewCount": 222869,
    "bookmarkCount": 1040,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "当前，市场对那些懂得人工智能 (AI) 的开发者有着巨大的需求缺口。与此同时，由于大多数大学尚未调整其课程以适应编程工作在有了 AI 工具后生产力大幅提高的新现实，因此近年来计算机科学 (CS) 毕业生的失业率也有所上升。\n\n当我面试 AI 工程师——那些擅长构建 AI 应用的人——时，我关注的是他们是否具备以下能力：\n- 利用 AI 辅助快速构建软件系统\n- 运用提示 (prompting)、RAG (检索增强生成)、评估 (evals)、智能体 (agentic) 工作流以及机器学习等 AI 构建模块来开发应用程序\n- 快速开发原型并迭代\n\n与那些仍然按照 2022 年（即生成式 AI 出现之前）的方式编写代码的人相比，拥有这些技能的人能够完成的工作量要大得多。我每周都会与大型企业交流，他们渴望雇用数百甚至更多具备这些技能的人才；也有初创公司拥有绝妙的创意，但却没有足够的工程师来将其变为现实。随着越来越多的企业采用 AI，我预计这种人才短缺只会加剧！与此同时，尽管最近的计算机科学 (CS) 毕业生面临更高的失业率，但其就业不足率（即从事不需要学位的——工作）仍低于大多数其他专业。这正是我们为何同时听到计算机科学 (CS) 毕业生失业的传闻以及市场对 AI 工程师需求旺盛导致薪资上涨的原因。\n\n当编程从穿孔卡片演变为键盘和终端时，雇主在一段时间内仍然雇用穿孔卡片程序员。但最终，所有开发者都必须转向新的编码方式。AI 工程同样正在掀起一场巨大的变革浪潮。\n\n有一种刻板印象认为，“AI 原生”的大学应届毕业生表现会超越经验丰富的开发者。这在一定程度上是事实。我曾多次在全栈软件工程岗位上，选择雇用真正了解 AI 的应届毕业生，而非那些仍沿用 2022 年工作模式的经验丰富的开发者。但我认识的最优秀的开发者并非应届毕业生 (绝无冒犯之意！)。他们是那些一直紧跟 AI 变化的经验丰富的开发者。当今最高效的程序员，他们不仅深入理解计算机、懂得如何架构软件以及如何进行复杂的权衡，而且还熟悉前沿的 AI 工具。\n\n当然，2022 年的一些技能正在变得过时。例如，我们当时必须记住的许多编码语法已不再重要，因为我们不再需要大量地手动编写代码。但即使，比如说，30% 的计算机科学 (CS) 知识已经过时，剩下的 70%——辅以现代 AI 知识——才是造就真正高效开发者的关键。(即使穿孔卡片过时后，对编程的基本理解对于在键盘上输入代码也非常有帮助。)\n\n如果不了解计算机的工作原理，你就不能仅仅通过“凭感觉”来编写代码从而走向卓越。基础知识仍然至关重要，而对于那些额外了解 AI 的人来说，工作机会更是数不胜数！\n\n[原始文本: https://t.co/nqzPC6eUpR ]"
  },
  {
    "id": "1961118026398617648",
    "url": "https://x.com/AndrewYNg/status/1961118026398617648",
    "text": "Parallel agents are emerging as an important new direction for scaling up AI. AI capabilities have scaled with more training data, training-time compute, and test-time compute. Having multiple agents run in parallel is growing as a technique to further scale and improve performance.\n\nWe know from work at Baidu by my former team, and later OpenAI, that AI models’ performance scales predictably with the amount of data and training computation. Performance rises further with test-time compute such as in agentic workflows and in reasoning models that think, reflect, and iterate on an answer. But these methods take longer to produce output. Agents working in parallel offer another path to improve results, without making users wait.\n\nReasoning models generate tokens sequentially and can take a long time to run. Similarly, most agentic workflows are initially implemented in a sequential way. But as LLM prices per token continue to fall — thus making these techniques practical — and product teams want to deliver results to users faster, more and more agentic workflows are being parallelized.\n\nSome examples:\n- Many research agents now fetch multiple web pages and examine their texts in parallel to try to synthesize deeply thoughtful research reports more quickly.\n- Some agentic coding frameworks allow users to orchestrate many agents working simultaneously on different parts of a code base. Our short course on Claude Code shows how to do this using git worktrees.\n- A rapidly growing design pattern for agentic workflows is to have a compute-heavy agent work for minutes or longer to accomplish a task, while another agent monitors the first and gives brief updates to the user to keep them informed. From here, it’s a short hop to parallel agents that work in the background while the UI agent keeps users informed and perhaps also routes asynchronous user feedback to the other agents.\n\nIt is difficult for a human manager to take a complex task (like building a complex software application) and break it down into smaller tasks for human engineers to work on in parallel; scaling to huge numbers of engineers is especially challenging. Similarly, it is also challenging to decompose tasks for parallel agents to carry out. But the falling cost of LLM inference makes it worthwhile to use a lot more tokens, and using them in parallel allows this to be done without significantly increasing the user’s waiting time.\n\nI am also encouraged by the growing body of research on parallel agents. For example, I enjoyed reading “CodeMonkeys: Scaling Test-Time Compute for Software Engineering” by Ryan Ehrlich and others, which shows how parallel code generation helps you to explore the solution space. The mixture-of-agents architecture by Junlin Wang is a surprisingly simple way to organize parallel agents: Have multiple LLMs come up with different answers, then have an aggregator LLM combine them into the final output.\n\nThere remains a lot of research as well as engineering to explore how best to leverage parallel agents, and I believe the number of agents that can work productively in parallel — like the humans who can work productively in parallel — will be very high.\n\n[Original text, with links: https://t.co/ElcJZyzcfw ]",
    "createdAt": "Thu Aug 28 17:25:47 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 359,
    "replyCount": 115,
    "likeCount": 1849,
    "quoteCount": 38,
    "viewCount": 318804,
    "bookmarkCount": 1081,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "并行 AI 智能体 (AI Agent) 正在成为提升 AI 能力的一个重要新方向。AI 的能力提升离不开更多的训练数据、训练时计算 (training-time compute) 和测试时计算 (test-time compute)。如今，让多个 AI 智能体并行运行，正日益成为进一步扩展能力、提高性能的关键技术。\n\n我们从我前团队在 Baidu 的工作以及后来 OpenAI 的研究中了解到，AI 模型的性能可以预测地随着数据量和训练计算量而提升。通过测试时计算，性能可以得到进一步增强，例如在 AI 智能体工作流中，以及那些能够思考、反思并迭代优化答案的推理模型中。不过，这些方法通常需要更长时间才能产生最终输出。而并行工作的 AI 智能体则提供了一条新途径，可以在不让用户等待的情况下，快速提升结果质量。\n\n推理模型通常会按顺序生成 Token，这个过程可能非常耗时。同样，大多数 AI 智能体工作流最初也是以顺序方式实现的。然而，随着大语言模型 (LLM) 每 Token 的价格持续下降，使得这些技术变得越来越实用，并且产品团队也希望更快地向用户交付结果，因此越来越多的 AI 智能体工作流正在走向并行化。\n\n一些典型例子包括：\n*   许多研究型 AI 智能体现在可以并行抓取多个网页，并同时分析其文本内容，从而更快地合成出深入细致的研究报告。\n*   一些 AI 智能体编程框架允许用户协调多个 AI 智能体同时处理代码库的不同部分。我们在关于 Claude Code 的快速入门课程中展示了如何使用 git worktrees 来实现这一点。\n*   AI 智能体工作流中一个迅速兴起的设计模式是：让一个计算密集型 AI 智能体花费几分钟甚至更长时间来完成一项复杂任务，同时另一个 AI 智能体对其进行监控，并向用户提供简要更新，以便用户能及时了解进展。在此基础上，下一步很自然地就会发展到让并行 AI 智能体在后台工作，而用户界面 (UI) AI 智能体则负责向用户传达信息，甚至可能将用户异步的反馈路由给其他 AI 智能体。\n\n对人类管理者来说，将一项复杂的任务 (例如构建一个大型软件应用) 分解成更小的子任务，再分配给工程师并行完成，是相当困难的；尤其当工程师数量庞大时，挑战更大。同样，为并行 AI 智能体分解任务也面临着挑战。但是，LLM 推理成本的下降使得使用大量 Token 变得划算，而通过并行处理，可以在不显著增加用户等待时间的前提下完成这项工作。\n\n我也对并行 AI 智能体研究领域日益增长的成果感到鼓舞。例如，我非常喜欢 Ryan Ehrlich 等人撰写的《CodeMonkeys: Scaling Test-Time Compute for Software Engineering》一文，它展示了并行代码生成如何帮助我们探索更广阔的解决方案空间。Junlin Wang 提出的混合 AI 智能体架构，则提供了一种出人意料的简单方法来组织并行 AI 智能体：让多个 LLM 生成不同的答案，然后由一个聚合器 LLM (aggregator LLM) 将它们组合成最终输出。\n\n我们还有大量的研究和工程工作需要探索，才能找到充分利用并行 AI 智能体的最佳方式。我相信，能够高效并行工作的 AI 智能体数量——就像能够高效并行工作的人类一样——将会非常庞大。\n\n[原文链接： https://t.co/ElcJZyzcfw ]"
  },
  {
    "id": "1960731961494004077",
    "url": "https://x.com/AndrewYNg/status/1960731961494004077",
    "text": "Build better RAG by letting a team of agents extract and connect your reference materials into a knowledge graph. Our new short course, “Agentic Knowledge Graph Construction,” taught by @Neo4j Innovation Lead @akollegger, shows you how.\n\nKnowledge graphs are an important way to store information accurately but they are a lot of work to build manually.\n\nIn this course you’ll learn how to build a team of agents that turn data– in this case product reviews and invoices from suppliers–into structured graphs of entities and relationships for RAG.\n\nLearn how agents can automatically handle the time-consuming work of building graphs — extracting entities and relationships (e.g., Product \"contains\" Assembly, Part \"supplied_by\" Supplier, Customer review \"mentions\" Product), deduplicating them, fact-checking them, and committing them to a graph database — so your retrieval system can find right information to generate accurate output. For example, you can use agents to help trace customer complaints directly to specific suppliers, manufacturing processes, and product hierarchies, thus turning fragmented information into queryable business intelligence.\n\nSkills you’ll gain:\n- Build, store, and access knowledge graphs using the Neo4j graph database\n- Build multi-agent systems using Google’s Agent Development Kit (ADK)\n- Set up a loop of agentic workflows to propose and refine a graph schema through fact-checking\n- Connect agent-generated graphs of unstructured and structured data into a unified knowledge graph\n\nThis course gets into the practicum of why knowledge graphs give more accurate information retrieval than vector search alone, especially for high-stakes applications where precision matters more than fuzzy similarity matching.\n\nSign up here: https://t.co/2txZfYqGZ9",
    "createdAt": "Wed Aug 27 15:51:42 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 426,
    "replyCount": 57,
    "likeCount": 2371,
    "quoteCount": 18,
    "viewCount": 161192,
    "bookmarkCount": 2631,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "让一组 AI 智能体将您的参考资料提取并连接成知识图谱，从而构建更好的检索增强生成 (Retrieval-Augmented Generation，简称 RAG) 系统。我们的新短期课程《Agentic Knowledge Graph Construction》由 @Neo4j 创新负责人 @akollegger 主讲，将向您展示具体方法。\n\n知识图谱是准确存储信息的重要方式，但手动构建它们通常耗时费力。\n\n在本课程中，您将学习如何构建一个 AI 智能体团队，将数据——例如来自供应商的产品评论和发票——转化为结构化的实体和关系图谱，以支持 RAG 应用。\n\n了解 AI 智能体如何自动处理构建图谱的耗时工作：它们能够提取实体和关系（例如，产品\"包含\"组件，零件\"由\"供应商\"供应\"，客户评论\"提及\"产品），进行去重，事实核查，并将这些信息提交到图数据库。这样，您的检索系统就能找到准确的信息并生成精确的输出。例如，您可以利用 AI 智能体将客户投诉直接追溯到具体的供应商、制造流程和产品层次结构，从而将零散的信息转化为可供查询的商业智能。\n\n您将获得的技能：\n- 使用 Neo4j 图数据库构建、存储和访问知识图谱\n- 使用 Google 的 Agent Development Kit (ADK) 构建多智能体系统\n- 建立 AI 智能体工作流循环，通过事实核查来提出和完善图谱模式\n- 将 AI 智能体生成的非结构化和结构化数据图谱连接成统一的知识图谱\n\n本课程将深入探讨实践层面，解释了为什么知识图谱能够提供比单独的向量搜索更准确的信息检索，尤其适用于那些对精确度要求高于模糊相似性匹配的关键应用。\n\n在此注册：https://t.co/2txZfYqGZ9"
  },
  {
    "id": "1958595307929051587",
    "url": "https://x.com/AndrewYNg/status/1958595307929051587",
    "text": "The products that teams worked on at Buildathon: https://t.co/iA48xG9yU2",
    "createdAt": "Thu Aug 21 18:21:25 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 5,
    "replyCount": 4,
    "likeCount": 51,
    "quoteCount": 0,
    "viewCount": 26423,
    "bookmarkCount": 26,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "Buildathon 上团队完成的产品：https://t.co/iA48xG9yU2"
  },
  {
    "id": "1958595107457999073",
    "url": "https://x.com/AndrewYNg/status/1958595107457999073",
    "text": "On Saturday at the Buildathon hosted by AI Fund and https://t.co/zpIxRSuky4, over 100 developers competed to build software products quickly using AI assisted coding. I was inspired to see developers build functional products in just 1-2 hours. The best practices for rapid engineering are changing quickly along with the tools, and I loved the hallway conversations sharing tips with other developers on using AI to code!\n\nThe competitors raced to fulfill product specs like this one (you can see the full list in our github repo; link in reply): \nProject: Codebase Time Machine\nDescription: Navigate any codebase through time, understanding evolution of features and architectural decisions.\nRequirements:\n- Clone repo and analyze full git history\n- Build semantic understanding of code changes over time\n- Answer questions like “Why was this pattern introduced?” or “Show me how auth evolved”\n- Visualize code ownership and complexity trends\n- Link commits to business features/decisions\n\nTeams had 6½ hours to build 5 products. And many of them managed to do exactly that! They created fully functional applications with good UIs and sometimes embellishments.\n\nWhat excites me most isn’t just what can now be built in a few hours. Rather, it is that, if AI assistance lets us build basic but fully functional products this quickly, then imagine what can now be done in a week, or a month, or six months. If the teams that participated in the Buildathon had this velocity of execution and iterated over multiple cycles of getting customer feedback and using that to improve the product, imagine how quickly it is now possible to build great products.\n\nOwning proprietary software has long been a moat for businesses, because it has been hard to write complex software. Now, as AI assistance enables rapid engineering, this moat is weakening. \n\nWhile many members of the winning teams had computer science backgrounds — which does provide an edge — not all did. Team members who took home prizes included a high school senior, a product manager, and a healthcare entrepreneur who initially posted on Discord that he was “over his skis” as someone who “isn't a coder.” I was thrilled that multiple participants told me they exceeded their own expectations and discovered they can now build faster than they realized. If you haven’t yet pushed yourself to build quickly using agentic coding tools, you, too, might be surprised at what you can do!\n\nAt AI Fund and https://t.co/zpIxRSuky4, we pride ourselves on building and iterating quickly. At the Buildathon, I saw many teams execute quickly using a wide range of tools including Claude Code, GPT-5, Replit, Cursor, Windsurf, Trae, and many others.\n\nI offer my hearty congratulations to all the winners!\n- 1st Place: Milind Pathak, Mukul Pathak, and  Sapna Sangmitra (Team Vibe-as-a-Service), a team of three family members. They also received an award for Best Design.\n- 2nd Place: David Schuster, Massimiliano Viola, and Manvik Pasula. (Team Two Coders and a Finance Guy).\n- Solo Participant Award: Ivelina Dimova, who had just flown to San Francisco from Portugal, and who worked on the 5 projects not sequentially, but in parallel!\n- Graph Thinking Award: Divya Mahajan, Terresa Pan, and Achin Gupta (Team A-sync).\n- Honorable mentions went to finalists Alec Hewitt, Juan Martinez, Mark Watson and Sophia Tang (Team Secret Agents) and Yuanyuan Pan, Jack Lin, and Xi Huang (Team Can Kids).\n\nTo everyone who participated, thank you! Through events like these, I hope we can all learn from each other, encourage each other, invent new best practices, and spread the word about where agentic coding is taking software engineering.\n\n[Original text: https://t.co/wJbQMrnZdL ]",
    "createdAt": "Thu Aug 21 18:20:37 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 52,
    "replyCount": 41,
    "likeCount": 427,
    "quoteCount": 7,
    "viewCount": 56243,
    "bookmarkCount": 129,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "上周六，由 AI Fund 和 https://t.co/zpIxRSuky4 共同主办了一场“编程马拉松” (Buildathon)，吸引了 100 多名开发者参与，他们利用 AI 辅助编码 (AI assisted coding) 争相快速开发软件产品。我深受启发，看到开发者们在短短 1-2 小时内就构建出了功能完善的产品。随着工具的不断更新，快速开发的最佳实践也在迅速演变。我非常喜欢在活动现场与开发者们交流，分享他们使用 AI 进行编程的技巧！\n\n参赛者们竞相完成以下这类产品要求（您可以在我们的 GitHub 仓库中查看完整列表，链接附在文章后）：\n项目：代码库时间机器\n描述：在时间维度上浏览任何代码库，深入了解功能如何演变以及架构决策如何形成。\n要求：\n- 克隆仓库并分析完整的 Git 历史记录\n- 深入理解代码随时间变化的语义\n- 回答诸如“为什么会引入这种模式？”或“展示认证模块的演变过程”等问题\n- 以可视化方式展现代码所有权和复杂性的变化趋势\n- 将代码提交与业务功能/决策关联起来\n\n各团队有 6.5 小时来构建 5 个产品。许多团队都成功完成了任务！他们不仅创建了功能齐全、用户界面 (UI) 友好的应用程序，有些甚至还加入了额外的精巧设计。\n\n最令我兴奋的，不仅仅是现在短短几小时内能够构建出的产品。更重要的是，如果 AI 辅助能够让我们如此迅速地开发出基础但功能完整的应用，那么试想，在一周、一个月乃至六个月的时间里，又能完成怎样的壮举。如果参加这场编程马拉松的团队能够保持这样的执行速度，并且经历多个迭代周期，不断收集客户反馈并据此改进产品，那么，现在开发出卓越产品的速度将会快得惊人。\n\n长期以来，拥有专有软件一直是企业的一道“护城河”，因为编写复杂的软件并非易事。然而，随着 AI 辅助赋能快速开发，这道“护城河”正在逐渐变弱。\n\n尽管许多获奖团队成员拥有计算机科学背景——这确实能带来优势——但并非所有人都如此。获奖团队成员中，包括一名高中生、一名产品经理和一名医疗保健领域的创业者。这位创业者最初在 Discord 上发帖，称自己“超出了能力范围” (over his skis)，因为他“不是一个编码员”。我非常高兴，多位参与者告诉我，他们超出了自己的预期，发现自己现在构建产品的速度比他们想象的要快。如果你还没有尝试使用 AI 智能体 (AI Agent) 编码工具来快速构建，你或许也会对自己能完成的事情感到惊讶！\n\n在 AI Fund 和 https://t.co/zpIxRSuky4，我们以快速构建和迭代为荣。在这次编程马拉松中，我看到许多团队利用各种工具迅速投入开发，这些工具包括 Claude Code、GPT-5、Replit、Cursor、Windsurf、Trae 等等。\n\n我向所有获奖者致以衷心的祝贺！\n- 第一名：Milind Pathak、Mukul Pathak 和 Sapna Sangmitra (Team Vibe-as-a-Service)，一个由三名家庭成员组成的团队。他们还获得了最佳设计奖。\n- 第二名：David Schuster、Massimiliano Viola 和 Manvik Pasula (Team Two Coders and a Finance Guy)。\n- 个人参与奖：Ivelina Dimova，她刚从葡萄牙飞到旧金山，并且不是按顺序，而是并行地完成了 5 个项目！\n- 图形思维奖：Divya Mahajan、Terresa Pan 和 Achin Gupta (Team A-sync)。\n- 荣誉奖授予决赛选手 Alec Hewitt、Juan Martinez、Mark Watson 和 Sophia Tang (Team Secret Agents)，以及 Yuanyuan Pan、Jack Lin 和 Xi Huang (Team Can Kids)。\n\n感谢所有参与者！通过这样的活动，我希望我们都能互相学习、互相鼓励、创造新的最佳实践，并推广 AI 智能体编码正在如何重塑软件工程。\n\n[原文：https://t.co/wJbQMrnZdL ]"
  },
  {
    "id": "1958165941369634825",
    "url": "https://x.com/AndrewYNg/status/1958165941369634825",
    "text": "AI Dev 25 is coming to NYC on November 14!\n\n1,200+ developers will dive into technical topics such as:\n- Agentic AI: Multi-agent orchestration, tool use, complex reasoning chains\n- Coding with AI: Agentic coding assistants, automated testing, debugging strategies\n- Context engineering: Advanced RAG, structured context, memory systems\n- Multimodal AI: Vision-language models, audio processing, cross-modal architectures\n- Fintech applications: Fraud detection, credit modeling, regulatory compliance\n\nOur Pi Day AI Dev event sold out quickly, so we booked a bigger venue this time. Tickets available here: https://t.co/baLDrB1EPd",
    "createdAt": "Wed Aug 20 13:55:16 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 75,
    "replyCount": 56,
    "likeCount": 477,
    "quoteCount": 6,
    "viewCount": 65646,
    "bookmarkCount": 111,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "AI Dev 25 大会将于 11 月 14 日在纽约市举行！\n\n届时，将有超过 1,200 名开发者共同探索以下前沿技术主题：\n- **AI 智能体 (AI Agent)**：涵盖多智能体编排、工具使用以及复杂的推理链构建。\n- **AI 辅助编程**：包括 AI 智能体编码助手、自动化测试以及高效的调试策略。\n- **上下文工程 (Context Engineering)**：深入研究高级检索增强生成 (RAG)、结构化上下文处理和记忆系统。\n- **多模态 AI (Multimodal AI)**：聚焦视觉-语言模型、音频处理技术和跨模态架构设计。\n- **金融科技应用**：讨论欺诈检测、信用风险建模和监管合规等实际应用。\n\n我们上次的 Pi Day AI Dev 活动门票很快就抢购一空，所以这次我们特意预订了更大的场地。立即点击这里购买门票：https://t.co/baLDrB1EPd"
  },
  {
    "id": "1957475040523644936",
    "url": "https://x.com/AndrewYNg/status/1957475040523644936",
    "text": "Just as many businesses are transforming to become more capable by using AI, universities are too. I recently visited the UK to receive an honorary doctorate from the University of Exeter’s Faculty of Environment, Science and Economy. @UniofExeter The name of this faculty stood out to me as a particularly forward-looking way to organize an academic division. Having Computer Science sit alongside Environmental Science and the Business School creates natural opportunities for collaboration across these fields.\n\nLeveraging AI leads a university to do things differently. Speaking with Vice Chancellor Lisa Roberts, Deputy Vice Chancellor Timothy Quine, and CS Department Head Andrew Howes, I was struck by the university leadership’s pragmatic and enthusiastic embrace of AI. This is not a group whose primary worry is whether students will cheat using AI. This is a group that is thinking about how to create a student body that is empowered through AI, whether by teaching more students to code, helping them use AI tools effectively, or showing them what’s newly possible in their disciplines.\n\nExeter is a wonderful place to create synergies between AI, environmental science, and business. It hosts 5 of the world’s top 21 most influential climate scientists according to Reuters, and its scholars are major contributors to reports by the UN’s IPCC (Intergovernmental Panel on Climate Change) as well as pioneers in numerous areas of climate research including geoengineering, which I wrote about previously. Its Centre for Environmental Intelligence, a partnership with the Met Office (the UK’s national weather service), applies AI to massive climate datasets. More work like this is needed to understand climate change and strategies for mitigation and adaptation. Add to this its Business School — named Business School of the Year by the consultancy Times Higher Education — and you have the ingredients for building applications and pursuing interdisciplinary studies that span technological, environmental, and economic realities.\n\nHaving been born in the UK and spent most of my career in Silicon Valley, I find it exciting to see Exeter’s leadership embrace AI with an enthusiasm I more often associate with California. The UK has always punched above its weight in research, and seeing that tradition continue in the AI era is encouraging.\n\nJust as every company is becoming an AI company, every university must become an AI university — not just teaching AI, but using it to advance every field of study. This doesn’t mean abandoning disciplinary expertise. It means maintaining technical excellence while ensuring AI enhances every field.\n\nLike almost all other universities and businesses worldwide, Exeter’s AI transformation is just beginning. But the enthusiastic embrace of AI by its leadership will give it momentum. As someone who is proud to be an honorary graduate of the university, I look forward to seeing what comes next!\n\n[Original text: https://t.co/Y1PyN17Qzs ]",
    "createdAt": "Mon Aug 18 16:09:52 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 99,
    "replyCount": 36,
    "likeCount": 509,
    "quoteCount": 4,
    "viewCount": 53666,
    "bookmarkCount": 32,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "正如许多企业都在积极运用 人工智能 (AI) 来提升自身能力一样，大学也在经历一场类似的转型。我最近访问了英国，并在埃克塞特大学 (University of Exeter) 的环境、科学与经济学院荣幸地获得了荣誉博士学位。 @UniofExeter 这个学院的名称给我留下了深刻印象，因为它体现了一种特别具有前瞻性的学术部门组织方式。将计算机科学 (Computer Science) 与环境科学 (Environmental Science) 和商学院 (Business School) 放在一起，为这些领域之间的交叉合作创造了天然的机遇。\n\n善用 AI 会促使大学以全新的方式开展工作。在与副校长 Lisa Roberts、副教务长 Timothy Quine 以及计算机科学系主任 Andrew Howes 交流时，大学领导层对 AI 务实而热情的态度令我印象深刻。他们关心的重点并非学生是否会利用 AI 作弊，而是在思考如何通过 AI 赋能学生群体，无论是通过教授更多学生编程技能，帮助他们有效利用 AI 工具，还是向他们展示在各自学科领域中可能实现的新突破。\n\n埃克塞特大学是促成 AI、环境科学和商业之间协同效应的理想之地。根据路透社 (Reuters) 的报道，该校拥有全球最具影响力的 21 位气候科学家中的 5 位，其学者也是联合国政府间气候变化专门委员会 (UN’s IPCC - Intergovernmental Panel on Climate Change) 报告的主要撰稿人，并且在包括地球工程 (geoengineering) 在内的众多气候研究领域都是先驱者，我此前也曾撰文探讨过地球工程。该校的环境智能中心 (Centre for Environmental Intelligence) 与英国国家气象局 (Met Office) 合作，将 AI 应用于海量的气候数据集。我们需要更多类似的工作来深入理解气候变化及其减缓和适应战略。再加上该校的商学院——被咨询公司 Times Higher Education 评为“年度商学院”——这些条件共同为开发跨越技术、环境和经济现实的应用，以及开展跨学科研究奠定了基础。\n\n我出生在英国，职业生涯的大部分时间都在硅谷 (Silicon Valley) 度过，因此我很高兴看到埃克塞特大学的领导层以我通常在加利福尼亚 (California) 才能看到的热情拥抱 AI。英国在研究领域一直都表现出色，能够看到这一传统在 AI 时代得以延续，这令人非常鼓舞。\n\n正如每家公司都在转型成为一家 AI 公司一样，每所大学也必须成为一所 AI 大学——这不仅仅意味着教授 AI 课程，更重要的是要利用 AI 来推动每个研究领域的发展。这并非要放弃学科专业知识，而是要在保持技术卓越的同时，确保 AI 能够赋能和提升每一个领域。\n\n像全球几乎所有其他大学和企业一样，埃克塞特大学的 AI 转型才刚刚起步。但其领导层对 AI 的热情接纳将为其注入强大的动力。作为该大学的荣誉校友，我对此深感自豪，并期待着看到未来将带来哪些精彩！\n\n[Original text: https://t.co/Y1PyN17Qzs ]"
  },
  {
    "id": "1955656413075919051",
    "url": "https://x.com/AndrewYNg/status/1955656413075919051",
    "text": "Buildathon: The Rapid Engineering Competition livestreams this Saturday, August 16. Top developers will compete to build 5+ products in a single day using AI coding assistants – projects that traditionally took weeks. Watch live as they advance through semifinals and finals, and see how fast software can now be built!  Register at https://t.co/3vAkmZDU4V",
    "createdAt": "Wed Aug 13 15:43:17 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 206,
    "replyCount": 57,
    "likeCount": 777,
    "quoteCount": 12,
    "viewCount": 117430,
    "bookmarkCount": 225,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "本周六，8月16日，“Buildathon：快速工程竞赛”将进行直播。顶尖开发者们将利用 AI 编码助手（AI coding assistants），在短短一天内构建出 5 个以上的项目——要知道，这些项目在过去往往需要数周才能完成。快来观看直播，见证他们如何一路晋级半决赛和决赛，亲眼看看如今软件开发的速度能有多快！立即注册，请访问 https://t.co/3vAkmZDU4V"
  },
  {
    "id": "1953509055584252013",
    "url": "https://x.com/AndrewYNg/status/1953509055584252013",
    "text": "Recently Meta made headlines with unprecedented, massive compensation packages for AI model builders exceeding $100M (sometimes spread over multiple years). With the company planning to spend $66B-72B this year on capital expenses such as data centers, a meaningful fraction of which will be devoted to AI, from a purely financial point of view, it’s not irrational to spend a few extra billion dollars on salaries to make sure this hardware is used well.\n\nA typical software-application startup that’s not involved in training foundation models might spend 70-80% of its dollars on salaries, 5-10% on rent, and 10-25% on other operating expenses (cloud hosting, software licenses, marketing, legal/accounting, etc.). But scaling up models is so capital-intensive, salaries are a small fraction of the overall expense. This makes it feasible for businesses in this area to pay their relatively few employees exceptionally well. If you’re spending tens of billions of dollars on GPU hardware, why not spend just a tenth of that on salaries? Even before Meta’s recent offers, salaries of AI model trainers have been high, with many being paid $5-10M/year, although Meta has raised these numbers to new heights.\n\nMeta carries out many activities, including run Facebook, Instagram, WhatsApp, and Oculus. But the Llama/AI-training part of its operations is particularly capital-intensive. Many of Meta’s properties rely on user-generated content (UGC) to attract attention, which is then monetized through advertising. AI is a huge threat and opportunity to such businesses: If AI-generated content (AIGC) substitutes for UGC to capture people's attention to sell ads against, this will transform the social-media landscape.\n\nThis is why Meta — like TikTok, YouTube, and other social-media properties — is paying close attention to AIGC, and why making significant investments in AI is rational. Further, when Meta hires a key employee, not only does it gain the future work output of that person, but it also potentially gets insight into a competitor’s technology, which also makes its willingness to pay high salaries a rational business move (so long as it does not adversely affect the company’s culture).\n\nThe pattern of capital-intensive businesses compensating employees extraordinarily well is not new. For example, Netflix expects to spend a huge $18B this year on content. This makes the salary expense of paying its 14,000 employees a small fraction of the total expense, which allows the company to routinely pay above-market salaries. Its ability to spend this way also shapes a distinctive culture that includes elements of “we’re a sports team, not a family” (which seems to work for Netflix but isn’t right for everyone). In contrast, a labor-intensive manufacturing business like Foxconn, which employs over 1 million people globally, has to be much more price-sensitive in what it pays people.\n\nEven a decade ago, when I led a team that worked to scale up AI, I built spreadsheets that modeled how much of my budget to allocate toward salaries and how much to allocate toward GPUs (using a custom model for how much productive output N employees and M GPUs would lead to, so I could optimize N and M subject to my budget constraint). Since then, the business of scaling up AI has skewed the spending significantly toward GPUs.\n\nI’m happy for the individuals who are getting large pay packages. And regardless of any individual's pay, I’m grateful for the contributions of everyone working in AI. Everyone in AI deserves a good salary, and while the gaps in compensation are growing, I believe this reflects the broader phenomenon that developers who work in AI, at this moment in history, have an opportunity to make a huge impact and do world-changing work.\n\n[Original text: https://t.co/5wQe7foww8 ]",
    "createdAt": "Thu Aug 07 17:30:27 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 490,
    "replyCount": 109,
    "likeCount": 3743,
    "quoteCount": 46,
    "viewCount": 478698,
    "bookmarkCount": 1760,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "最近，Meta 公司因向其 AI 模型构建者提供了前所未有的、高达上亿美元（有时分多年支付）的巨额薪酬方案而登上了新闻头条。该公司计划今年在数据中心等资本支出上花费 660 亿至 720 亿美元，其中有相当大一部分将投入到 AI 领域。从纯粹的财务角度来看，为了确保这些硬件得到充分利用而额外投入几十亿美元用于支付薪水，这笔开销是完全合理的。"
  },
  {
    "id": "1953097967361245251",
    "url": "https://x.com/AndrewYNg/status/1953097967361245251",
    "text": "I'm thrilled to announce the definitive course on Claude Code, created with @AnthropicAI and taught by Elie Schoppik @eschoppik. If you want to use highly agentic coding - where AI works autonomously for many minutes or longer, not just completing code snippets - this is it.\n\nClaude Code has been a game-changer for many developers (including me!), but there's real depth to using it well. This comprehensive course covers everything from fundamentals to advanced patterns.\n\nAfter this short course, you'll be able to:\n- Orchestrate multiple Claude subagents to work on different parts of your codebase simultaneously\n- Tag Claude in GitHub issues and have it autonomously create, review, and merge pull requests\n- Transform messy Jupyter notebooks into clean, production-ready dashboards\n- Use MCP tools like Playwright so Claude can see what's wrong with your UI and fix it autonomously\n\nWhether you're new to Claude Code or already using it, you'll discover powerful capabilities that can fundamentally change how you build software.\n\nI'm very excited about what agentic coding lets everyone now do. Please take this course!\n\nhttps://t.co/HGM8ArDalK",
    "createdAt": "Wed Aug 06 14:16:56 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1022,
    "replyCount": 137,
    "likeCount": 6791,
    "quoteCount": 105,
    "viewCount": 747408,
    "bookmarkCount": 8523,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我非常高兴地宣布，由 @AnthropicAI 合作创建、Elie Schoppik @eschoppik 亲授的 Claude Code 权威课程正式上线了。如果你想实现高度智能体编程 (agentic coding) ——让 AI 能够自主运行数分钟甚至更长时间，而不仅仅是完成代码片段——那么这门课程正是你所需要的。\n\nClaude Code 对许多开发者 (包括我本人 !) 来说，都是一个颠覆性的工具，但要用好它确实需要深入的理解和技巧。这门全面的课程将涵盖从基础知识到高级模式的所有内容。\n\n完成这门短期的课程后，你将能够：\n- 编排多个 Claude 子智能体 (subagents) 同时处理你的代码库的不同部分\n- 在 GitHub issues 中提及 Claude，让它自主创建、审查和合并拉取请求 (pull requests)\n- 将杂乱的 Jupyter notebooks 转换成整洁、可用于生产环境的仪表盘\n- 使用像 Playwright 这样的 MCP 工具，让 Claude 能够发现你的 UI (用户界面) 中存在的问题并自主修复\n\n无论你是 Claude Code 的新手，还是已经在使用它，你都将发现其强大的功能，这些功能将从根本上改变你构建软件的方式。\n\n我对智能体编程现在能让每个人做的事情感到非常兴奋。请务必参加这门课程！\n\nhttps://t.co/HGM8ArDalK"
  },
  {
    "id": "1952838045235126510",
    "url": "https://x.com/AndrewYNg/status/1952838045235126510",
    "text": "I'm thrilled @OpenAI has released two open weight models. Thank you to all my friends at OpenAI for this gift! I'm also encouraged that from my quick tests gpt-oss-120b looks strong (though we should still wait for rigorous 3rd party evals).",
    "createdAt": "Tue Aug 05 21:04:06 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 369,
    "replyCount": 74,
    "likeCount": 2696,
    "quoteCount": 11,
    "viewCount": 179472,
    "bookmarkCount": 173,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我非常高兴 @OpenAI 发布了两个开放权重模型。感谢我在 OpenAI 的所有朋友带来这份惊喜！我也很受鼓舞，从我进行的快速测试来看，gpt-oss-120b 表现非常强劲 ( 尽管我们仍需等待严谨的第三方评估结果 )。"
  },
  {
    "id": "1950941108000964654",
    "url": "https://x.com/AndrewYNg/status/1950941108000964654",
    "text": "There is now a path for China to surpass the U.S. in AI. Even though the U.S. is still ahead, China has tremendous momentum with its vibrant open-weights model ecosystem and aggressive moves in semiconductor design and manufacturing. In the startup world, we know momentum matters: Even if a company is small today, a high rate of growth compounded for a few years quickly becomes an unstoppable force. This is why a small, scrappy team with high growth can threaten even behemoths. While both the U.S. and China are behemoths, China’s hypercompetitive business landscape and rapid diffusion of knowledge give it tremendous momentum. The White House’s AI Action Plan released last week, which explicitly champions open source (among other things), is a very positive step for the U.S., but by itself it won’t be sufficient to sustain the U.S. lead.\n\nNow, AI isn’t a single, monolithic technology, and different countries are ahead in different areas. For example, even before Generative AI, the U.S. had long been ahead in scaled cloud AI implementations, while China has long been ahead in surveillance technology. These translate to different advantages in economic growth as well as both soft and hard power. Even though nontechnical pundits talk about “the race to AGI” as if AGI were a discrete technology to be invented, the reality is that AI technology will progress continuously, and there is no single finish line. If a company or nation declares that it has achieved AGI, I expect that declaration to be less a technology milestone than a marketing milestone. A slight speed advantage in the Olympic 100m dash translates to a dramatic difference between winning a gold medal versus a silver medal. An advantage in AI prowess translates into a proportionate advantage in economic growth and national power; while the impact won’t be a binary one of either winning or losing everything, these advantages nonetheless matter.\n\nLooking at Artificial Analysis and LMArena leaderboards, the top proprietary models were developed in the U.S., but the top open models come from China. Google’s Gemini 2.5 Pro, OpenAI’s o4, Anthropic’s Claude 4 Opus, and Grok 4 are all strong models. But open alternatives from China such as DeepSeek R1-0528, Kimi K2 (designed for agentic reasoning), Qwen3 variations (including Qwen3-Coder, which is strong at coding) and Zhipu’s GLM 4.5 (whose post-training software was released as open source) are close behind, and many are ahead of Meta’s Llama 4 and Google’s Gemma 3 — the U.S.’ best open-weights offerings.\n\nBecause many U.S. companies have taken a secretive approach to developing foundation models — a reasonable business strategy — the leading companies spend huge numbers of dollars to recruit key team members from each other who might know the “secret sauce“ that enabled a competitor to develop certain capabilities. So knowledge does circulate, but at high cost and slowly. In contrast, in China’s open AI ecosystem, many advanced foundation model companies undercut each other on pricing, make bold PR announcements, and poach each others’ employees and customers. This Darwinian life-or-death struggle will lead to the demise of many of the existing players, but the intense competition breeds strong companies.\n\nIn semiconductors, too, China is making progress. Huawei’s CloudMatrix 384 aims to compete with Nvidia’s GB200 high-performance computing system. While China has struggled to develop GPUs with a similar capability as Nvidia’s top-of-the-line B200, Huawei is trying to build a competitive system by combining a larger number (384 instead of 72) of lower-capability chips. China’s automotive sector once struggled to compete with U.S. and European internal combustion engine vehicles, but leapfrogged ahead by betting on electric vehicles. It remains to be seen how effective Huawei’s alternative architectures prove to be, but the U.S. export restrictions have given Huawei and other Chinese businesses a strong incentive to invest heavily in developing their own technology. Further, if China were to develop its domestic semiconductor manufacturing capabilities while the U.S. remained reliant on TSMC in Taiwan, then the U.S.’ AI roadmap would be much more vulnerable to a disruption of the Taiwan supply chain (perhaps due to a blockade or, worse, a hot war).\n\nWith the rise of electricity, the internet, and other general-purpose technologies, there was room for many nations to benefit, and the benefit to one nation hasn’t come at the expense of another. I know of businesses that, many months back, planned for a future in which China dominates open models (indeed, we are there at this moment, although the future depends on our actions). Given the transformative impact of AI, I hope all nations — especially democracies with a strong respect for human rights and the rule of law — will clear roadblocks from AI progress and invest in open science and technology to increase the odds that this technology will support democracy and benefit the greatest possible number of people.\n\n[Full text: https://t.co/jn0KNi3gmA ]",
    "createdAt": "Thu Jul 31 15:26:21 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1082,
    "replyCount": 253,
    "likeCount": 4284,
    "quoteCount": 156,
    "viewCount": 629438,
    "bookmarkCount": 1648,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "中国现在有希望在人工智能 (AI) 领域超越美国。尽管美国目前仍保持领先，但中国凭借其蓬勃发展的开源权重模型生态系统，以及在半导体设计和制造领域的积极布局，正积蓄着巨大的发展势头。在创业圈，我们都明白“势头”的重要性：一家公司即便今天规模不大，但如果能保持高速增长，几年时间的复合效应很快就会使其发展成为一股不可阻挡的力量。这就是为什么一个虽然规模小、但充满活力的团队，凭借高增长率，也能对行业巨头构成威胁。虽然美国和中国都可称得上是巨无霸，但中国竞争异常激烈的商业环境和知识的快速传播赋予了它巨大的发展动力。白宫上周发布的 AI 行动计划，明确支持开源 (Open Source) 等策略，对美国而言是一个非常积极的信号，但仅凭它本身不足以维持美国在 AI 领域的领先地位。\n\n需要明确的是，AI 并非一项单一、整体 (Monolithic) 的技术，不同国家在不同领域各有优势。例如，即使在生成式 AI (Generative AI) 兴起之前，美国在规模化云 AI 实施方面就长期领先，而中国则在监控技术方面占据上风。这些优势继而转化为经济增长以及软实力和硬实力方面的不同优势。尽管一些非技术评论员将“通用人工智能 (AGI) 竞赛”描绘成一场旨在发明某种独立技术的角逐，但现实是 AI 技术将持续演进，没有一个明确的终点线。如果一家公司或国家宣布已实现 AGI，我预计那与其说是一个技术里程碑，不如说是一个营销里程碑。在奥运会 100 米短跑中，哪怕是微小的速度优势，也意味着金牌和银牌之间的巨大差异。同理，在 AI 实力上的优势，将转化为经济增长和国家实力的相应优势；虽然这种影响不会是全盘皆赢或全盘皆输的二元结果，但这些优势无疑至关重要。\n\n审视 Artificial Analysis 和 LMArena 的排行榜，我们可以发现顶级的专有模型来自美国，而顶级的开放模型则源自中国。Google 的 Gemini 2.5 Pro、OpenAI 的 o4、Anthropic 的 Claude 4 Opus 和 Grok 4 都是性能强劲的模型。但来自中国的开放替代品，如 DeepSeek R1-0528、Kimi K2 (专为 AI 智能体 (AI Agent) 推理设计)、Qwen3 系列模型 (包括在编码方面表现出色的 Qwen3-Coder) 以及智谱的 GLM 4.5 (其后期训练软件已作为开源发布)，正紧随其后，其中许多甚至超越了 Meta 的 Llama 4 和 Google 的 Gemma 3 — 这两款是美国目前最优秀的开源权重产品。\n\n由于许多美国公司在开发基础模型 (Foundation Models) 时采取了保密策略 — 这无疑是一种合理的商业策略 — 因此，领先公司不惜投入巨资，相互挖角核心团队成员，以期获得竞争对手开发某些能力的“独家秘诀”。因此，知识确实在流通，但其代价高昂且速度缓慢。相比之下，在中国开放的 AI 生态系统中，许多先进的基础模型公司为了争夺市场，不仅在定价上相互竞争，还会发布大胆的公关声明，甚至相互挖走员工和客户。这种达尔文式的“优胜劣汰”竞争，虽然会导致许多现有参与者被淘汰，但激烈的竞争无疑会孕育出更强大的公司。\n\n在半导体领域，中国同样在取得进展。华为的 CloudMatrix 384 系统旨在与英伟达的 GB200 高性能计算系统一较高下。尽管中国在开发与英伟达顶级 B200 性能相当的图形处理器 (GPU) 方面仍面临挑战，但华为正尝试通过组合更多数量 (384 个而非 72 个) 的性能稍低的芯片来构建一个具有竞争力的系统。中国汽车行业曾一度难以与美国和欧洲的内燃机汽车竞争，但通过大力发展电动汽车实现了弯道超车。华为的替代架构最终效果如何尚待观察，但美国的出口限制已经为华为及其他中国企业提供了强大的动力，促使它们大力投资开发自身技术。此外，如果中国能够发展起国内的半导体制造能力，而美国仍依赖台湾的台积电 (TSMC)，那么美国的 AI 发展路线图将更容易受到台湾供应链中断 (可能由于封锁，甚至更糟的，一场热战) 的影响，变得更为脆弱。\n\n随着电力、互联网以及其他通用技术 (General-Purpose Technologies) 的兴起，许多国家都从中受益，一个国家获得的益处并未以牺牲另一个国家为代价。我了解到一些企业早在数月前就已预见到中国主导开放模型的未来 (事实上，我们目前正处于这一时刻，尽管未来仍取决于我们的行动)。鉴于 AI 具有变革性影响，我衷心希望所有国家 — 尤其是那些高度尊重人权和法治的民主国家 — 能够为 AI 进步清除障碍，并积极投资于开放科学和技术，从而增加这项技术支持民主并惠及尽可能多民众的可能性。\n\n [原文链接: https://t.co/jn0KNi3gmA ]"
  },
  {
    "id": "1948032883664519244",
    "url": "https://x.com/AndrewYNg/status/1948032883664519244",
    "text": "Announcing our new event - Buildathon: The Rapid Engineering Competition. See the video for details, and please apply to participate!",
    "createdAt": "Wed Jul 23 14:50:06 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 524,
    "replyCount": 61,
    "likeCount": 1403,
    "quoteCount": 14,
    "viewCount": 96578,
    "bookmarkCount": 427,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "隆重推出我们的新活动——Buildathon: 快速工程竞赛！观看视频了解详情，并欢迎大家踊跃报名参与！"
  },
  {
    "id": "1947308544916889979",
    "url": "https://x.com/AndrewYNg/status/1947308544916889979",
    "text": "The invention of modern writing instruments like the typewriter made writing easier, but they also led to the rise of writer’s block, where deciding what to write became the bottleneck. Similarly, the invention of agentic coding assistants has led to a new builder’s block, where the holdup is deciding what to build. I call this the Product Management Bottleneck.\n\nProduct management is the art and science of deciding what to build. Because highly agentic coding accelerates the writing of software to a given product specification, deciding what to build is the new bottleneck, especially in early-stage projects. As the teams I work with take advantage of agentic coders, I increasingly value product managers (PMs) who have very high user empathy and can make product decisions quickly, so the speed of product decision-making matches the speed of coding.\n\nPMs with high user empathy can make decisions by gut and get them right a lot of the time. As new information comes in, they can keep refining their mental models of what users like or do not like — and thereby refine their gut — and keep making fast decisions of increasing quality.\n\nMany tactics are available to get user feedback and other forms of data that shape our beliefs about users. They include  conversations with a handful of users, focus groups, surveys, and A/B tests on scaled products. But to drive progress at GenAI speed, I find that synthesizing all these sources of data in a PM's gut helps us move faster.\n\nLet me illustrate with an example. Recently, my team debated which of 4 features users would prefer. I had my instincts, but none of us were sure, so we surveyed about 1,000 users. The results contradicted my initial beliefs — I was wrong! So what was the right thing to do at this point?\n- Option 1: Go by the survey and build what users told us clearly they prefer.\n- Option 2: Examine the survey data in detail to see how it changes my beliefs about what users want. That is, refine my mental model of users. Then use my revised mental model to decide what to do.\n\nEven though some would consider Option 1 the “data-driven” way to make decisions, I consider this an inferior approach for most projects. Surveys may be flawed. Further, taking time to run a survey before making a decision results in slow decision-making.\n\nIn contrast, using Option 2, the survey results give much more generalizable information that can help me shape not just this decision, but many others as well. And it lets me process this one piece of data alongside all the user conversations, surveys, market reports, and observations of user behavior when they’re engaging with our product to form a much fuller view on how to serve users. Ultimately, that mental model drives my product decisions.\n\nOf course, this technique does not always scale. For example, with programmatic online advertising in which AI might try to optimize the number of clicks on ads shown, an automated system conducts far more experiments in parallel and gathers data on what users do and do not click on, to filter through a PM's mental model of users. When a system needs to make a huge number of decisions, such as what ads to show (or products to recommend) on a huge number of pages, PM review and human intuition do not scale.\n\nBut in products where a team is making a small number of critical decisions such as what key features to prioritize, I find that data — used to help build a good mental model of the user, which is then applied to make decisions very quickly — is still the best way to drive rapid progress and relieve the Product Management Bottleneck.\n\n[Original text: https://t.co/1tulDs3k7U ]",
    "createdAt": "Mon Jul 21 14:51:51 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 287,
    "replyCount": 74,
    "likeCount": 1233,
    "quoteCount": 28,
    "viewCount": 341746,
    "bookmarkCount": 698,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "现代写作工具，比如打字机的发明，让写作变得更轻松，但也因此催生了“作家之困 (writer’s block)”——写什么反而成了最大的难题。类似地，能动性编码助手 (agentic coding assistants) 的出现，也带来了一种新的“构建者之困 (builder’s block)”——此时，决定要构建什么成了阻碍。我将这种现象称为“产品管理瓶颈 (Product Management Bottleneck)”。\n\n产品管理的核心，正是关于如何决定构建什么的艺术和科学。由于高能动性的编码技术能够极大加速根据既定产品规范编写软件的过程，在项目早期阶段，决定“做什么”就成了新的瓶颈。在我所合作的团队中，随着他们越来越多地借助能动性编码器 (agentic coders)，我发现那些拥有极高用户同理心 (user empathy) 且能迅速做出产品决策的产品经理 (PMs) 变得尤为宝贵，因为这样才能让产品决策的速度与编码效率同步。\n\n具备高用户同理心的产品经理，往往能凭直觉做出决策，并且在大多数情况下都是正确的。当新的信息不断涌入时，他们能持续完善自己对用户喜好和行为的**心理模型 (mental models)**（即对用户需求和偏好的内在理解）——从而不断提升直觉的准确性——并持续做出质量更高、速度更快的决策。\n\n有许多策略可以帮助我们获取用户反馈及其他形式的数据，从而形成我们对用户的认知。这些方法包括与少量用户进行访谈、组织焦点小组、开展问卷调查，以及对成熟产品进行 A/B 测试。然而，为了以 GenAI 的速度推动项目进展，我发现将所有这些数据来源融汇到产品经理的直觉中，能帮助我们行动得更快。\n\n让我用一个例子来具体说明。最近，我的团队正在讨论用户会更偏爱我们提供的四个功能中的哪一个。我有一些直觉，但我们谁都无法确定，于是我们对大约1,000名用户进行了调查。结果出乎我的意料——我错了！那么，在这种情况下，正确的做法应该是什么呢？\n- 选项 1: 完全依据调查结果，去构建用户明确表示喜欢的功能。\n- 选项 2: 详细审视调查数据，看看它如何改变我对用户需求的理解。换句话说，是完善我对用户的心理模型。然后根据我修正后的心理模型来决定下一步行动。\n\n尽管有些人可能会认为选项 1 是“数据驱动”的决策方式，但我认为对于大多数项目来说，这是一种次优的方法。调查本身可能存在缺陷。此外，在做出决策前花费时间进行调查，也会导致决策过程缓慢。\n\n相比之下，采用选项 2，调查结果能提供更具普适性的信息，不仅有助于我做出当前这个决策，还能帮助我处理许多其他决策。它让我可以将这份数据，与所有的用户访谈、调查报告、市场分析以及用户在使用我们产品时的行为观察相结合，从而对如何更好地服务用户形成一个更全面的视角。最终，正是这种深化的心理模型驱动着我的产品决策。\n\n当然，这种方法并非总是能大规模应用。例如，在程序化在线广告领域，AI 可能会尝试优化广告的点击次数。在这种情况下，自动化系统会并行执行大量实验，并收集用户点击或不点击广告的数据，而不会仅仅依赖产品经理对用户的心理模型进行过滤。当一个系统需要做出海量决策时，比如在无数页面上显示哪些广告 (或推荐哪些产品)，产品经理的审查和人类直觉就难以跟上。\n\n但在那些团队需要做出少数关键决策的产品中，例如优先开发哪些核心功能，我发现数据——用来帮助建立一个良好的用户心理模型，并基于此快速做出决策——仍然是推动快速进展和缓解产品管理瓶颈的最佳途径。\n\n[原文链接: https://t.co/1tulDs3k7U ]"
  },
  {
    "id": "1945502636012445937",
    "url": "https://x.com/AndrewYNg/status/1945502636012445937",
    "text": "Announcing a new Coursera course: Retrieval Augmented Generation (RAG)\n\nYou'll learn to build high performance, production-ready RAG systems in this hands-on, in-depth course created by https://t.co/zpIxRSuky4 and taught by @ZainHasan6, experienced AI and ML engineer, researcher, and educator. \n\nRAG is a critical component today of many LLM-based applications in customer support, internal company Q&A systems, even many of the leading chatbots that use web search to answer your questions. This course teaches you in-depth how to make RAG work well.\n\nLLMs can produce generic or outdated responses, especially when asked specialized questions not covered in its training data. RAG is the most widely used technique for addressing this. It brings in data from new data sources, such as internal documents or recent news, to give the LLM the relevant context to private, recent, or specialized information. This lets it generate more grounded and accurate responses.\n\nIn this course, you’ll learn to design and implement every part of a RAG system, from retrievers to vector databases to generation to evals. You’ll learn about the fundamental principles behind RAG and how to optimize it at both the component and whole-system levels.\n\nAs AI evolves, RAG is evolving too. New models can handle longer context windows, reason more effectively, and can be parts of complex agentic workflows. One exciting growth area is Agentic RAG, in which an AI agent at runtime (rather than it being hardcoded at development time) autonomously decides what data to retrieve, and when/how to go deeper. Even with this evolution, access to high-quality data at runtime is essential, which is why RAG is a key part of so many applications.\n\nYou'll learn via hands-on experiences to:\n- Build a RAG system with retrieval and prompt augmentation\n- Compare retrieval methods like BM25, semantic search, and Reciprocal Rank Fusion\n- Chunk, index, and retrieve documents using a Weaviate vector database and a news dataset\n- Develop a chatbot, using open-source LLMs hosted by Together AI, for a fictional store that answers product and FAQ questions\n- Use evals to drive improving reliability, and incorporate multi-modal data\n\nRAG is an important foundational technique. Become good at it through this course!\n\nPlease sign up here: https://t.co/81DSVlDEOW",
    "createdAt": "Wed Jul 16 15:15:48 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 329,
    "replyCount": 63,
    "likeCount": 1676,
    "quoteCount": 14,
    "viewCount": 121427,
    "bookmarkCount": 1428,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "Coursera 推出全新课程：检索增强生成 (Retrieval Augmented Generation, RAG)\n\n在这门由 https://t.co/zpIxRSuky4 精心打造，并由经验丰富的 AI 和机器学习 (ML) 工程师、研究员及教育家 @ZainHasan6 亲自授课的深度实践课程中，你将学习如何构建高性能、生产级的 RAG 系统。\n\n如今，RAG 已成为许多基于 大语言模型 (LLM) 的应用中的核心组件，包括客户支持、企业内部问答系统，甚至许多利用网络搜索来回答问题的顶尖聊天机器人。本课程将深入教你如何让 RAG 发挥最佳效能。\n\n大语言模型有时会给出泛泛而谈或信息过时的回答，特别是当它们被问及训练数据中未涵盖的专业问题时。RAG 是解决这一问题的最普遍采用的技术方案。它能够从新的数据源（如内部文档或最新新闻）中引入相关信息，为 大语言模型 提供与私有、专业或最新信息相关的上下文。这使得 大语言模型 能够生成更可靠、更精准的回答。\n\n在这门课程中，你将学习设计和实现 RAG 系统的每一个环节，从检索器、向量数据库，到内容生成和系统评估等各个方面。你将深入理解 RAG 背后的基本原理，并学习如何在组件层面和整个系统层面进行优化。\n\n随着 AI 的不断发展，RAG 技术也在持续演进。新模型能够处理更长的上下文窗口，进行更有效的推理，并能够融入复杂的 AI 智能体 (AI Agent) 工作流。其中一个激动人心的增长领域是 Agentic RAG，在这种模式下，AI 智能体 在运行时（而非在开发阶段进行硬编码）自主决策需要检索哪些数据，以及何时、如何进行更深层次的探索。即便有了这些新发展，在运行时访问高质量数据依然至关重要，这也是 RAG 成为众多应用核心技术的原因所在。\n\n你将通过亲手实践学习到：\n- 构建一个具备检索和提示增强功能的 RAG 系统\n- 比较各种检索方法，如 BM25、语义搜索和 Reciprocal Rank Fusion\n- 学习如何利用 Weaviate 向量数据库和新闻数据集，对文档进行分块、建立索引并高效检索\n- 开发一个聊天机器人，使用 Together AI 托管的开源 大语言模型，为一个虚构商店回答产品和常见问题\n- 通过评估来提升系统可靠性，并整合多模态数据\n\nRAG 是一项重要的基础技术，本课程将助你掌握其精髓！\n\n请在此处注册：https://t.co/81DSVlDEOW"
  },
  {
    "id": "1945148766962729370",
    "url": "https://x.com/AndrewYNg/status/1945148766962729370",
    "text": "Announcing AI Aspire, our new advisory firm to help enterprises with their AI strategy and transformation journey! We are partnering with Bain & Company and looking forward to helping businesses unlock scalable, transformative value.\n\nC-suite is now realizing that top-down leadership is needed for AI transformation. But figuring out the implications of AI on a particular business is extremely complex. The Bain team is world class at helping businesses craft strategy and navigate complex landscapes. Kirsty Tan (at AI Aspire) and I are thrilled to be working with Bain's Chuck Whitten, Sarah Elk, Erika Serow and team to help businesses.\n\nQuestions that C-suite and boards are now asking include:\n- What new products are now possible? What can we (or our competitors) now do for customers that were not possible before?\n- How do we use AI to boost productivity? What workflows can be streamlined or processes reinvented?\n- What technology investments, for example in data infrastructure, should now be prioritized? And what are the risks, for example to security and compliance?\n- What are the HR implications: What roles do we hire more or fewer of?\n- How do we bring the team with us for the journey, and enable transformation from within?\n- Does AI enable new competitors; or alternatively, are there now new markets that are possible for us to move into?\n\nTechnology is no longer merely a support system for business — it is  engine of growth for the companies nimble enough to adapt and daring enough to lead.\n\nPress release: https://t.co/cjYX3yoWEn",
    "createdAt": "Tue Jul 15 15:49:39 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 161,
    "replyCount": 66,
    "likeCount": 878,
    "quoteCount": 9,
    "viewCount": 122038,
    "bookmarkCount": 462,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "隆重推出 AI Aspire，我们全新的咨询公司，旨在助力企业制定人工智能 (AI) 战略并顺利完成转型之旅！我们正与 Bain & Company 携手合作，期待能帮助各行各业的客户释放可扩展、颠覆性的价值。\n\n如今，企业高管 (C-suite) 们正逐渐意识到，AI 转型需要自上而下的领导力。然而，要弄清楚 AI 对特定业务的深远影响，却是一项极其复杂的挑战。Bain 团队在协助企业制定战略和应对复杂局面方面堪称世界一流。AI Aspire 的 Kirsty Tan 和我本人，都非常高兴能与 Bain 的 Chuck Whitten、Sarah Elk、Erika Serow 及其团队合作，共同为企业提供帮助。\n\n企业高管和董事会目前关注的问题包括：\n- 现在有哪些全新的产品或服务成为可能？我们（或我们的竞争对手）现在能够为客户提供哪些以往无法实现的功能？\n- 我们应如何运用 AI 来大幅提升生产力？哪些工作流程可以被精简优化，或者哪些业务流程可以被彻底重塑？\n- 例如在数据基础设施方面，现在应该优先考虑哪些技术投资？同时，可能面临哪些风险，比如在安全和合规性方面？\n- 对于人力资源 (HR) 又有哪些影响：我们应该增聘哪些职位，又该缩减哪些职位？\n- 我们如何确保团队在转型过程中保持同步，并从内部激发转型的动力？\n- AI 是否会催生新的竞争对手？或者，是否存在我们能够进军的新兴市场？\n\n技术不再仅仅是业务的辅助系统，它已成为那些足够敏捷以适应变化、足够大胆以引领潮流的公司的增长引擎。\n\n新闻稿：https://t.co/cjYX3yoWEn"
  },
  {
    "id": "1943710282381180934",
    "url": "https://x.com/AndrewYNg/status/1943710282381180934",
    "text": "Last week, the United States Congress passed President Trump’s “Big Beautiful Bill.” I’m disappointed it didn’t include a proposed moratorium on U.S. state-level AI regulation. While there is a role for AI regulation, it is when the technology is new and poorly understood that lobbyists are most likely to succeed at pushing through anti-competitive regulations that hamper open-source and other beneficial AI efforts. A moratorium would have bought more time for regulators to figure out the realistic risks and rewards of AI and thereby avoid bad regulatory proposals.\n\nMany jurisdictions loosely follow this trajectory:\n- When new AI technology is still poorly understood, companies can make grandiose statements about its benefits or dangers, and both traditional and social media are ineffective at fact-checking them and tend to parrot what they say. During this initial period, businesses can get away with saying almost anything.\n- This opens opportunities for hype as well as fear mongering based on exaggerated claims about AI dangers. Some businesses exploit this opportunity to try to get regulators to pass anti-competitive laws that impede open-source and other competitors.\n- But eventually, smart regulators learn enough about AI to understand its realistic benefits and risks. For example, the U.S. Senate’s bipartisan Insight Forum on AI, which I participated in, heard from many stakeholders and came to support innovation and dismiss ill-founded fears of “AI takeover” and the like.\n\nIndeed, the European Union went through this trajectory as well. After the EU AI Act was passed, many regulators realized many of its “protections” are not actually helpful. They relaxed some of the law’s provisions to make it less stifling of innovation than many observers initially had feared.\n\nThere are AI regulations that would limit harmful applications appropriately, for example, banning non-consensual deepfake porn and preventing misleading marketing. However, many states, which have less resources than the federal government to deeply understand AI, have proposed harmful regulations, especially those that aim to regulate the technology rather than the applications.\n\nFor example:\n- California’s SB 1047 purported to impose safety requirements on frontier AI systems, but it placed ambiguous and/or technically infeasible requirements on model creators to prevent harmful downstream uses. This is akin to holding the maker of a hammer liable if someone uses it for harmful purposes. Fortunately, Governor Gavin Newsom quashed SB 1047 with a veto.\n- New York’s Responsible AI Safety and Education Act, which passed the state legislature in June and awaits Governor Kathy Hochul’s signature or veto, also places ambiguous and unreasonable requirements on model builders, purportedly to guard against theoretical “critical harms.” It would hamper open source without making anyone meaningfully safer.\n- The Texas Responsible AI Governance Act initially included many of the problematic elements of SB 1047. It would have created unreasonable requirements that model providers would have had a hard time complying with, and compliance would have amounted to safety theater that was unlikely to actually make people safer. Fortunately, as Texas regulators came to understand AI better, they significantly scaled back the law, and Governor Greg Abbott signed it into law in late June. The final law focuses on specific application areas, establishes an advisory council and regulatory sandbox, and places more burden on government agencies than private companies.\n\nSadly, I see the net impact of the regulations proposed so far as negative. Many would severely hamper innovation despite some lesser positive benefits. This is why a moratorium on state-level regulation would have been a net benefit to AI and to society. Shutting down bad regulations for a limited period would have given regulators time to figure out AI technology and ignore irresponsible fear mongering. In addition, it would have helped them avoid creating a patchwork of state-level regulations that businesses large and small have a hard time complying with.\n\nPerhaps a 10-year blanket moratorium was a step too far. A more modest, say, 2-year moratorium, and one that covered only the most problematic regulatory proposals, might have had a better chance of passing.\n\nEven though a moratorium did not make it into Trump’s bill, I hope that efforts continue in the U.S. and other nations to give regulators time to understand the real risks and benefits of AI, and not pass stifling regulations during that initial period when the technology is new and the power of fear mongering is strongest.\n\n[Original text: https://t.co/56ZkPD9ta5 ]",
    "createdAt": "Fri Jul 11 16:33:38 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 72,
    "replyCount": 39,
    "likeCount": 332,
    "quoteCount": 10,
    "viewCount": 67932,
    "bookmarkCount": 72,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "<p>上周，美国国会通过了特朗普总统的“宏大美丽法案”（Big Beautiful Bill）。我感到失望的是，这项法案并未包含一项旨在暂停美国州级人工智能 (AI) 监管的提案。尽管对 AI 进行监管是必要的，但当这项技术尚处于新生阶段、人们对其了解不足时，游说者最容易成功推动那些具有反竞争性质的法规，从而阻碍<a href=\"https://baike.baidu.com/item/%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6\">开源</a> (open-source) 和其他有益的 AI 发展。一项暂停令本可以为监管机构争取更多时间，以便他们充分理解 AI 的实际风险和潜在回报，从而避免出台不恰当的监管方案。</p>\n<p>许多司法管辖区大都遵循以下发展轨迹：</p>\n<ul>\n<li>当新兴的 AI 技术仍未被充分理解时，公司可能会对其益处或潜在危险做出夸大其词的声明，而传统媒体和社交媒体在事实核查方面往往效率不高，容易不加批判地重复这些说法。在这个初始阶段，企业几乎可以随意发表言论而无需承担太多后果。</li>\n<li>这为基于对 AI 危险的夸大描述而产生的炒作和恐慌情绪提供了滋生土壤。一些企业会利用这种机会，试图促使监管机构通过反竞争性法律，从而阻碍开源项目和其他竞争对手的发展。</li>\n<li>然而，最终，明智的监管机构会积累足够多的 AI 知识，从而能够理解其真实的益处和风险。例如，我曾参与的美国参议院两党 AI 洞察论坛，就听取了众多利益相关者的意见，并开始支持创新，同时驳斥了“AI 接管”等毫无根据的担忧。</li>\n</ul>\n<p>事实上，欧盟也经历了类似的发展轨迹。在《欧盟 AI 法案》 (EU AI Act) 通过后，许多监管机构意识到其不少“保护措施”实际上并无助益。于是，他们放宽了该法案的部分条款，使其对创新的抑制作用低于许多观察家最初的担忧。</p>\n<p>有些 AI 监管措施确实能够恰当地限制有害应用，例如，禁止未经同意的<a href=\"https://baike.baidu.com/item/%E6%B7%B1%E5%BA%A6%E4%BC%AA%E9%80%A0\">深度伪造</a> (deepfake) 色情制品，以及防止误导性营销。然而，许多州在深入理解 AI 方面，其资源不如联邦政府充足，却已经提出了一些有害的监管措施，特别是那些旨在监管技术本身而非其具体应用的提议。</p>\n<p>例如：</p>\n<ul>\n<li>加州的 SB 1047 法案声称要对<a href=\"https://baike.baidu.com/item/%E5%89%8D%E6%B2%BF%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD\">前沿 AI 系统</a> (frontier AI systems) 施加安全要求，但它对模型创建者提出了模糊不清，甚至技术上不可行的要求，以期防止有害的下游使用。这无异于如果有人用锤子做有害用途，就追究锤子制造商的责任。幸运的是，州长 Gavin Newsom 行使否决权，阻止了 SB 1047 法案的通过。</li>\n<li>纽约的《负责任 AI 安全与教育法案》已于六月通过州议会，目前正等待州长 Kathy Hochul 签署或否决。该法案同样对模型开发者施加了模糊且不合理的要求，声称是为了防范理论上的“关键危害”。在未能显著提升任何人的安全性的前提下，它反而会阻碍开源发展。</li>\n<li>德克萨斯州的《负责任 AI 治理法案》最初也包含了 SB 1047 法案中的许多问题要素。它曾提出一些不合理的要求，使得模型提供商将难以遵守；而这种遵守行为不过是<a href=\"https://baike.baidu.com/item/%E5%AE%89%E5%85%A8%E5%89%A7%E5%9C%BA\">安全剧场</a> (safety theater)，不太可能真正提高人们的安全性。幸运的是，随着德克萨斯州监管机构对 AI 的理解加深，他们大幅缩减了该法案的范围，州长 Greg Abbott 已于六月下旬将其签署成为法律。最终的法律侧重于特定的应用领域，建立了咨询委员会和<a href=\"https://baike.baidu.com/item/%E7%9B%91%E7%AE%A1%E6%B2%99%E7%9B%92\">监管沙盒</a> (regulatory sandbox)，并将更多责任和负担分配给政府机构，而非私人企业。</li>\n</ul>\n<p>遗憾的是，我认为迄今为止提出的这些法规的总体影响是负面的。许多法规尽管有一些微小的积极益处，却会严重阻碍创新。这就是为什么暂停州级监管本可以为 AI 和整个社会带来显著的总体益处。在有限的时间内阻止不当法规，能让监管机构有时间弄清楚 AI 技术，并忽略不负责任的恐慌散布。此外，这还有助于他们避免形成一套让大小企业都难以遵守的零散州级法规。</p>\n<p>或许全面的 10 年暂停令可能过于激进。一个更温和的方案，例如，为期 2 年的暂停令，并且仅限于处理那些问题最为突出的监管提议，或许会有更大的通过机会。</p>\n<p>尽管暂停令最终未能纳入特朗普的法案，但我希望美国及其他国家能继续努力，给予监管机构足够的时间来理解 AI 的真实风险和益处，避免在该技术尚新颖、恐慌散布影响力最强的初始阶段，就通过扼杀创新的法规。</p>\n<p>[原文链接：https://t.co/56ZkPD9ta5 ]</p>"
  },
  {
    "id": "1943385392469962846",
    "url": "https://x.com/AndrewYNg/status/1943385392469962846",
    "text": "My talk at YC Startup School on how to build AI startups. I share tips from @AI_Fund on how to use AI to build fast. Let me know what you think!",
    "createdAt": "Thu Jul 10 19:02:38 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 323,
    "replyCount": 53,
    "likeCount": 1471,
    "quoteCount": 11,
    "viewCount": 178548,
    "bookmarkCount": 1153,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isQuote": true,
    "isPinned": false,
    "tranlastedContent": "我在 YC Startup School 做了关于如何建立 AI 初创公司的演讲。我分享了来自 @AI_Fund 的经验和技巧，讲解如何利用 AI 快速构建和发展。欢迎大家提出宝贵意见！"
  },
  {
    "id": "1943319708859834499",
    "url": "https://x.com/AndrewYNg/status/1943319708859834499",
    "text": "Agentic Document Extraction now supports field extraction! Many doc extraction use cases extract specific fields from forms and other structured documents. You can now input a picture or PDF of an invoice, request the vendor name, item list, and prices, and get back the extracted fields. Or input a medical form and specify a schema to extract patient name, patient ID, insurance number, etc. \n\nOne cool feature: If you don't feel like writing a schema (json specification of what fields to extract) yourself, upload one sample document and write a natural language prompt saying what you want, and we automatically generate a schema for you.\n\nSee the video for details!",
    "createdAt": "Thu Jul 10 14:41:38 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 361,
    "replyCount": 67,
    "likeCount": 2277,
    "quoteCount": 18,
    "viewCount": 190568,
    "bookmarkCount": 2055,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "Agentic Document Extraction 现在支持**字段提取** (field extraction) 功能！许多文档提取的场景都需要从表格或其他结构化文档中提取特定字段。现在，你可以上传一张发票的图片或 PDF，请求提取供应商名称、项目列表和价格，然后就能得到这些已提取的字段信息。或者，你也可以上传一份医疗表格，并指定一个**模式** (schema) 来提取患者姓名、患者 ID、保险号码等。\n\n其中一项很棒的功能是：如果你不想自己动手编写**模式** (schema) (也就是定义要提取哪些字段的 JSON **规范** (JSON specification) )，只需上传一份样本文档，并用**自然语言提示** (natural language prompt) 说明你的需求，我们就会自动为你生成一个模式。\n\n观看视频了解更多详情！"
  },
  {
    "id": "1942952817049915596",
    "url": "https://x.com/AndrewYNg/status/1942952817049915596",
    "text": "New Course: Post-training of LLMs\n\nLearn to post-train and customize an LLM in this short course, taught by @BanghuaZ, Assistant Professor at  the University of Washington @UW, and co-founder of @NexusflowX.\n\nTraining an LLM to follow instructions or answer questions has two key stages: pre-training and post-training. In pre-training, it learns to predict the next word or token from large amounts of unlabeled text. In post-training, it learns useful behaviors such as following instructions, tool use, and reasoning.\n\nPost-training transforms a general-purpose token predictor—trained on trillions of unlabeled text tokens—into an assistant that follows instructions and performs specific tasks. Because it is much cheaper than pre-training, it is practical for many more teams to incorporate post-training methods into their workflows than pre-training.\n\nIn this course, you’ll learn three common post-training methods—Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Online Reinforcement Learning (RL)—and how to use each one effectively. With SFT, you train the model on pairs of input and ideal output responses. With DPO, you provide both a preferred (chosen) and a less preferred (rejected) response and train the model to favor the preferred output. With RL, the model generates an output, receives a reward score based on human or automated feedback, and updates the model to improve performance.\n\nYou’ll learn the basic concepts, common use cases, and principles for curating high-quality data for effective training. Through hands-on labs, you’ll download a pre-trained model from Hugging Face and post-train it using SFT, DPO, and RL to see how each technique shapes model behavior.\n\nIn detail, you’ll:\n- Understand what post-training is, when to use it, and how it differs from pre-training.\n- Build an SFT pipeline to turn a base model into an instruct model.\n- Explore how DPO reshapes behavior by minimizing contrastive loss—penalizing poor responses and reinforcing preferred ones.\n- Implement a DPO pipeline to change the identity of a chat assistant.\n- Learn online RL methods such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), and how to design reward functions.\n- Train a model with GRPO to improve its math capabilities using a verifiable reward.\n\nPost-training is one of the most rapidly developing areas of LLM training. Whether you’re building a high-accuracy context-specific assistant, fine-tuning a model's tone, or improving task-specific accuracy, this course will give you experience with the most important techniques shaping how LLMs are post-trained today.\n\nPlease sign up here: https://t.co/efSt2FnVNS",
    "createdAt": "Wed Jul 09 14:23:44 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 329,
    "replyCount": 38,
    "likeCount": 1488,
    "quoteCount": 10,
    "viewCount": 122792,
    "bookmarkCount": 1185,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程：大语言模型的后训练\n\n在这门短课程中，你将学习如何对大语言模型 (LLM) 进行后训练和定制。课程由华盛顿大学 @UW 助理教授、@NexusflowX 联合创始人 @BanghuaZ 授课。\n\n训练一个大语言模型以使其能够遵循指令或回答问题，通常需要经历两个关键阶段：预训练和后训练。在预训练阶段，模型从海量的未标记文本数据中学习预测下一个词或 Token。而在后训练阶段，模型则被训练出各种实用行为，比如理解并遵循指令、使用工具以及进行推理。\n\n后训练是一个关键的转化过程，它能将一个在数万亿个未标记文本 Token 上训练出的、只能预测下一个 Token 的通用模型，转变为一个能够遵循指令并执行特定任务的智能助手。由于后训练的成本远低于预训练，因此对于更多团队来说，将后训练方法整合到其工作流程中，比进行预训练更为实际可行。\n\n在本课程中，你将学习三种常见的后训练方法——监督微调 (Supervised Fine-Tuning, SFT)、直接偏好优化 (Direct Preference Optimization, DPO) 和在线强化学习 (Online Reinforcement Learning, RL)——以及如何有效地运用它们。\n- 使用 SFT，你可以在一系列输入和理想输出的响应对上训练模型。\n- 使用 DPO，你同时提供一个偏好的（选定的）响应和一个不那么偏好的（被拒绝的）响应，然后训练模型使其更倾向于生成偏好的输出。\n- 而 RL 方法则是模型生成一个输出后，根据人类或自动反馈获得一个奖励分数，然后模型会根据这个分数进行更新，从而提升性能。\n\n你将学习基本概念、常见应用场景，以及如何筛选和整理高质量数据以进行高效训练。通过实践操作，你将从 Hugging Face 下载一个预训练模型，并分别使用 SFT、DPO 和 RL 对其进行后训练，亲身感受每种技术如何塑造模型的行为。\n\n具体来说，你将：\n- 理解什么是后训练，何时应该使用它，以及它与预训练有何不同。\n- 构建一个 SFT 流程，将一个基础模型转化为一个能够理解指令的模型。\n- 探索 DPO 如何通过最小化对比损失来改变模型行为——即通过惩罚不佳响应并强化偏好响应来实现。\n- 实现一个 DPO 流程，以改变一个聊天助手的个性或风格。\n- 学习在线强化学习方法，例如近端策略优化 (Proximal Policy Optimization, PPO) 和组相对策略优化 (Group Relative Policy Optimization, GRPO)，并了解如何设计奖励函数。\n- 使用 GRPO 训练一个模型，并通过可验证的奖励机制提升其数学能力。\n\n后训练是大语言模型训练领域发展最快的方向之一。无论你是想构建一个高精度的特定场景助手、微调模型的语气风格，还是提高特定任务的准确性，本课程都将为你提供当下大语言模型后训练中最重要的技术实践经验。\n\n请在此处注册：https://t.co/efSt2FnVNS"
  },
  {
    "id": "1940864335083196819",
    "url": "https://x.com/AndrewYNg/status/1940864335083196819",
    "text": "I’d like to share a tip for getting more practice building with AI — that is, either using AI building blocks to build applications or using AI coding assistance to create powerful applications quickly: If you find yourself with only limited time to build, reduce the scope of your project until you can build something in whatever time you do have.\n\nIf you have only an hour, find a small component of an idea that you're excited about that you can build in an hour. With modern coding assistants like Anthropic’s Claude Code (my favorite dev tool right now), you might be surprised at how much you can do even in short periods of time! This gets you going, and you can always continue the project later.\n\nTo become good at building with AI, most people must (i) learn relevant techniques, for example by taking online AI courses, and (ii) practice building. I know developers who noodle on ideas for months without actually building anything — I’ve done this too! — because we feel we don’t have time to get started. If you find yourself in this position, I encourage you to keep cutting the initial project scope until you identify a small component you can build right away.\n\nLet me illustrate with an example — one of my many small, fun weekend projects that might never go anywhere, but that I’m glad I did.\n\nHere’s the idea: Many people fear public speaking. And public speaking is challenging to practice, because it's hard to organize an audience. So I thought it would be interesting to build an audience simulator to provide a digital audience of dozens to hundreds of virtual people on a computer monitor and let a user practice by speaking to them.\n\nOne Saturday afternoon, I found myself in a coffee shop with a couple of hours to spare and decided to give the audience simulator a shot. My familiarity with graphics coding is limited, so instead of building a complex simulator of a large audience and writing AI software to simulate appropriate audience responses, I decided to cut scope significantly to (a) simulating an audience of one person (which I could replicate later to simulate N persons), (b) omitting AI and letting a human operator manually select the reaction of the simulated audience (similar to Wizard of Oz prototyping), and (c) implementing the graphics using a simple 2D avatar.\n\nUsing a mix of several coding assistants, I built a basic version in the time I had. The avatar could move subtly and blink, but otherwise it used basic graphics. Even though it fell far short of a sophisticated audience simulator, I am glad I built this. In addition to moving the project forward and letting me explore different designs, it advanced my knowledge of basic graphics. Further, having this crude prototype to show friends helped me get user feedback that shaped my views on the product idea.\n\nI have on my laptop a list of ideas of things that I think would be interesting to build. Most of them would take much longer than the handful of hours I might have to try something on a given day, but by cutting their scope, I can get going, and the initial progress on a project helps me decide if it’s worth further investment. As a bonus, hacking on a wide variety of applications helps me practice a wide range of skills. But most importantly, this gets an idea out of my head and potentially in front of prospective users for feedback that lets the project move faster.\n\n[Original text: https://t.co/UP6arTWAdV ]",
    "createdAt": "Thu Jul 03 20:04:51 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 484,
    "replyCount": 76,
    "likeCount": 1650,
    "quoteCount": 32,
    "viewCount": 321412,
    "bookmarkCount": 1316,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我希望分享一个关于如何通过 AI （人工智能）获得更多实践构建的技巧——也就是说，无论是利用 AI 构建模块来开发应用程序，还是借助 AI 编码辅助来快速创建强大的应用：如果你发现自己用于开发的时间有限，那就请缩小项目范围，直到你能在所拥有的任何时间内完成一些构建。\n\n如果你只有一个小时，那么就找一个你感兴趣的想法中，能在这一小时内完成的小组件。有了 Anthropic 的 Claude Code （我目前最喜欢的开发工具）这类现代编码助手，你可能会惊讶于即使在短时间内，你也能完成多少工作！这能让你尽快上手，而且你随时都可以选择稍后继续这个项目。\n\n要擅长利用 AI 进行开发，大多数人必须 (i) 学习相关技术，例如通过参加在线 AI 课程，以及 (ii) 实践构建。我认识一些开发者，他们会沉浸在想法中好几个月，但却没有任何实际的产出——我也曾这样做过！——因为我们总觉得没有时间开始。如果你发现自己处于这种境地，我鼓励你持续缩减初始项目范围，直到找到可以立即着手构建的小模块。\n\n让我用一个例子来说明——这是我众多小型、有趣的周末项目之一，它们可能最终不了了之，但我仍然很高兴我尝试了。\n\n这个想法是这样的：许多人害怕公开演讲。然而，公开演讲又很难练习，因为组织听众是一件麻烦事。所以我想，如果能构建一个观众模拟器，在电脑显示器上提供数十到数百个虚拟人的数字听众，并让用户通过向他们讲话来练习，那会很有趣。\n\n一个周六下午，我发现自己在一个咖啡馆里，有几个小时的空闲时间，于是决定尝试一下这个观众模拟器。我对图形编程的了解不多，所以与其构建一个复杂的大型观众模拟器并编写 AI 软件来模拟适当的观众反应，我决定大幅缩减项目范围，具体做法是： (a) 只模拟一个人的观众 (我以后可以复制它来模拟 N 个人)， (b) 暂时不引入 AI ，而是让人工操作员手动选择模拟观众的反应 (类似于绿野仙踪原型法 (Wizard of Oz prototyping))，以及 (c) 使用简单的 2D 虚拟形象 (2D avatar) 来实现图形。\n\n通过结合使用几个编码助手，我在有限的时间内构建了一个基本版本。这个虚拟形象可以进行细微的动作并眨眼，但除此之外，它只使用了基本的图形效果。尽管它远不是一个复杂的观众模拟器，但我很高兴我构建了它。除了推动项目向前发展并让我探索不同的设计，它还加深了我对基础图形的理解。此外，能够向朋友展示这个粗糙的原型，这帮助我获得了用户反馈，从而塑造了我对产品想法的看法。\n\n我的笔记本电脑上有一个我认为有趣的开发想法清单。它们中的大多数都需要比我某天可能拥有的几个小时多得多的时间，但通过缩减其范围，我可以开始行动，并且项目的初步进展有助于我决定它是否值得进一步投资。作为额外的好处，尝试开发各种各样的应用程序有助于我练习广泛的技能。但最重要的是，这能让一个想法从我的脑海中释放出来，并可能呈现在潜在用户面前以获取反馈，从而让项目更快地推进。\n\n[Original text: https://t.co/UP6arTWAdV ]"
  },
  {
    "id": "1938265468986659075",
    "url": "https://x.com/AndrewYNg/status/1938265468986659075",
    "text": "On Monday, a United States District Court ruled that training LLMs on copyrighted books constitutes fair use. A number of authors had filed suit against Anthropic for training its models on their books without permission. Just as we allow people to read books and learn from them to become better writers, but not to regurgitate copyrighted text verbatim, the judge concluded that it is fair use for AI models to do so as well.\n\nIndeed, Judge Alsup wrote that the authors’ lawsuit is “no different than it would be if they complained that training schoolchildren to write well would result in an explosion of competing works.” While it remains to be seen whether the decision will be appealed, this ruling is reasonable and will be good for AI progress. (Usual caveat: I am not a lawyer and am not giving legal advice.)\n\nAI has massive momentum, but a few things could put progress at risk:\n- Regulatory capture that stifles innovation, including especially open source\n- Loss of access to cutting-edge semiconductor chips (the most likely cause would be war breaking out in Taiwan)\n- Regulations that severely impede access to data for training AI systems\n\nAccess to high-quality data is important. Even though the mass media tends to talk about the importance of building large data centers and scaling up models, when I speak with friends at companies that train foundation models, many describe a very large amount of their daily challenges as data preparation. Specifically, a significant fraction of their day-to-day work follows the usual Data Centric AI practices of identifying high-quality data (books are one important source), cleaning data (the ruling describes Anthropic taking steps like removing book pages' headers, footers, and page numbers), carrying out error analyses to figure out what types of data to acquire more of, and inventing new ways to generate synthetic data.\n\nI am glad that a major risk to data access just decreased. Appropriately, the ruling further said that Anthropic’s conversion of books from paper format to digital — a step that’s needed to enable training — also was fair use. However, in a loss for Anthropic, the judge indicated that, while training on data that was acquired legitimately is fine, using pirated materials (such as  texts downloaded from pirate websites) is not fair use. Thus, Anthropic still may be liable on this point. Other LLM providers, too, will now likely have to revisit their practices if they use datasets that may contain pirated works.\n\nOverall, the ruling is positive for AI progress. Perhaps the biggest benefit is that it reduces ambiguity with respect to AI training and copyright and (if it stands up to appeals) makes the roadmap for compliance clearer. This decision indicates it is okay to train on legitimately acquired data to build models that generate transformational outputs, and to convert printed books to digital format for this purpose. However, downloading from pirate sites (as well as permanently building a “general purpose” library of texts, stored indefinitely for purposes to be determined, without permission from the relevant copyright holders) are not considered fair use.\n\nI am very sympathetic with the many writers who are worried about their livelihoods being affected by AI. I don‘t know the right solution for that. Society is better off with free access to more data; but if a subset of people is significantly negatively affected, I hope we can figure out an arrangement that compensates them fairly.\n\n[Original text: https://t.co/kxcCgL4tpH ]",
    "createdAt": "Thu Jun 26 15:57:53 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 374,
    "replyCount": 121,
    "likeCount": 1221,
    "quoteCount": 54,
    "viewCount": 158103,
    "bookmarkCount": 341,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "本周一，美国地方法院裁定，使用受版权保护的书籍来训练大语言模型（LLMs）属于合理使用。此前，一些作者曾起诉 Anthropic 公司，理由是其未经允许就用他们的作品训练 AI 模型。法官的结论是，正如我们允许人们通过阅读书籍来学习并成为更好的作者，但不允许他们逐字逐句地抄袭受版权保护的文本一样，AI 模型在训练过程中这样做也应被视为合理使用。\n\n事实上，Alsup 法官写道，这些作者的诉讼“与他们抱怨训练学童写好文章会导致大量竞争作品出现的情况并无二致。” 尽管这项裁决是否会上诉尚待观察，但这一判决是合理且有利于 AI 发展的。（常规免责声明：我并非律师，不提供法律建议。）\n\n当前 AI 发展势头迅猛，但有几项因素可能会阻碍其进步：\n- 监管俘获 (Regulatory capture) 抑制创新，尤其对开源项目影响显著\n- 失去获取尖端半导体芯片的途径（最可能的原因是台湾地区爆发冲突）\n- 严苛的法规，严重限制 AI 系统训练数据的获取\n\n获取高质量数据至关重要。尽管大众媒体更倾向于关注构建大型数据中心和扩展模型规模，但当我与那些训练基础模型公司的朋友交流时，许多人提到他们日常工作中的一大挑战就是数据准备。具体来说，他们大部分的日常工作都遵循着常见的数据中心 AI (Data Centric AI) 实践：识别高质量数据（书籍是重要的来源之一），清理数据（判决书中提到 Anthropic 采取了删除书页的页眉、页脚和页码等措施），进行错误分析以明确需要获取更多哪种类型的数据，以及探索生成合成数据的新方法。\n\n令我欣慰的是，一项关于数据获取的主要风险刚刚有所缓解。此外，判决书恰当地指出，Anthropic 将书籍从纸质版转换为数字版——这是训练 AI 所需的一个步骤——也属于合理使用。然而，对 Anthropic 而言，不利的一点是，法官指出，虽然使用合法获取的数据进行训练没有问题，但使用盗版材料（比如从盗版网站下载的文本）不构成合理使用。因此，Anthropic 在这一点上仍可能面临法律责任。其他大语言模型提供商，如果他们使用的数据集可能包含盗版作品，现在也需要重新审视自己的做法。\n\n总而言之，这项裁决对 AI 的进步是积极的。也许它最大的好处在于减少了 AI 训练和版权之间的不确定性，并且（如果上诉后维持原判）能让合规的路径变得更加清晰。这项裁决表明，使用合法获取的数据来构建能产生变革性输出的模型是允许的，并且为此目的将印刷书籍转换为数字格式也是可以的。然而，从盗版网站下载（以及未经相关版权所有者许可，永久建立一个“通用”文本库，无限期存储以待日后使用）则不被视为合理使用。\n\n我非常理解许多作家对于生计可能受 AI 影响的担忧。我不知道该如何完美解决这个问题。社会确实需要更自由地获取更多数据，这样会更好；但如果有一部分人的利益受到了显著损害，我希望我们能找到一种公平补偿他们的方案。\n\n[原文链接： https://t.co/kxcCgL4tpH ]"
  },
  {
    "id": "1937907934094360582",
    "url": "https://x.com/AndrewYNg/status/1937907934094360582",
    "text": "New Course: ACP: Agent Communication Protocol\n\nLearn to build agents that communicate and collaborate across different frameworks using ACP in this short course built with @IBMResearch's BeeAI, and taught by @sandi_besen, AI Research Engineer & Ecosystem Lead at IBM, and @nicholasrenotte, Head of AI Developer Advocacy at IBM.\n\nBuilding a multi-agent system with agents built or used by different teams and organizations can become challenging. You may need to write custom integrations each time a team updates their agent design or changes their choice of agentic orchestration framework.\n\nThe Agent Communication Protocol (ACP) is an open protocol that addresses this challenge by standardizing how agents communicate, using a unified RESTful interface that works across frameworks. In this protocol, you host an agent inside an ACP server, which handles requests from an ACP client and passes them to the appropriate agent. Using a standardized client-server interface allows multiple teams to reuse agents across projects. It also makes it easier to switch between frameworks, replace an agent with a new version, or update a multi-agent system without refactoring the entire system.\n\nIn this course, you’ll learn to connect agents through ACP. You’ll understand the lifecycle of an ACP Agent and how it compares to other protocols, such as MCP (Model Context Protocol) and A2A (Agent-to-Agent). You’ll build ACP-compliant agents and implement both sequential and hierarchical workflows of multiple agents collaborating using ACP.\n\nThrough hands-on exercises, you’ll build:\n- A RAG agent with CrewAI and wrap it inside an ACP server.\n- An ACP Client to make calls to the ACP server you created.\n- A sequential workflow that chains an ACP server, created with Smolagents, to the RAG agent.\n- A hierarchical workflow using a router agent that transforms user queries into tasks, delegated to agents available through ACP servers.\n- An agent that uses MCP to access tools and ACP to communicate with other agents.\n\nYou’ll finish up by importing your ACP agents into the BeeAI platform, an open-source registry for discovering and sharing agents.\n\nACP enables collaboration between agents across teams and organizations. By the end of this course, you’ll be able to build ACP agents and workflows that communicate and collaborate regardless of framework.\n\nPlease sign up here: https://t.co/csyHrswJuB",
    "createdAt": "Wed Jun 25 16:17:10 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 362,
    "replyCount": 52,
    "likeCount": 1547,
    "quoteCount": 17,
    "viewCount": 102818,
    "bookmarkCount": 949,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "<新课程: ACP: AI 智能体通信协议>\n\n这门短期课程将教您如何使用 AI 智能体通信协议 (Agent Communication Protocol，ACP) 来构建 AI 智能体 (AI Agent)，让它们能够在不同的框架之间顺畅通信与协作。课程由 @IBMResearch 的 BeeAI 平台支持，并由 IBM 的 AI 研究工程师兼生态系统负责人 @sandi_besen 以及 IBM 的 AI 开发者宣传负责人 @nicholasrenotte 共同讲授。\n\n当我们尝试构建一个多 AI 智能体系统时，如果其中的 AI 智能体是由不同团队和组织开发或使用的，那么这个过程可能会变得非常具有挑战性。每当一个团队更新他们的 AI 智能体设计，或者更改他们选择的 AI 智能体编排框架时，您可能都需要重新编写定制化的集成方案。\n\nAI 智能体通信协议 (ACP) 就是为了解决这一挑战而设计的开放协议。它通过标准化 AI 智能体之间的通信方式，提供了一个统一的 RESTful 接口，能够跨越各种框架。在这个协议中，您需要将 AI 智能体托管在一个 ACP 服务器内部，由该服务器负责处理来自 ACP 客户端的请求，并将其转发给对应的 AI 智能体。采用标准化的客户端-服务器接口，不仅能让多个团队在不同的项目中重用 AI 智能体，还能更轻松地实现框架切换、AI 智能体版本更新，或者在不重构整个系统的前提下升级多 AI 智能体系统。\n\n在本课程中，您将学习如何通过 ACP 连接各种 AI 智能体。您将深入了解 ACP AI 智能体的生命周期，并将其与其他协议（如模型上下文协议 (Model Context Protocol，MCP) 和 AI 智能体间通信协议 (Agent-to-Agent，A2A)）进行比较。您将亲手构建符合 ACP 规范的 AI 智能体，并实现使用 ACP 协作的多个 AI 智能体的顺序工作流和分层工作流。\n\n通过一系列动手练习，您将构建:\n- 一个基于 CrewAI 的 RAG AI 智能体，并将其封装在 ACP 服务器中。\n- 一个 ACP 客户端，用于调用您创建的 ACP 服务器。\n- 一个顺序工作流，将使用 Smolagents 创建的 ACP 服务器与 RAG AI 智能体连接起来。\n- 一个分层工作流，其中路由 AI 智能体负责将用户查询转换为具体任务，并将这些任务委派给通过 ACP 服务器可用的 AI 智能体。\n- 一个既使用 MCP 访问工具，又使用 ACP 与其他 AI 智能体通信的 AI 智能体。\n\n完成课程后，您会将您的 ACP AI 智能体导入 BeeAI 平台——这是一个用于发现和共享 AI 智能体的开源注册表。\n\nACP 有效地促进了 AI 智能体在不同团队和组织间的协作。完成本课程的学习后，您将能够构建出 ACP AI 智能体和工作流，无论底层框架如何，它们都能实现无缝通信与协作。\n\n请点击此处注册: https://t.co/csyHrswJuB"
  },
  {
    "id": "1935741989204770837",
    "url": "https://x.com/AndrewYNg/status/1935741989204770837",
    "text": "One of the most effective things the U.S. or any other nation can do to ensure its competitiveness in AI is to welcome high-skilled immigration and international students who have the potential to become high-skilled. For centuries, the U.S. has welcomed immigrants, and this helped make it a worldwide leader in technology. Letting immigrants and native-born Americans collaborate makes everyone better off. Reversing this stance would have a huge negative impact on U.S. technology development.\n\nI was born in the UK and came to the U.S. on an F-1 student visa as a relatively unskilled and clueless teenager to attend college. Fortunately I gained skills and became less clueless over time. After completing my graduate studies, I started working at Stanford under the OPT (Optional Practical Training) program, and later an H-1B visa, and ended up staying here. Many other immigrants have followed similar paths to contribute to the U.S.\n\nI am very concerned that making visas harder to obtain for students and high-skilled workers, such as the pause in new visa interviews that started last month and a newly chaotic process of visa cancellations, will hurt our ability to attract great students and workers. In addition, many international students without substantial means count on being able to work under OPT to pay off the high cost of a U.S. college degree. Gutting the OPT program, as has been proposed, would both hurt many international students’ ability to study here and deprive U.S. businesses of great talent. (This won’t stop students from wealthy families. But the U.S. should try to attract the best talent without regard to wealth.)\n\nFailure to attract promising students and high-skilled workers would have a huge negative impact on American competitiveness in AI. Indeed, a recent report by the National Security Commission on Artificial Intelligence exhorts the government to “strengthen AI talent through immigration.”\n\nIf talented people do not come to the U.S., will they have an equal impact on global AI development just working somewhere else? Unfortunately, the net impact will be negative. The U.S. has a number of tech hubs including Silicon Valley, Seattle, New York, Boston/Cambridge, Los Angeles, Pittsburgh and Austin, and these hubs concentrate talent and foster innovation. (This is why cities, where people can more easily find each other and collaborate, promote innovation.) Making it harder for AI talent to find each other and collaborate will slow down innovation, and it will take time for new hubs to become as advanced.\n\nNonetheless, other nations are working hard to attract immigrants who can drive innovation — a good move for them! Many have thoughtful programs to attract AI and other talent. There are the UK’s Global Talent Visa, France’s French Tech Visa, Australia’s Global Talent Visa, the UAE’s Golden Visa, Taiwan’s Employment Gold Card, China’s Thousand Talents Plan, and many more. The U.S. is fortunate that many people already want to come here to study and work. Squandering that advantage would be a huge unforced error.\n\nBeyond the matter of national competitiveness, there is the even more important ethical matter of making sure people are treated decently. I have spoken with international students who are terrified that their visas may be canceled arbitrarily. One recently agonized about whether to attend an international conference to present a research paper, because they were worried about being unable to return. In the end, with great sadness, they cancelled their trip. I also spoke with a highly skilled technologist who is in the U.S. on an H-1B visa. Their company shut down, leading them — after over a decade in this country, and with few ties to their nation of origin — scrambling to find alternative employment that would enable them to stay. \n\nThese stories, and many far worse, are heartbreaking. While I do what I can to help individuals I know personally, it is tragic that we are creating such an uncertain environment for immigrants, that many people who have extraordinary skills and talents will no longer want to come here.\n\nTo every immigrant or migrant in the U.S. who is concerned about the current national environment: I see you and empathize with your worries. As an immigrant myself, I will be fighting to protect everyone’s dignity and right to due process, and to encourage legal immigration, which makes both the U.S. and individuals much better off.\n\n[Full text, with links: https://t.co/6JNJz88Qyq ]",
    "createdAt": "Thu Jun 19 16:50:29 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 396,
    "replyCount": 307,
    "likeCount": 2167,
    "quoteCount": 69,
    "viewCount": 520932,
    "bookmarkCount": 316,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "美国或任何其他国家为确保其在 AI 领域（AI field）的竞争力，最有效的举措之一就是欢迎高技能移民和有潜力成为高技能人才的国际学生。几个世纪以来，美国一直张开双臂欢迎移民，这正是其成为全球技术领导者的原因之一。让移民和本土出生的美国人携手合作，能让所有人都受益。反转这一立场，将对美国的技术发展造成巨大的负面影响。\n\n我出生在英国，十几岁时以一个相对缺乏经验、懵懂无知的少年身份，持 F-1 学生签证来到美国上大学。幸运的是，随着时间的推移，我逐渐习得技能，变得不再那么懵懂。完成研究生学业后，我通过 OPT (Optional Practical Training) 项目开始在斯坦福大学工作，后来又获得了 H-1B 签证，并最终留在了这里。许多其他移民也循着类似的路径，为美国做出了贡献。\n\n我非常担忧，让学生和高技能人才更难获得签证，例如上个月开始暂停新的签证面试，以及新近出现的混乱的签证取消流程，都将损害我们吸引优秀学生和人才的能力。此外，许多经济条件并不宽裕的国际学生，都指望能通过 OPT 工作来支付在美国攻读大学学位的高昂费用。正如有人提议的那样，如果取消 OPT 项目，不仅会损害许多国际学生在这里学习的可能，也会让美国企业失去大批优秀人才。（当然，这不会阻止来自富裕家庭的学生。但美国应该努力吸引最优秀的人才，而不论其财富状况。）\n\n未能吸引有前途的学生和高技能人才，将对美国在 AI 领域（AI field）的竞争力产生巨大的负面影响。事实上，国家人工智能安全委员会（National Security Commission on Artificial Intelligence）最近的一份报告明确呼吁政府“通过移民加强 AI 人才”。\n\n如果人才不来美国，他们只是在其他地方工作，这对全球 AI 发展会产生同样的影响吗？不幸的是，净效应将是负面的。美国拥有众多科技中心，包括硅谷、西雅图、纽约、波士顿/剑桥、洛杉矶、匹兹堡和奥斯汀，这些中心汇聚人才，促进创新。（这就是为什么城市能够促进创新，因为人们在这里更容易相互联系和协作。）如果让 AI 人才更难相互找到并协作，将减缓创新步伐，而新的中心要达到同样的先进水平，则需要时间。\n\n尽管如此，其他国家正在努力吸引能够推动创新的移民——这对它们来说是明智之举！许多国家都制定了深思熟虑的计划，以吸引 AI 及其他领域的人才。例如，英国的全球人才签证、法国的法国科技签证、澳大利亚的全球人才签证、阿联酋的黄金签证、台湾的就业金卡、中国的千人计划等等。美国是幸运的，因为许多人已经渴望来这里学习和工作。浪费这一优势将是一个巨大的自我伤害。\n\n除了国家竞争力问题，还有一个更重要的道德问题，那就是确保人们受到公正对待。我曾与一些国际学生交谈，他们非常担心自己的签证可能被任意取消。有一位学生最近就纠结是否要参加一个国际会议并发表研究论文，因为他们担心可能无法返回。最终，他们带着巨大的遗憾取消了行程。我还与一位持 H-1B 签证在美国的高技能技术专家交谈。他们的公司倒闭了，这导致他们——在这个国家生活了十多年，与原籍国几乎没有联系——不得不手忙脚乱地寻找替代工作，以便能够留下来。\n\n这些故事，以及许多远比这糟糕的经历，都令人心碎。虽然我尽力帮助我认识的个人，但我们为移民创造了这样一个充满不确定性的环境，导致许多拥有非凡技能和才华的人不再愿意来这里，这实在令人悲哀。\n\n致所有身在美国、担忧当前国家环境的移民们：我理解并同情你们的忧虑。作为一名移民，我将为保护每个人的尊严和正当程序（due process）权利而奋斗，并鼓励合法移民，因为这能让美国和个人都受益良多。\n\n[全文，带链接： https://t.co/6JNJz88Qyq ]"
  },
  {
    "id": "1935350552692658202",
    "url": "https://x.com/AndrewYNg/status/1935350552692658202",
    "text": "Introducing \"Building with Llama 4.\" This short course is created with @Meta @AIatMeta, and taught by  @asangani7, Director of Partner Engineering for Meta’s AI team.\n\nMeta’s new Llama 4 has added three new models and introduced the Mixture-of-Experts (MoE) architecture to its family of open-weight models, making them more efficient to serve. \n\nIn this course, you’ll work with two of the three new models introduced in Llama 4. First is Maverick, a 400B  parameter model, with 128 experts and 17B active parameters. Second is Scout, a 109B parameter model with 16 experts and 17B active parameters. Maverick and Scout support long context windows of up to a million tokens and 10M tokens, respectively. The latter is enough to support directly inputting even fairly large GitHub repos for analysis!\n\nIn hands-on lessons, you’ll build apps using Llama 4’s new multimodal capabilities including reasoning across multiple images and image grounding, in which you can identify elements in images. You’ll also use the official Llama API, work with Llama 4’s long-context abilities, and learn about Llama’s newest open-source tools: its prompt optimization tool that automatically improves system prompts and synthetic data kit that generates high-quality datasets for fine-tuning.\n\nIf you need an open model, Llama is a great option, and the Llama 4 family is an important part of any GenAI developer's toolkit. Through this course, you’ll learn to call Llama 4 via API, use its optimization tools, and build features that span text, images, and large context.\n\nPlease sign up here: https://t.co/oRFRi9vQNg",
    "createdAt": "Wed Jun 18 14:55:03 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 250,
    "replyCount": 34,
    "likeCount": 886,
    "quoteCount": 3,
    "viewCount": 66483,
    "bookmarkCount": 464,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "隆重推出“使用 Llama 4 进行开发”课程。这门短期课程由 @Meta @AIatMeta 合作打造，并由 Meta AI 团队的合作伙伴工程总监 @asangani7 亲自授课。\n\nMeta 全新的 Llama 4 新增了三款模型，并为其开源权重模型家族引入了专家混合 (Mixture-of-Experts, MoE) 架构，这大大提升了模型的服务效率。\n\n在本课程中，你将亲身体验 Llama 4 中引入的三款新模型中的两款。首先是 Maverick，这是一个拥有 4000 亿 (400B) 参数的模型，包含 128 个专家和 170 亿 (17B) 活跃参数。其次是 Scout，一个 1090 亿 (109B) 参数模型，包含 16 个专家和 170 亿 (17B) 活跃参数。Maverick 和 Scout 分别支持高达一百万个 Token 和一千万个 Token 的长上下文窗口。后者甚至足以直接输入相当大的 GitHub 代码库进行分析！\n\n在实践课程中，你将利用 Llama 4 的全新多模态功能构建应用程序，这些功能包括跨多个图像进行推理，以及图像内容理解 (image grounding)，通过它你可以识别图像中的具体元素。你还将学习如何使用官方 Llama API，利用 Llama 4 的长上下文处理能力，并了解 Llama 最新的开源工具：能够自动优化系统提示词的提示优化工具 (prompt optimization tool)，以及可以生成高质量微调数据集的合成数据工具包 (synthetic data kit)。\n\n如果你正在寻找一款开放模型，Llama 无疑是绝佳选择，而 Llama 4 系列更是任何生成式 AI (Generative AI) 开发人员工具包中的重要组成部分。通过本课程，你将学会如何通过 API 调用 Llama 4，使用其优化工具，并构建能够跨越文本、图像和长上下文的功能。\n\n请点击此链接注册：https://t.co/oRFRi9vQNg"
  },
  {
    "id": "1933185193059516442",
    "url": "https://x.com/AndrewYNg/status/1933185193059516442",
    "text": "There’s a new breed of GenAI Application Engineers who can build more-powerful applications faster than was possible before, thanks to generative AI. Individuals who can play this role are highly sought-after by businesses, but the job description is still coming into focus. Let me describe their key skills, as well as the sorts of interview questions I use to identify them.\n\nSkilled GenAI Application Engineers meet two primary criteria: (i) They are able to use the new AI building blocks to quickly build powerful applications. (ii) They are able to use AI assistance to carry out rapid engineering, building software systems in dramatically less time than was possible before. In addition, good product/design instincts are a significant bonus.\n\nAI building blocks. If you own a lot of copies of only a single type of Lego brick, you might be able to build some basic structures. But if you own many types of bricks, you can combine them rapidly to form complex, functional structures. Software frameworks, SDKs, and other such tools are like that. If all you know is how to call a large language model (LLM) API, that's a great start. But if you have a broad range of building block types — such as prompting techniques, agentic frameworks, evals, guardrails, RAG, voice stack, async programming, data extraction, embeddings/vectorDBs, model fine tuning, graphDB usage with LLMs, agentic browser/computer use, MCP, reasoning models, and so on — then you can create much richer combinations of building blocks.\n\nThe number of powerful AI building blocks continues to grow rapidly. But as open-source contributors and businesses make more building blocks available, staying on top of what is available helps you keep on expanding what you can build. Even though new building blocks are created, many building blocks from 1 to 2 years ago (such as eval techniques or frameworks for using vectorDBs) are still very relevant today.\n\nAI-assisted coding. AI-assisted coding tools enable developers to be far more productive, and such tools are advancing rapidly. Github Copilot, first announced in 2021 (and made widely available in 2022), pioneered modern code autocompletion. But shortly after, a new breed of AI-enabled IDEs such as Cursor and Windsurf offered much better code-QA and code generation. As LLMs improved, these AI-assisted coding tools that were built on them improved as well.\n\nNow we have highly agentic coding assistants such as OpenAI’s Codex and Anthropic’s Claude Code (which I really enjoy using and find impressive in its ability to write code, test, and debug autonomously for many iterations). In the hands of skilled engineers — who don’t just “vibe code” but deeply understand AI and software architecture fundamentals and can steer a system toward a thoughtfully selected product goal — these tools make it possible to build software with unmatched speed and efficiency.\n\nI find that AI-assisted coding techniques become obsolete much faster than AI building blocks, and techniques from 1 or 2 years ago are far from today's best practices. Part of the reason for this might be that, while AI builders might use dozens (hundreds?) of different building blocks, they aren’t likely to use dozens of different coding assistance tools at once, and so the forces of Darwinian competition are stronger among tools. Given the massive investments in this space by  Anthropic, Google, OpenAI, and other players, I expect the frenetic pace of development to continue, but keeping up with the latest developments in AI-assisted coding tools will pay off, since each generation is much better than the last.\n\nBonus: Product skills. In some companies, engineers are expected to take pixel-perfect drawings of a product, specified in great detail, and write code to implement it. But if a product manager has to specify even the smallest detail, this slows down the team. The shortage of AI product managers exacerbates this problem. I see teams move much faster if GenAI Engineers also have some user empathy as well at basic skill at designing products, so that, given only high-level guidance on what to build (“a user interface that lets users see their profiles and change their passwords”), they can make a lot of decisions themselves and build at least a prototype to iterate from.\n\nWhen interviewing GenAI Application Engineers, I will usually ask about their mastery of AI building blocks and ability to use AI-assisted coding, and sometimes also their product/design instincts. One additional question I've found highly predictive of their skill is, “How do you keep up with the latest developments in AI?” Because AI is evolving so rapidly, someone with good strategies for keeping up — such as reading The Batch and taking short courses 😃, regular hands-on practice building projects, and having a community to talk to — really does stay ahead of the game.\n\n[Original post: https://t.co/I3alxNs0vn ]",
    "createdAt": "Thu Jun 12 15:30:41 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 813,
    "replyCount": 138,
    "likeCount": 4431,
    "quoteCount": 84,
    "viewCount": 530992,
    "bookmarkCount": 5754,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "得益于生成式 AI (generative AI)，一类新型的生成式 AI 应用程序工程师 (GenAI Application Engineers) 正在崛起，他们能够以比以往更快的速度构建出更强大的应用程序。具备这种能力的人才备受企业青睐，尽管目前对该职位的描述仍在不断完善中。接下来，我将介绍他们的关键技能，以及我用来识别这类人才的面试问题类型。\n\n一名出色的生成式 AI 应用程序工程师需满足两大核心标准： (i) 他们能够运用新型 AI 构建模块 (AI building blocks) 迅速开发出强大的应用程序。 (ii) 他们能够借助 AI 辅助 (AI assistance) 进行快速工程 (rapid engineering)，以远超以往的速度构建软件系统。此外，良好的产品/设计直觉 (product/design instincts) 也是一个重要的加分项。\n\nAI 构建模块。如果你只有一种类型的乐高积木 (Lego brick)，你或许能搭出一些基本结构。但如果你拥有多种类型的积木，就能迅速将它们组合起来，形成复杂且功能齐全的结构。软件框架、SDK 及其他类似工具亦是如此。如果你只知道如何调用大语言模型 (LLM) API，这已经是个不错的开端。但如果你掌握了广泛的构建模块类型——例如提示技术 (prompting techniques)、智能体框架 (agentic frameworks)、评估 (evals)、护栏 (guardrails)、RAG、语音堆栈 (voice stack)、异步编程 (async programming)、数据提取 (data extraction)、嵌入/向量数据库 (embeddings/vectorDBs)、模型微调 (model fine tuning)、大语言模型与图数据库 (graphDB) 的结合使用、智能体驱动的浏览器/计算机操作 (agentic browser/computer use)、MCP、推理模型 (reasoning models) 等等——那么你就能创造出更丰富多样的组合。\n\n强大的 AI 构建模块数量正持续快速增长。随着开源贡献者和企业不断推出更多构建模块，及时了解这些新工具将帮助你不断拓展可构建的范围。尽管新模块层出不穷，但许多一两年前的构建模块（例如评估技术或使用向量数据库的框架）在今天依然非常重要。\n\nAI 辅助编程 (AI-assisted coding)。AI 辅助编程工具极大地提升了开发人员的生产力，并且这些工具本身也在飞速发展。Github Copilot 于 2021 年首次公布（并于 2022 年广泛发布），率先开启了现代代码自动补全的新纪元。但不久之后，Cursor 和 Windsurf 等新型 AI 赋能的集成开发环境 (IDEs) 便提供了更出色的代码质量保证 (code-QA) 和代码生成功能。随着大语言模型的进步，这些基于大语言模型构建的 AI 辅助编程工具也随之升级。\n\n如今，我们已拥有具备高度智能体能力的编程助手，例如 OpenAI 的 Codex 和 Anthropic 的 Claude Code (我非常喜欢使用，并对其在多次迭代中自主编写、测试和调试代码的能力印象深刻）。在技艺精湛的工程师手中——那些不只“凭感觉写代码”，而是深入理解 AI 和软件架构基础，并能将系统导向深思熟虑的产品目标的工程师——这些工具能以无与伦比的速度和效率构建软件。\n\n我发现 AI 辅助编程技术的过时速度远超 AI 构建模块，一两年前的技术与当今的最佳实践已相去甚远。部分原因可能在于，AI 开发者可能会使用几十种（甚至几百种？）不同的构建模块，但他们不太可能同时使用几十种不同的编程辅助工具，因此这些工具之间的优胜劣汰竞争更为激烈。鉴于 Anthropic、Google、OpenAI 及其他参与者在该领域的巨大投入，我预计开发步伐将继续保持狂热，但持续关注 AI 辅助编程工具的最新进展将带来丰厚回报，因为每一代产品都比上一代更为出色。\n\n额外加分：产品技能。在某些公司，工程师被期望根据高度详细、像素级精确的产品设计图来编写代码实现。但如果产品经理连最小的细节都必须指定，这会大大减慢团队的速度。AI 产品经理的短缺使得这个问题雪上加霜。我发现，如果生成式 AI 工程师同时具备一定的用户同理心和基本的产品设计技能，那么在仅获得高层次指导 (例如：“一个让用户查看个人资料和更改密码的用户界面”) 的情况下，他们就能自行做出许多决策，并至少构建一个可供迭代的原型，这样团队的进展会快得多。\n\n在面试生成式 AI 应用程序工程师时，我通常会询问他们对 AI 构建模块的掌握程度以及使用 AI 辅助编程的能力，有时也会考察他们的产品/设计直觉。我发现，一个额外的问题对预测他们的技能非常有帮助，那就是：“您如何跟上 AI 的最新发展？” 因为 AI 发展如此迅速，那些有良好策略来跟进的人才——例如阅读 The Batch 和参加短期课程 😃、定期动手实践项目，并拥有一个可以交流的社区——确实能保持领先。\n\n[原文链接： https://t.co/I3alxNs0vn ]"
  },
  {
    "id": "1932822251273093247",
    "url": "https://x.com/AndrewYNg/status/1932822251273093247",
    "text": "Learn to build and deploy GenAI pipelines in \"Orchestrating Workflows for GenAI Applications\", built in partnership with @astronomerio and taught by Kenten Danas, the company's DevRel Senior Manager, and Tamara Fingerlin, developer advocate. \n\nMany GenAI applications require executing a pipeline comprising many steps. For example, a RAG app for recommending books might ingest and embed book descriptions, store the embeddings in a vector database, and later use the database to retrieve and recommend specific books based on a user query. After having prototyped this -- maybe in a Jupyter notebook -- how do you turn this into a reliable, repeatable workflow to run in production? \n\nIn this short course, you’ll learn to build reliable GenAI pipelines and orchestrate them using the popular open-source tool Airflow 3.0. You’ll learn to break down a workflow into discrete tasks so that an orchestration framework can schedule tasks to run in the right order at the right time (using time-based or data-aware triggers), and execute tasks in parallel when possible. It can also use retries to recover gracefully from failure (such as transient API rate limits) and provide observability (using Airflow UI) to help you track the status of the pipeline. You'll do this by using Airflow dags, which helps sequence tasks that need to run in a specific order, with clear task dependencies. \n\nBy the end of this course, you’ll know how to turn your prototype Jupyter notebook or Python script into production-ready workflow. \n\nPlease sign up here:  https://t.co/Ei8JZeS0ey",
    "createdAt": "Wed Jun 11 15:28:29 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 174,
    "replyCount": 41,
    "likeCount": 843,
    "quoteCount": 6,
    "viewCount": 72585,
    "bookmarkCount": 393,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "在“编排生成式 AI (Generative AI) 应用的工作流”课程中，你将学习如何构建和部署 GenAI 管道。这门课程是与 @astronomerio 合作开发的，由该公司 DevRel 高级经理 Kenten Danas 和开发者倡导者 Tamara Fingerlin 共同授课。\n\n许多 GenAI 应用都需要执行一个包含多个步骤的管道。举例来说，一个用于图书推荐的检索增强生成 (Retrieval Augmented Generation, RAG) 应用，可能需要获取并嵌入（embed）图书描述，将这些嵌入向量存储在向量数据库中，然后根据用户的查询，利用该数据库检索并推荐特定的图书。在你完成原型设计——也许是在 Jupyter notebook 中——之后，如何将其转化为一个可靠、可重复、能在生产环境中稳定运行的工作流呢？\n\n在这个短期课程中，你将学习如何构建可靠的 GenAI 管道，并使用流行的开源工具 Airflow 3.0 来编排它们。你将学习如何将一个工作流分解成独立的任务，以便编排框架能让任务在正确的时间以正确的顺序运行 (通过基于时间或数据感知的触发器)，并在可能的情况下并行执行任务。它还能通过重试机制来妥善处理故障 (例如临时的 API 速率限制)，并提供可观测性 (通过 Airflow UI) 来帮助你追踪管道的运行状态。你将通过使用 Airflow dags (有向无环图) 来实现这些，它有助于按特定顺序、以明确的任务依赖关系来组织需要运行的任务。\n\n学完本课程后，你将知道如何把你的 Jupyter notebook 原型或 Python 脚本转化为生产级别的就绪工作流。\n\n请在此处报名： https://t.co/Ei8JZeS0ey"
  },
  {
    "id": "1931072122853691639",
    "url": "https://x.com/AndrewYNg/status/1931072122853691639",
    "text": "Hanging out with @juberti , OpenAI’s head of realtime AI, responsible for the company’s voice AI products. One thing both of us agree on: while some things in AI are overhyped, voice applications seem underhyped right now. The application opportunities seem larger than the amount of developer or business attention on this right now.",
    "createdAt": "Fri Jun 06 19:34:06 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 148,
    "replyCount": 62,
    "likeCount": 722,
    "quoteCount": 9,
    "viewCount": 77924,
    "bookmarkCount": 94,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "与 OpenAI 实时 AI (realtime AI) 负责人 @juberti 交流时，他负责公司旗下的语音 AI (voice AI) 产品。我们都一致认为：尽管人工智能 (AI) 领域有些技术被过度宣传，但语音应用目前似乎被低估了。它所蕴含的应用机会，远超当前开发者或商业领域的关注度。"
  },
  {
    "id": "1930320263780151402",
    "url": "https://x.com/AndrewYNg/status/1930320263780151402",
    "text": "Thank you for your pioneering research work @lateinteraction on DSPy (together with @matei_zaharia, @ChrisGPotts and many others). I'm glad we could do a short course on this!",
    "createdAt": "Wed Jun 04 17:46:29 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 77,
    "replyCount": 13,
    "likeCount": 299,
    "quoteCount": 1,
    "viewCount": 61537,
    "bookmarkCount": 50,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isQuote": true,
    "isPinned": false,
    "tranlastedContent": "感谢 @lateinteraction 在 DSPy 方面的开创性研究工作 (这项工作也离不开 @matei_zaharia、@ChrisGPotts 以及其他许多人的贡献)。我很高兴我们能就此开设一门短期课程!"
  },
  {
    "id": "1930277912030392356",
    "url": "https://x.com/AndrewYNg/status/1930277912030392356",
    "text": "New short course: DSPy: Build and Optimize Agentic Apps\n\nDSPy is a powerful open-source framework for automatically tuning prompts for GenAI applications. In this course, you'll learn to use DSPy, together with MLflow. This is built in partnership with @databricks and taught by @ChenMoneyQ, co-lead of the DSPy framework.\n\nMany AI builders spend hours hand-tuning prompts. When given a set of evals, DSPy automates this process. It’s especially useful for optimizing prompts, including few-shot prompts,  in complex agentic AI workflows. Further, if you switch an application to a newer LLM, performance can degrade if your prompts were optimized to the previous model. DSPy automatically optimizes the entire system for the new LLM as well, using just a few evaluation examples.\n\nThis course teaches DSPy works, and best practices for using it. You’ll write programs using DSPy’s signature-based programming model, debug them with MLflow tracing -- to gain visibility into how different parts of a pipeline, as well as how the overall system, are performing -- and automatically improve their accuracy with DSPy Optimizer.\n\nPlease sign up here: https://t.co/bb8uILyepf",
    "createdAt": "Wed Jun 04 14:58:11 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 313,
    "replyCount": 40,
    "likeCount": 1523,
    "quoteCount": 27,
    "viewCount": 177674,
    "bookmarkCount": 1319,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新短期课程：DSPy：构建和优化智能体应用 (Agentic Apps)\n\nDSPy 是一个功能强大的开源框架，旨在自动调整生成式 AI (GenAI) 应用的提示词 (prompts)。在本课程中，你将学习如何结合 MLflow 使用 DSPy。该课程由 @databricks 合作开发，并由 DSPy 框架的共同负责人 @ChenMoneyQ 授课。\n\n许多 AI 开发者花费大量时间手动调整提示词。有了 DSPy，在提供一组评估 (evals) 数据后，它能自动完成这一过程。这对于优化复杂智能体 AI 工作流中的提示词（包括少样本 (few-shot) 提示词）尤其有用。此外，如果你的应用程序切换到更新的大语言模型 (LLM)，而提示词是为旧模型优化的，性能可能会随之下降。DSPy 能够利用少量评估示例，自动为新的大语言模型优化整个系统。\n\n本课程将讲解 DSPy 的工作原理及其最佳实践。你将使用 DSPy 基于签名的编程模型来编写程序，并借助 MLflow 追踪 (tracing) 进行调试——这将帮助你清晰了解管道 (pipeline) 的各个部分以及整个系统的运行表现——最后通过 DSPy Optimizer 自动提高程序的准确性。\n\n请在此处注册：https://t.co/bb8uILyepf"
  },
  {
    "id": "1929906213208113409",
    "url": "https://x.com/AndrewYNg/status/1929906213208113409",
    "text": "Everyone should learn to code with AI! At AI Fund, everyone - not just engineers - can vibe code or use AI assistance to code. This has been great for our creativity and productivity. I hope more teams will  empower everyone to build with AI. Please watch the  video for details. https://t.co/rsGC1QSKHL",
    "createdAt": "Tue Jun 03 14:21:11 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 427,
    "replyCount": 79,
    "likeCount": 2278,
    "quoteCount": 24,
    "viewCount": 159465,
    "bookmarkCount": 1339,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "每个人都应该学习如何用 AI (人工智能) 编程！在 AI Fund，每个人——不仅仅是工程师——都能亲身体验编程或使用 AI 辅助编程。这对我们的创造力和生产力非常有益。我希望更多团队能够让每个人都利用 AI 进行创造。请观看视频了解详情。https://t.co/rsGC1QSKHL"
  },
  {
    "id": "1928099650269237359",
    "url": "https://x.com/AndrewYNg/status/1928099650269237359",
    "text": "I am alarmed by the proposed cuts to U.S. funding for basic research, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.\n\nIf not for funding for my early work in deep learning from the National Science Foundation (NSF)  and Defense Advanced Research Projects Agency (DARPA), which disburse a good deal of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.\n\nIn fact, such funding benefits the U.S. more than any other nation.  Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.\n\nWhy does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.\n\nIn a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.\n\nThus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technology points out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”\n\nFurther, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Some studies (link in original post, below) also show how knowledge diffuses locally much faster than globally.\n\nChina was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:\n- There is ample funding for open academic research in China.\n- China’s businesses such as DeepSeek and Alibaba have released cutting-edge, open-weights models. This openness at the corporate level accelerates diffusion of knowledge.\n- China’s labor laws make non-compete agreements (which stop an employee from jumping ship to a competitor) relatively hard to enforce, and the work culture supports significant idea sharing among employees of different companies; this has made circulation of ideas relatively efficient.\n\nWhile there’s also much about China that I would not want the U.S. to emulate, the openness of its tech ecosystem has helped it accelerate.\n\nIn 1945, Vannevar Bush’s landmark report “Science, The Endless Frontier” laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S.\n\nThe good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.\n\n[Original post, with links: https://t.co/JR3x4O1iVr ]",
    "createdAt": "Thu May 29 14:42:33 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 473,
    "replyCount": 106,
    "likeCount": 2655,
    "quoteCount": 48,
    "viewCount": 323701,
    "bookmarkCount": 471,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "美国拟议削减基础研究资金的计划，以及这可能对美国在人工智能 (AI) 及其他领域的竞争力造成的影响，让我深感忧虑。尽管对开放共享研究的资助惠及全球，但受益最大的无疑是开展这项研究的国家。\n\n如果当初没有国家科学基金会 (NSF) 和国防高级研究计划局 (DARPA) 对我早期深度学习工作的资助（这两个机构负责分配美国大部分研究经费），我就不会在扩展方面有所发现，这些发现促使我提议成立 Google Brain 来大规模发展深度学习。我担心，一旦基础科学的资金遭到削减，美国乃至全世界都将错失未来的重要新思潮。\n\n事实上，这种资助模式让美国比其他任何国家都受益匪多。科学研究能为所在国带来最大益处，原因有二：(1) 新知识在该国传播得最快； (2) 研究过程本身就能为该国培养出新的专业人才。\n\n为什么生成式 AI (Generative AI) 的大部分创新仍然发生在硅谷呢？因为该地区有两个团队——发明了 Transformer 网络的 Google Brain 和将其规模化的 OpenAI——完成了大量早期工作。随后，这些团队成员有的跳槽到附近的商业公司，有的创办了竞争对手，还有的与当地大学合作。此外，当地的社交网络也通过随意的咖啡聚会、本地会议，甚至儿童玩伴聚会（同龄孩子的父母在这些场合见面并交流技术想法），迅速传播了知识。通过这种方式，知识在硅谷内部的传播速度远超其他地区。\n\n同理，在美国开展的研究，其知识在美国国内的扩散速度也远快于向其他地理区域的传播。当研究成果通过论文和/或开源形式公开分享时，这种现象尤为显著：如果研究人员获准讨论某个想法，他们就能更快地分享更多信息，例如如何真正让某个算法奏效的诀窍和技巧。这也能让其他人更快地找到能解答他们疑问的专家。学术环境中产生的知识传播尤其迅速。学术界通常是完全开放的，学生和教授与许多公司的员工不同，他们拥有充分的自由来谈论自己的研究工作。\n\n因此，资助美国的基础研究不仅对美国最有利，也对我们的盟友大有裨益。当然，开放性也会惠及我们的对手。但正如美国众议院科学、空间和技术委员会的一个小组委员会所指出的那样：“...公开共享基础研究并非没有风险。但……研究的开放性对于竞争力和安全至关重要，以至于它所带来的风险，即对手也可能从科学开放中受益，是值得承担的。”\n\n此外，生成式 AI 的发展日新月异，因此保持领先地位至关重要。例如，尽管现在许多团队都能训练出达到 GPT-3.5 甚至 GPT-4 级别能力的模型，但这似乎并未对 OpenAI 造成太大冲击。OpenAI 正在忙于通过开发尖端的 o4、Codex、GPT-4.1 等来拓展其业务。技术发明者能够率先将其商业化，而在一个快速发展的世界中，尖端技术无疑是最具价值的。一些研究 （详见下文原始帖子链接）也表明，知识在本地传播的速度远快于全球传播。\n\n2022 年 ChatGPT 首次推出时，中国在生成式 AI 领域明显落后于美国。然而，中国的技术生态系统内部非常开放，这帮助它在过去两年中迎头赶上：\n- 中国为开放的学术研究提供了充足的资金支持。\n- 像 DeepSeek 和 Alibaba 这样的中国企业已经发布了尖端的开源模型 (open-weights models)。这种企业层面的开放性极大地加速了知识的传播。\n- 中国的劳动法使得竞业禁止协议 （阻止员工跳槽到竞争对手公司）相对难以执行，并且工作文化支持不同公司员工之间进行大量思想交流；这使得思想的流通效率相对较高。\n\n虽然中国在许多方面我并不希望美国效仿，但其技术生态系统的开放性确实帮助它实现了加速发展。\n\n1945 年，Vannevar Bush 那份里程碑式的报告《科学，无尽的前沿》为美国研究和人才培养的公共资助奠定了关键原则。这些原则使得美国得以在科学进步方面主导数十年。美国联邦对科学的资助创造了无数突破，不仅极大地造福了美国，也惠及了全世界，同时培养了几代本土科学家以及同样贡献卓著的移民人才。\n\n令人欣慰的是，这套成功策略如今已是众所周知。我希望有更多国家能够效仿，大力投资科学和人才。我也希望，作为这一成功模式的开创者，美国不会通过大幅削减科学研究资金而背离它。\n\n[原文链接： https://t.co/JR3x4O1iVr ]"
  },
  {
    "id": "1927384264779170259",
    "url": "https://x.com/AndrewYNg/status/1927384264779170259",
    "text": "Agentic Document Extraction just got much faster! From previous 135sec median processing time down to 8sec. Extracts not just text but diagrams, charts, and form fields from PDFs to give LLM-ready output. Please see the video for details and some application ideas. https://t.co/29lOKf6UGO",
    "createdAt": "Tue May 27 15:19:52 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 605,
    "replyCount": 97,
    "likeCount": 3820,
    "quoteCount": 25,
    "viewCount": 288036,
    "bookmarkCount": 4081,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "基于 AI 智能体 (AI Agent) 的文档提取技术速度变得更快了！其处理时间已从之前的平均 135 秒大幅缩短至 8 秒。这项技术不仅能从 PDF 文档中提取文本，还能识别并提取其中的示意图、图表和表单字段，从而提供可以直接用于大语言模型 (LLM) 的数据输出。有关更多细节和应用场景的启发，请观看此视频：https://t.co/29lOKf6UGO"
  },
  {
    "id": "1925575163325948123",
    "url": "https://x.com/AndrewYNg/status/1925575163325948123",
    "text": "In the age of AI, large corporations — not just startups — can move fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain.\n\nLarge companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?\n\nThanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable.\n\nFortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission.\n\nThe sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute.\n\nWithin this sandbox, there can be broad scope for experimentation, and — importantly — a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company’s brand, and so on.\n\nUnder this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs. This also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones.\n\nI often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I’m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters.\n\n[Original text: https://t.co/Jn1QLnrRlI ]",
    "createdAt": "Thu May 22 15:31:09 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 215,
    "replyCount": 68,
    "likeCount": 1118,
    "quoteCount": 18,
    "viewCount": 136115,
    "bookmarkCount": 749,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "在人工智能 (AI) 时代，大公司——不只是初创公司——也能快速行动。我经常与大公司的 C 级高管 (C-suite) 和董事会探讨 AI 战略与实施，在此想分享一些适用于大型企业的想法。其中一个关键，就是创造一个环境，让那些小而精干的团队无需层层审批就能大胆创新。接下来，我将详细阐述这一点。\n\n大公司之所以比初创公司慢，原因有很多。但为什么即使是大公司内部仅有三人的精干团队，其速度也比同等规模的初创公司慢呢？一个主要原因在于，大公司有更多顾虑，它们不能承受一个小型团队在开发和发布某个功能时，不慎泄露敏感信息、损害公司品牌、影响收入、招致监管审查，或以其他方式对业务重要部分造成损害。为了规避这些风险，我曾见过一些公司要求团队在发布任何内容之前，必须经过隐私审查、营销审查、财务审查、法律审查等等。然而，如果工程师们需要获得五位副总裁的签字批准，才能启动一个最小可行产品 (MVP) 来进行实验，他们又如何能够快速发现客户需求、迅速迭代，或开发出任何有意义的新产品呢？\n\n得益于 AI 辅助编码技术，现在的世界拥有了快速构建软件原型的能力。然而，许多大公司为防范合法下行风险 (downside risk) 而设计的流程，却让它们无法充分利用这项能力。相比之下，那些没有收入、没有客户、也没有品牌声誉的小型初创公司，其下行风险是有限的。事实上，破产倒闭本身就是一种非常现实的可能性，因此快速行动比为了规避下行风险而缓慢行动，具有更高的价值权衡。在最糟糕的情况下，它或许只是以一种新的方式走向失败，但如果成功，它将可能创造巨大的价值。\n\n幸运的是，大公司有办法摆脱这个困境。它们可以为团队创建一个沙盒环境，让团队以一种严格限制下行风险的方式进行实验。这样一来，这些团队就可以大大加快速度，而无需为了获得任何人的许可而拖延。\n\n这个沙盒环境可以是一套书面政策，不一定非要通过软件来实现。例如，它可能只允许团队在公司员工以及可能已签署保密协议 (NDA) 的早期测试者 (alpha testers) 身上测试初生产品，并且不允许访问敏感信息。它可能只被允许在与公司没有直接关联的全新品牌下发布产品实验。或许它必须在预先分配的计算资源预算内运行。\n\n在这个沙盒内部，团队可以拥有广阔的实验空间，更重要的是，他们可以自由地进行实验，而无需频繁地请求许可，因为他们可能造成的下行风险是有限的。此外，当一个原型展现出足够的潜力值得推广时，公司就可以投入资源，确保软件的可靠性、安全性、对敏感信息的恰当处理、符合公司品牌形象等。\n\n在这种框架下，更容易建立一种鼓励学习、构建和实验的企业文化，并对那些现在以适度成本出现的不可避免的失败加以肯定。为了找到一两个“全垒打”（指获得巨大成功）般的创意，可以构建数十甚至数百个原型，并迅速淘汰，这都是探索过程中必须付出的代价。这也使得团队在快速迭代这些为了找到有价值创意所需的数十个原型时，能够迅速行动。\n\n我经常与大公司探讨 AI 战略与实施。我的快速核对清单通常会考虑三个方面：人才（people）、流程（process）和平台（platform）。本文只探讨了流程的一部分，重点是快速行动。我对初创公司和大公司利用 AI 所能做的事情充满信心，未来我将在其他文章中讨论人才和平台的作用。\n\n[原文链接: https://t.co/Jn1QLnrRlI ]"
  },
  {
    "id": "1925213790892929149",
    "url": "https://x.com/AndrewYNg/status/1925213790892929149",
    "text": "New Course: Reinforcement Fine-Tuning LLMs with GRPO! \n\nLearn to use reinforcement learning to improve your LLM performance in this short course, built in collaboration with @Predibase, and taught by @TravisAddair, its Co-Founder and CTO, and @grg_arnav, its Senior Engineer and Machine Learning Lead.\n\nReasoning models have been one of the most important developments in LLMs. Reinforcement Fine-Tuning (RFT) uses rewards to encourage LLMs to find solutions to multi-step reasoning tasks such as solving math problems and debugging code - without needing pre-existing training examples like in traditional supervised fine-tuning.\n\nGroup Relative Policy Optimization (GRPO) is a   reinforcement fine-tuning algorithm gaining rapid adoption. Developed by the DeepSeek team and used to train the R1 reasoning model, GRPO uses reward functions that you can write in Python to assign rewards to model responses. It’s beneficial for tasks with verifiable outcomes and can work well even with fewer than 100 training examples. It can also significantly improve the reasoning ability of smaller LLMs, making applications faster and more cost effective.\n\nIn this course, you’ll take a technical deep dive into RFT with GRPO. You’ll learn to build reward functions that you can use in the GRPO training process to guide an LLM toward better performance on multi-step reasoning tasks.\n\nIn detail, you’ll:\n- Learn when reinforcement fine-tuning is a better fit than supervised fine-tuning, especially for tasks involving multi-step reasoning or limited labeled data.\n- Understand how GRPO uses programmable reward functions as a more scalable alternative to the human feedback required for other reinforcement learning algorithms, such as RLHF and DPO.\n- Frame the Wordle game as a reinforcement fine-tuning problem and see how an LLM can learn to plan, analyze feedback, and improve its strategy over time.\n- Design reward functions that power the reinforcement fine-tuning process.\n- Learn techniques for evaluating more subjective tasks, such as rating the quality of a text summary, using an LLM as a judge.\n- Understand why reward hacking happens and how to avoid it by adding penalty functions to discourage undesirable behaviors.\n- Learn the four key components of the loss calculation in the GRPO algorithm: token probability distribution ratios, advantages, clipping, and KL-divergence.\n- Launch reinforcement fine-tuning jobs using Predibase’s hosted training services.\n\nBy the end of this course, you’ll be able to build and fine-tune LLMs using reinforcement learning to improve reasoning without relying on large labeled datasets or subjective human feedback.\n\nPlease sign up here: https://t.co/2BSuKuzE6N",
    "createdAt": "Wed May 21 15:35:11 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 184,
    "replyCount": 29,
    "likeCount": 1264,
    "quoteCount": 17,
    "viewCount": 84598,
    "bookmarkCount": 873,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程：用 GRPO 强化微调 (Reinforcement Fine-Tuning) 大语言模型 (LLMs)！\n\n这门短期课程将教你如何使用强化学习来提升你的大语言模型性能。它由 @Predibase 合作开发，并由其联合创始人兼首席技术官 @TravisAddair 和高级工程师兼机器学习负责人 @grg_arnav 亲自授课。\n\n推理模型一直是大语言模型发展中最重要的突破之一。强化微调 (RFT) 通过奖励机制，鼓励大语言模型自主探索多步推理任务的解决方案，例如解决数学问题和调试代码。与传统的监督微调不同，它不需要预先存在的训练示例。\n\n群组相对策略优化 (GRPO) 是一种正迅速普及的强化微调算法。它由 DeepSeek 团队开发，并成功应用于训练 R1 推理模型。GRPO 允许你用 Python 编写奖励函数，为模型的响应分配奖励。这种方法特别适用于结果可验证的任务，并且即使训练示例少于 100 个也能表现出色。此外，它还能显著提升小型大语言模型的推理能力，从而让应用运行更快、成本效益更高。\n\n在这门课程中，你将对 GRPO 强化微调进行一次深入的技术探索。你将学习如何构建奖励函数，并在 GRPO 训练过程中使用它们，从而引导大语言模型在多步推理任务上取得更好的表现。\n\n具体来说，你将：\n- 了解何时强化微调比监督微调更适用，尤其是在涉及多步推理或有限标注数据的任务中。\n- 理解 GRPO 如何利用可编程的奖励函数，作为替代其他强化学习算法（例如 RLHF 和 DPO）所需的人类反馈的一种更具扩展性的方案。\n- 将 Wordle 游戏转化为一个强化微调问题，并观察大语言模型如何学习规划、分析反馈并逐步优化其策略。\n- 设计能够驱动强化微调过程的奖励函数。\n- 学习评估更主观任务的技术，例如使用大语言模型作为裁判，对文本摘要的质量进行评分。\n- 理解为什么会发生奖励欺骗 (reward hacking)，以及如何通过增加惩罚函数来遏制不良行为以避免它。\n- 学习 GRPO 算法中损失计算的四个关键组成部分：Token 概率分布比率、优势、裁剪和 KL 散度。\n- 使用 Predibase 的托管训练服务启动强化微调任务。\n\n完成本课程后，你将能够使用强化学习来构建和微调大语言模型，从而在不依赖大量标注数据集或主观人类反馈的情况下，提升模型的推理能力。\n\n请在此处注册：https://t.co/2BSuKuzE6N"
  },
  {
    "id": "1923045958511886549",
    "url": "https://x.com/AndrewYNg/status/1923045958511886549",
    "text": "AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value.\n\nFor the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabled speed rather than AI-enabled cost reduction.\n\nThat AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value.\n\nI see this pattern across more and more businesses. Consider the following scenarios:\n- If a lender can approve loans in minutes using AI, rather than days waiting for a human to review them, this creates more borrowing opportunities (and also lets the lender deploy its capital faster). Even if human-in-the-loop review is needed, using AI to get the most important information to the reviewer might speed things up. The ability to provide loans quickly opens up the market to new customers in need of rapid funds and helps customers who need a quick positive or negative decision to accept the loan or move on.\n- If an academic institution gives homework feedback to students in minutes (via sophisticated autograding) rather than days (via human grading), not only is the automation cheaper, the rapid feedback facilitates better learning.\n- If an online seller can approve purchases faster, this can lead to more sales. For example, many platforms that accept online ad purchases have an approval process that can take hours or days; if approvals can be done faster, they can earn revenue faster. Further, for customers buying ads, being able to post an ad in minutes lets them test ideas faster and also makes the ad product more valuable.\n- If a company’s sales department can prioritize leads and respond to prospective customers in minutes or hours rather than days — closer to when the customers’ buying intent first led them to contact the company — sales representatives might close more deals. Likewise, a business that can respond more quickly to requests for proposals may win more deals.\n\nI’ve written previously about looking at the tasks a company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth.\n\nGrowth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth.\n\n[Original text: https://t.co/qx2Ir6pkSp  ]",
    "createdAt": "Thu May 15 16:00:59 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 192,
    "replyCount": 79,
    "likeCount": 891,
    "quoteCount": 34,
    "viewCount": 124357,
    "bookmarkCount": 406,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "AI 不仅能让任务更便宜，还能让任务变得更快，但它在创造商业价值方面的重要性却被低估了。\n\n以编写代码这项任务为例，AI 无疑是颠覆性的技术。借助 AI 的帮助，编写软件所需的工作量和成本都大大降低了。然而，除了降低软件开发成本，AI 还在缩短从创意到工作原型 (working prototype) 的时间。更快地验证想法的能力正在改变团队探索和创新的方式。如果每月能测试 20 个想法，与每月只能测试 1 个想法相比，您的工作效率将发生翻天覆地的变化。这就是由 AI 赋能的速度带来的好处，而非仅仅是 AI 带来的成本降低。\n\nAI 赋能的自动化能够降低成本这一点已是共识。例如，提供自动化客户服务比运营人工客服中心要便宜。许多企业更乐意投资于增长，而非仅仅是削减成本；当一项任务变得更便宜时，一些企业会大幅增加这项任务的执行次数，从而实现增长。但是，另一种实现增长的方式却被低估了：让某些任务变得更快（无论它们是否也变得更便宜）能够创造巨大的新价值。\n\n我发现这种模式正日益渗透到越来越多的企业中。请看以下几种场景：\n- 如果贷款机构能利用 AI 在几分钟内批准贷款，而无需等待人工审核数天，这不仅能创造更多借贷机会，还能让贷款机构更快地利用其资本。即使仍需人工介入审核，AI 也能帮助审核员迅速获取最关键的信息，从而加快流程。快速提供贷款的能力不仅能为需要即时资金的新客户打开市场，还能帮助客户迅速获得贷款批复（无论是通过还是拒绝），以便做出下一步决策。\n- 如果学术机构能在几分钟内（通过复杂的自动评分系统）而不是几天内（通过人工评分）向学生提供作业反馈，那么自动化不仅更经济，快速的反馈还能促进更好的学习效果。\n- 如果在线商家能更快地批准购买请求，这可能带来更多销售。例如，许多接受在线广告购买的平台都有一个可能耗时数小时甚至数天的审批流程；如果审批能更快完成，它们就能更快地获得收入。此外，对于购买广告的客户来说，能在几分钟内发布广告，可以让他们更快地测试创意，也使得广告产品本身更具价值。\n- 如果一家公司的销售部门能在几分钟或几小时内，而非数天内，优先处理销售线索并回应潜在客户——这更接近客户最初因购买意愿而联系公司的时间——那么销售代表可能会达成更多交易。同样，一家能更快回应项目投标邀请的企业，也可能赢得更多业务。\n\n我之前曾撰文探讨如何审视公司各项任务，以发现 AI 的潜在应用。许多团队在进行这项工作时，通常侧重于如何降低任务成本，无论是为了节省开支还是为了能更频繁地执行这些任务。在进行这项分析时，请务必考虑 AI 是否也能显著加快某些任务的执行速度。一个值得关注的领域是营收路径上的任务序列。如果其中某些环节能被加速，或许就能有效推动营收增长。\n\n对大多数企业而言，增长比成本节约更具吸引力。如果您的业务中存在一些通过加速就能驱动增长的循环，那么 AI 或许就是解锁这种增长的关键工具。\n\n[Original text: https://t.co/qx2Ir6pkSp ]"
  },
  {
    "id": "1922671569429766178",
    "url": "https://x.com/AndrewYNg/status/1922671569429766178",
    "text": "New course: MCP: Build Rich-Context AI Apps with Anthropic. Learn to build AI apps that access tools, data, and prompts using the Model Context Protocol in this short course, created in partnership with Anthropic @AnthropicAI and taught by Elie Schoppik @eschoppik, its Head of Technical Education.\n\nConnecting AI applications to external systems that bring rich context to LLM-based applications has often meant writing custom integrations for each use case. MCP is an open protocol that standardizes how LLMs access tools, data, and prompts from external sources, and simplifies how you provide context to your LLM-based applications. For example, you can provide context via third-party tools that let your LLM make API calls to search the web, access data from local docs, retrieve code from a GitHub repo, and so on.\n\nMCP, developed by Anthropic, is based on a client-server architecture that defines the communication details between an MCP client, hosted inside the AI application, and an MCP server that exposes tools, resources, and prompt templates. The server can be a subprocess launched by the client that runs locally or an independent process running remotely.\n\nIn this hands-on course, you'll learn the core architecture behind MCP. You’ll create an MCP-compatible chatbot, build and deploy an MCP server, and connect the chatbot to your MCP server and other open-source servers.\n\nHere’s what you’ll do:\n- Understand why MCP makes AI development less fragmented and standardizes connections between AI applications and external data sources\n- Learn the core components of the client-server architecture of MCP and the underlying communication mechanism\n- Build a chatbot with custom tools for searching academic papers, and transform it into an MCP-compatible application\n- Build a local MCP server that exposes tools, resources, and prompt templates using FastMCP, and test it using MCP Inspector\n- Create an MCP client inside your chatbot to dynamically connect to your server\n- Connect your chatbot to reference servers built by Anthropic’s MCP team, such as filesystem, which implements filesystem operations, and fetch, which extracts contents from the web as markdown\n- Configure Claude Desktop to connect to your server and others, and explore how it abstracts away the low-level logic of MCP clients\n- Deploy your MCP server remotely and test it with the Inspector or other MCP-compatible applications\n- Learn about the roadmap for future MCP development, such as multi-agent architecture, MCP registry API, server discovery, authorization, and authentication\n\nMCP is an exciting and important technology that lets you build rich-context AI applications that connect to a growing ecosystem of MCP servers, with minimal integration work.\n\nPlease sign up here! https://t.co/UDyp8NRe8R",
    "createdAt": "Wed May 14 15:13:18 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 376,
    "replyCount": 44,
    "likeCount": 2085,
    "quoteCount": 33,
    "viewCount": 139740,
    "bookmarkCount": 1834,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程：MCP：Anthropic 携手助您构建富上下文 AI 应用。在这个短课程中，您将学习如何使用模型上下文协议 ( Model Context Protocol ) 构建可访问工具、数据和提示的 AI 应用。该课程由 Anthropic (@AnthropicAI) 合作创建，并由其技术教育主管 Elie Schoppik (@eschoppik) 授课。\n\n将 AI 应用程序连接到外部系统，为基于大语言模型 ( Large Language Model ) 的应用带来丰富上下文，过去通常意味着为每个用例编写自定义集成。MCP 是一种开放协议，它规范了 大语言模型 如何从外部源访问工具、数据和提示，并简化了您为基于 大语言模型 的应用提供上下文的方式。例如，您可以通过第三方工具提供上下文，这些工具能让您的大语言模型进行 API 调用，从而搜索网页、访问本地文档中的数据、从 GitHub 仓库检索代码等。\n\nMCP 由 Anthropic 开发，基于客户端-服务器 ( client-server ) 架构，它定义了托管在 AI 应用程序内部的 MCP 客户端与暴露工具、资源和提示模板的 MCP 服务器之间的通信细节。服务器可以是由客户端在本地启动的子进程，也可以是远程运行的独立进程。\n\n在这个实践课程中，您将学习 MCP 背后的核心架构。您将创建一个 MCP 兼容的聊天机器人，构建并部署一个 MCP 服务器，并将聊天机器人连接到您的 MCP 服务器和其他开源服务器。\n\n您将学习到：\n- 了解 MCP 如何减少 AI 开发的零散性，并标准化 AI 应用程序与外部数据源之间的连接\n- 学习 MCP 客户端-服务器架构的核心组件以及底层的通信机制\n- 使用自定义工具构建一个用于搜索学术论文的聊天机器人，并将其转换为一个 MCP 兼容的应用程序\n- 使用 FastMCP 构建一个暴露工具、资源和提示模板的本地 MCP 服务器，并使用 MCP Inspector 对其进行测试\n- 在您的聊天机器人内部创建一个 MCP 客户端，以动态连接到您的服务器\n- 将您的聊天机器人连接到 Anthropic 的 MCP 团队构建的参考服务器，例如 `filesystem` ( 实现文件系统操作 ) 和 `fetch` ( 将网页内容提取为 Markdown 格式 )\n- 配置 Claude Desktop 以连接到您的服务器和其他服务器，并探索它如何抽象或隐藏 MCP 客户端的底层逻辑\n- 远程部署您的 MCP 服务器，并使用 Inspector 或其他 MCP 兼容的应用程序对其进行测试\n- 了解未来 MCP 开发的路线图，例如多 AI 智能体 ( multi-agent ) 架构、MCP 注册表 API、服务器发现、授权和身份验证\n\nMCP 是一项令人兴奋且重要的技术，它能让您以最少的集成工作，构建连接到不断增长的 MCP 服务器生态系统的富上下文 AI 应用程序。\n\n请在此处注册！https://t.co/UDyp8NRe8R"
  },
  {
    "id": "1920480706439876889",
    "url": "https://x.com/AndrewYNg/status/1920480706439876889",
    "text": "Additional tips on achieving speed:\n- Concreteness: https://t.co/H21N1tzkHM\n- Domain expert’s gut: https://t.co/6zGF1Fv8Ym\n- AI assisted coding: https://t.co/g41HynaFZ0\n- Quick user feedback: https://t.co/7rA0dit4jx",
    "createdAt": "Thu May 08 14:07:35 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 19,
    "replyCount": 7,
    "likeCount": 115,
    "quoteCount": 1,
    "viewCount": 31969,
    "bookmarkCount": 90,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "在提升速度方面，还有一些额外的建议：\n- 保持具体：https://t.co/H21N1tzkHM\n- 依靠领域专家的直觉：https://t.co/6zGF1Fv8Ym\n- 利用 AI 辅助编程：https://t.co/g41HynaFZ0\n- 快速获取用户反馈：https://t.co/7rA0dit4jx"
  },
  {
    "id": "1920480460318130460",
    "url": "https://x.com/AndrewYNg/status/1920480460318130460",
    "text": "I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity.\n\nAI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): We co-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices.\n\nMany factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my posts here as well.\n\nIf you are building an AI startup, here are some ideas to consider:\n- A startup with a small team that pursues one focused, concrete idea can move really fast. Rather than hedging, it is often better to pursue one hypothesis (for example, build one concrete product) but also be willing to switch quickly to a different hypothesis (say, change what features you decide to build) if the data that comes back indicates the original hypothesis is flawed. Concreteness gets you speed!\n- A subject matter expert’s gut is remarkably good at making quick decisions. Obviously, there’s a role for data and user studies as well. But if you’re deciding whether to build feature A or B, or to sell first to user persona X or Y, sometimes a domain expert’s gut will point to a quick decision that you can execute and validate or falsify. Trusting a domain expert’s gut gets you speed!\n- AI-assisted coding is making prototyping faster than ever before. Yes, AI assistance is speeding up building reliable, enterprise-grade applications and maintaining legacy codebases. But the acceleration it brings to building stand-alone prototypes is far greater. This is because stand-alone prototypes have low requirements for reliability, integration, or even security (if, say, you run them in a sandbox environment). This lets us prototype and test at a ferocious velocity. AI-assisted coding (including vibe coding, where you might barely look at the code) gets you speed!\n- Finally, with faster prototyping, the bottleneck shifts to getting feedback from users. A single learning cycle might consist of (i) building a prototype and (ii) getting user feedback to inform the next iteration. Since (i) is now much faster than before, accelerating (ii) is growing in importance. This means teams that are skilled at finding prospective customers and getting their feedback in hours/days rather than weeks can go faster. For example, when building consumer products, I routinely approach strangers (in a respectful way) in public places to ask if they’re willing to give feedback on a prototype I’m working on. (Gathering feedback is more complex for enterprise products, because prospective customers are harder to track down.) Quick user feedback gets you speed!\n\nIn addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed!\n\nI’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!\n\n[Original text: https://t.co/I1nkYeTkFA ]",
    "createdAt": "Thu May 08 14:06:37 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 137,
    "replyCount": 58,
    "likeCount": 1198,
    "quoteCount": 27,
    "viewCount": 120142,
    "bookmarkCount": 481,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我非常高兴地宣布，AI Fund 旗下新基金已成功募得 1.9 亿美元，本轮融资获得了超额认购。我期待能与更多有志之士携手合作，共同创建造福人类的新公司。\n\nAI Fund 并非一家投资现有业务的传统风险投资机构。相反，我们是“风险建设者” (Venture Builder)，也可以称之为“创业工作室” (Venture Studio)：我们与创业团队共同孵化 AI 公司，这意味着我们的团队直接参与编写代码、与客户沟通以收集反馈、迭代产品设计、进行市场分析等等。我们很享受同时开发多个 AI 产品的过程，因此每天都在亲身实践着新兴 AI 初创公司的最佳方法。\n\n一家初创公司的成功离不开诸多因素。但如果非要我从中挑选一个，那一定是“速度”。初创公司的生死存亡，取决于它们能否快速做出正确决策并高效执行，这也是我在此前文章中反复强调的主题。\n\n如果你正在创办一家 AI 初创公司，这里有一些值得思考的建议：\n- 一支小团队的初创公司，如果能专注于一个明确、具体的想法，就能真正实现快速发展。与其犹豫不决，不如坚定地追求一个核心假设 (例如，开发一款具体的产品)，但同时也要乐于根据数据反馈，在发现原始假设有缺陷时，迅速转向新的假设 (比如，调整你决定开发的功能)。具体性就是速度的保证！\n- 领域专家 (subject matter expert) 的直觉在快速决策方面有着惊人的准确性。当然，数据和用户研究也同样重要。但当你需要在构建功能 A 或功能 B 之间做选择，或是决定是优先面向用户画像 X 还是 Y 销售时，领域专家的直觉有时能迅速帮你做出一个可以立即执行、验证或证伪的决策。相信领域专家的直觉，你就能更快！\n- AI 辅助编码 (AI-assisted coding) 让原型开发 (prototyping) 的速度达到了前所未有的水平。没错，AI 辅助正在加速开发可靠的企业级应用程序和维护遗留代码库。但它对构建独立原型 (stand-alone prototypes) 所带来的加速效果则要显著得多。这是因为独立原型对可靠性、集成度乃至安全性 (例如，如果你在一个沙盒环境 (sandbox environment) 中运行它们) 的要求都较低。这使得我们能够以惊人的速度进行原型设计和测试。AI 辅助编码 (包括“直觉编码” (vibe coding)，即你可能几乎不看代码，仅凭感觉编写) 能让你获得速度！\n- 最后，随着原型开发速度的加快，瓶颈转移到了从用户那里获取反馈。一个完整的学习周期可能包括 (i) 构建原型和 (ii) 获取用户反馈以指导下一次迭代。由于 (i) 现在比以前快得多，因此加速 (ii) 的重要性日益凸显。这意味着那些善于在数小时/数天而非数周内找到潜在客户并获取反馈的团队，能够更快地迭代和发展。例如，在开发消费产品时，我经常会以尊重的态度，在公共场所主动接触陌生人，询问他们是否愿意对我正在开发的原型提供反馈。(对于企业级产品，收集反馈更为复杂，因为潜在客户更难寻觅。) 快速的用户反馈能让你获得速度！\n\n除了速度，我发现对于初创公司成功至关重要的第二个标准是对技术有着深刻的理解。由于 AI 技术发展迅猛，一个对 AI 的能力边界、以及何时使用何种工具都有深入技术理解的团队，将能做出更明智的决策。这不仅能创造有意义的差异化优势，还能避免在错误的道路上浪费时间。良好的技术理解，同样能让你获得速度！\n\n我非常感谢 AI Fund 的投资者、团队以及创业伙伴与我们并肩协作。未来还有许多值得我们去创造和建设！\n\n[原始文本: https://t.co/I1nkYeTkFA ]"
  },
  {
    "id": "1920161212312268988",
    "url": "https://x.com/AndrewYNg/status/1920161212312268988",
    "text": "Learn to build conversational AI voice agents in \"Building AI Voice Agents for Production\", created in collaboration with @livekit and @realavatarai, and taught by @dsa (Co-founder & CEO of LiveKit), @shayneparlo (Developer Advocate, LiveKit), and @nedteneva (Head of AI at RealAvatar, an AI Fund portfolio company).\n\nVoice agents combine speech and reasoning capabilities to enable real-time conversations. They're already being used to support customer service, to improve accessibility in healthcare, for entertainment applications, and for talk therapy.\n\nIn this course, you’ll learn to build voice agents that listen, reason, and respond naturally. You’ll follow the architecture used to create the \"AI Andrew\" Avatar, a collaborative project between https://t.co/zpIxRSuky4 and RealAvatar that responds to users in what sounds like my voice. You’ll build a voice agent from scratch and deploy it to the cloud, enabling support for many simultaneous users.\n\nWhat you’ll learn:\n- Understand the fundamentals of voice agents, including key components like speech-to-text (STT), text-to-speech (TTS), and LLMs, and how latency is introduced at each layer.\n- Explore voice agent architectures and the trade-offs between modular pipelines and speech-to-speech APIs.\n- Explore how platforms like LiveKit mitigate latency issues with optimized networking infrastructure and low-latency communication protocols.\n- Learn how to connect client devices to voice agents using WebRTC—and why it outperforms HTTP and WebSocket for low-latency audio streaming.\n- Incorporate voice activity detection (VAD), end-of-turn detection, and context management to detect turns, handle interruptions, and manage conversational flow.\n- Understand the trade-offs between latency, quality, and cost in an example in which you build a voice agent and change its voice.\n- Equip your agent with metrics to measure latency at each stage of the voice pipeline and learn the key levers you can pull to make your agent faster and more responsive.\n\nThe voice agents built in this course also incorporate voice technology from @elevenlabsio, a supporting contributor to the project.\n\nBy the end of this course, you'll have learned the components of an AI voice agent pipeline, combined them into a system with low-latency communication, and deployed them on cloud infrastructure so it scales to many users.\n\nI’m looking forward to seeing what voice agents you build from this course!\n\nPlease sign up here: https://t.co/Offr7rPtDC",
    "createdAt": "Wed May 07 16:58:02 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 163,
    "replyCount": 40,
    "likeCount": 926,
    "quoteCount": 11,
    "viewCount": 86272,
    "bookmarkCount": 652,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "在 “构建用于生产环境的 AI 语音智能体 (Building AI Voice Agents for Production)” 课程中，你将学习如何构建对话式 AI 语音智能体。本课程由 @livekit 和 @realavatarai 合作打造，并由 LiveKit 联合创始人兼 CEO @dsa、LiveKit 开发者倡导者 @shayneparlo，以及 AI Fund 投资组合公司 RealAvatar 的 AI 负责人 @nedteneva 共同授课。\n\n语音智能体 (Voice Agent) 结合了语音和推理能力，能实现实时对话。它们已被广泛应用于客户服务支持、提升医疗保健的可访问性、各类娱乐应用以及谈话疗法等领域。\n\n通过本课程，你将学习如何构建能够自然地听懂、思考并做出回应的语音智能体。你将沿用创建“AI Andrew” Avatar 所采用的架构，这是 https://t.co/zpIxRSuky4 和 RealAvatar 之间的一个合作项目，它能以与我本人相似的声音回应用户。你将从零开始构建一个语音智能体，并将其部署到云端，使其能够支持大量的并发用户。\n\n你将学到什么：\n- 了解语音智能体的基本原理，包括语音转文本 (STT)、文本转语音 (TTS) 和 大语言模型 (LLM) 等核心组件，以及每个环节中可能引入的延迟。\n- 探索不同的语音智能体架构，并权衡模块化管道与语音到语音 API 之间的优劣。\n- 了解 LiveKit 等平台如何通过优化的网络基础设施和低延迟通信协议来缓解延迟问题。\n- 学习如何使用 WebRTC 将客户端设备连接到语音智能体，以及为什么它在低延迟音频流方面优于 HTTP 和 WebSocket。\n- 掌握如何结合语音活动检测 (VAD)、回合结束检测和上下文管理，从而准确识别对话轮次、有效处理中断并管理会话流程。\n- 通过一个示例，了解在构建和改变语音智能体声音时，延迟、音质和成本之间存在的权衡关系。\n- 为你的智能体添加衡量指标，以监测语音处理管道 (voice pipeline) 各阶段的延迟，并学习可调整的关键因素，从而让你的智能体更快、响应更灵敏。\n\n本课程中构建的语音智能体还融入了来自 @elevenlabsio 的语音技术，该公司是此项目的重要贡献者。\n\n到本课程结束时，你将掌握 AI 语音智能体管道的各个组件，能够将它们组合成一个具有低延迟通信的系统，并将其部署到云基础设施上，以支持大规模用户。\n\n我非常期待看到你们通过本课程构建出的语音智能体！\n\n请在此处注册：https://t.co/Offr7rPtDC"
  },
  {
    "id": "1918839960880529641",
    "url": "https://x.com/AndrewYNg/status/1918839960880529641",
    "text": "In addition to being a great investor, @WarrenBuffett has also been a great teacher, and I'm grateful to have learned a lot from him. For example, one concept I refer to frequently at @AI_Fund is the Circle of Competence, meaning we figure out what we're good at and what we're not, and act accordingly. His stepping down from Berkshire Hathaway will be the end of an era!",
    "createdAt": "Sun May 04 01:27:51 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 105,
    "replyCount": 38,
    "likeCount": 1090,
    "quoteCount": 10,
    "viewCount": 116363,
    "bookmarkCount": 132,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "除了是一位杰出的投资者，@WarrenBuffett 更是一位伟大的老师，我非常感激能从他那里学到很多。例如，我在 @AI_Fund 经常提到的一个概念是能力圈 (Circle of Competence)，这意味着我们要弄清楚自己擅长什么、不擅长什么，并据此采取行动。他从 Berkshire Hathaway 卸任，将标志着一个时代的终结！"
  },
  {
    "id": "1917986064851255658",
    "url": "https://x.com/AndrewYNg/status/1917986064851255658",
    "text": "Video of the talk: https://t.co/C5OB2qm7OP",
    "createdAt": "Thu May 01 16:54:46 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 21,
    "replyCount": 3,
    "likeCount": 102,
    "quoteCount": 1,
    "viewCount": 55903,
    "bookmarkCount": 49,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "本次讲座的视频: https://t.co/C5OB2qm7OP"
  },
  {
    "id": "1917985792607363189",
    "url": "https://x.com/AndrewYNg/status/1917985792607363189",
    "text": "I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels.\n\nKyle’s success has been with the support of Kira Learning (an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working!\n\nA key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of the flipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python code\n\nbest_$alty_snack = 'potato chips'\n\nKira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.\n\nAdditionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.\n\nSince learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach!\n\nI talked about Kyle (and other topics) at the ASU+GSV Summit on education. You can see a video online.\n\nIn the future, people who know how to code and build with AI will be much more productive than people who don’t. I’m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI.\n\n[Original text: https://t.co/F4xAhwfHaU ]",
    "createdAt": "Thu May 01 16:53:42 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 122,
    "replyCount": 47,
    "likeCount": 786,
    "quoteCount": 15,
    "viewCount": 80764,
    "bookmarkCount": 299,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我希望我们能让每个人都能利用 AI 进行开发。从 K-12 教育阶段开始，我们应该教导每一位学生融入 AI 的编程技能，因为这将使他们成长为更高效、更有能力的人。然而，计算机科学 (CS) 教师的缺口巨大。我最近与高中篮球教练 Kyle Creasy 交流，他于 2023 年毕业，获得了体育教育学士学位。直到两年前，他从未写过一行 Python 代码。现在——在 AI 的帮助下——他不仅会编写代码，还教授计算机科学 (CS)。Kyle 的故事令我深受启发，它为在小学和中学阶段推广 CS 教育提供了一个可行的模式。\n\nKyle 的成功得到了 Kira Learning (一家由 AI Fund 投资的公司) 的支持，其创始人 Andrea Pasinetti 和 Jagriti Agrawal 为 CS 教育描绘了一个令人振奋的愿景。在 K-12 课堂上，教师扮演着重要的社会情感支持角色，例如，鼓励学生并在他们遇到困难时伸出援手。此外，他们还被期望成为能够传授其学科所需内容的学科专家。Kira Learning 采用数字化的教学内容呈现方式——包括教育视频、自动评分的测验和集成 AI 的聊天机器人来回答学生问题（但不会直接给出家庭作业答案）——这样教师就可以把更多精力放在社会情感支持上。尽管这仍处于起步阶段，但似乎已经卓有成效！\n\n实现这一目标的关键在于 AI 如今能够实现的超个性化（这与过去采用有限的翻转课堂理念形成了鲜明对比）。例如，当学生在在线编程环境中遇到问题时，如果他们写了这样一行有错误的 Python 代码：\n\nbest_$alty_snack = 'potato chips'\n\nKira Learning 的 AI 系统能立即发现问题，并直接告诉教师，美元符号 $ 在变量名中是无效字符。它还可以为教师提供一个具体的问题，引导学生思考从而摆脱困境，比如“你能说出变量名中允许使用哪些字符吗？” 鉴于 AI 能够直接向学生提供个性化建议，它现在也能帮助教师提供个性化支持，这无疑将极大地推动 K-12 教育的发展。\n\n此外，智能体工作流 (agentic workflows) 能够自动化教师的许多重复性任务。例如，在设计课程时，将内容与教育标准（例如美国的 Common Core 或许多 CS 课程的 AP CS 标准）对齐是非常耗时的。让 AI 系统来执行这类任务已经证明对教师大有裨益。\n\n自从学会编程以来，Kyle 已经开发了许多软件。他自豪地给我看他用 matplotlib 工具生成的一份分析报告，内容是关于他的篮球队员三分球投篮尝试的（如上图所示），这份分析报告反过来也影响了球队在场上的策略。一个清晰的启示是：当一位篮球教练学会编程时，他们就成为了一名更出色的篮球教练！\n\n我在 ASU+GSV 教育峰会上谈到了 Kyle (以及其他话题)。您可以在线观看相关视频。\n\n在未来，懂得如何编写代码并利用 AI 进行开发的人，将比那些不具备这些技能的人生产力高得多。我对 AI 将如何开创 K-12 教育的新模式充满期待。通过向所有人提供计算机科学 (CS) 教育，我希望未来每个人都能利用 AI 进行开发。\n\n[原始文本: https://t.co/F4xAhwfHaU ]"
  },
  {
    "id": "1915421117998874899",
    "url": "https://x.com/AndrewYNg/status/1915421117998874899",
    "text": "Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding, including vibe coding, is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts!\n\nMy background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”!\n\nBut understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!)  In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance).\n\nDifferent programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages.\n\nSimilarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory.\n\nJust as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven’t already done so, I encourage you to try having an LLM write some code in a language you’d like to learn but perhaps haven’t yet gotten around to, and see if it helps you get some new applications to work.\n\n[Original text: https://t.co/NdjaPgwwuk ]",
    "createdAt": "Thu Apr 24 15:02:35 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 472,
    "replyCount": 129,
    "likeCount": 3341,
    "quoteCount": 65,
    "viewCount": 263141,
    "bookmarkCount": 1140,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "尽管我是一个比 JavaScript 开发者更擅长 Python 的人，但在 AI 辅助下，我最近编写了大量 JavaScript 代码。AI 辅助编程，包括随心所欲的编码 (vibe coding)，正在降低特定编程语言的重要性。当然，学习一门语言仍有助于我们理解核心概念。这种方式正在帮助许多开发者使用我们不熟悉的语言编写代码，从而让我们能够在更多场景中实现代码功能！\n\n我的专业背景是机器学习工程和后端开发，但 AI 辅助编程让我能够轻松地使用 JavaScript (JS) 或 TypeScript (TS) 这些我并不擅长的语言来构建前端系统 (用户与网站或应用程序交互的部分)。生成式 AI (Generative AI) 正在让语法变得不再那么重要，所以我们所有人都可以同时成为 Python、JS、TS、C++、Java，甚至是 Cobol 开发者。或许有一天，我们中的许多人将不再被定义为“Python 开发者”或“C++ 开发者”，而只是单纯的“开发者”！\n\n然而，理解不同语言背后的核心概念依然至关重要。这就是为什么学习至少一门像 Python 这样的语言，仍然能为我们提供一个坚实的基础，以便提示 大语言模型 (LLM) 生成 Python 和其他语言的代码。如果你从一种编程语言转向另一种执行类似任务但语法不同的语言——比如从 JS 到 TS，或从 C++ 到 Java，再或从 Rust 到 Go——一旦你掌握了第一种语言的概念，你就已经了解了许多提示 大语言模型 (LLM) 用第二种语言进行编码所需的关键概念。（虽然 TensorFlow 和 PyTorch 并非编程语言，但理解 TensorFlow 背后的深度学习概念，也会让你更容易让 大语言模型 (LLM) 为你编写 PyTorch 代码，反之亦然！）此外，你还能理解大部分生成的代码 (或许只需要一点 大语言模型 (LLM) 的协助)。\n\n不同的编程语言反映了组织计算的不同思路，因此理解这些概念仍然很重要。例如，一个不理解数组 (arrays)、字典 (dictionaries)、缓存 (caches) 和内存 (memory) 的人，在让 大语言模型 (LLM) 编写大多数语言代码时，效率会较低。\n\n同样，一个转向更多使用 JS 进行前端编程的 Python 开发者，将受益于学习前端系统背后的概念。例如，如果你希望 大语言模型 (LLM) 使用 React 框架构建前端，那么理解 React 如何将前端分解为可重用的 UI 组件，以及它如何更新决定网页外观的 DOM (Document Object Model) 数据结构，将对你大有裨益。这能让你更精确地提示 大语言模型 (LLM)，并有助于你在出现问题时理解如何修复。同理，如果你想让 大语言模型 (LLM) 帮助你编写 CUDA 或 ROCm 代码，了解 GPU 如何组织计算和内存会有很大帮助。\n\n正如精通多种人类语言的人能更轻松地与他人交流一样， 大语言模型 (LLM) 正在让开发者更容易在多种应用场景中构建系统。如果你还没有尝试过，我鼓励你让 大语言模型 (LLL) 用你希望学习但可能尚未涉足的语言编写一些代码，看看它是否能帮助你成功开发出一些新的应用程序。\n\n[Original text: https://t.co/NdjaPgwwuk ]"
  },
  {
    "id": "1915101920500564406",
    "url": "https://x.com/AndrewYNg/status/1915101920500564406",
    "text": "New short course: Building Code Agents with Hugging Face smolagents!\n\nLearn how to build code agents in this course, created in collaboration with @huggingface, and taught by @Thom_Wolf, its co-founder and CSO, and @AymericRoucher, Hugging Face’s Project Lead on Agents.\n\nTool-calling agents use LLMs to generate multiple function calls sequentially to complete a complex sequence of tasks. They generate one function call, execute it, observe, reason, and decide what to do next. Code agents take a different approach. They consolidate all these calls into a single block of code, letting the LLM lay out an entire action plan at once, which can be executed efficiently to provide more reliable results.\n\nYou’ll learn how to code agents using smolagents, a lightweight agentic framework from Hugging Face. Along the way, you’ll learn how to run LLM-generated code safely and develop an evaluation system to optimize your code agent for production.\n\nIn detail, you’ll learn:\n- How agentic systems have evolved, gaining greater levels of agency over time—and why code agents are a next step.\n- How code agents write their actions in code.\n- When code agents outperform function-calling agents.\n- How to run code agents safely in your system using a constrained Python interpreter and sandboxing using E2B.\n- To trace, debug, and assess the code agent to optimize its behaviours for complex requests.\n- How to build a research multi-agent system that can find information online and organize it into an interactive report.\n\nBy the end of this course, you’ll know how to build and run code agents using smolagents, and deploy them safely with a structured evaluation system in your projects.\n\nPlease sign up here! https://t.co/grcy0rH9Hg",
    "createdAt": "Wed Apr 23 17:54:13 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 229,
    "replyCount": 37,
    "likeCount": 1260,
    "quoteCount": 12,
    "viewCount": 126757,
    "bookmarkCount": 866,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程：用 Hugging Face smolagents 构建代码 AI 智能体 (AI Agent)！\n\n在这门课程中，你将学习如何构建代码 AI 智能体。本课程由 @huggingface 合作创建，并由其联合创始人兼首席战略官 (CSO) @Thom_Wolf 以及 Hugging Face AI 智能体项目负责人 @AymericRoucher 授课。\n\n工具调用 AI 智能体利用大语言模型 (LLM) 顺序生成多个函数调用，以完成一系列复杂的任务。它们会生成一个函数调用，执行它，观察结果，进行推理，然后决定下一步该怎么做。而代码 AI 智能体则采取了一种不同的方法。它们将所有这些调用整合到一个单一的代码块中，让 LLM 能够一次性制定出完整的行动计划，这样可以高效执行并提供更可靠的结果。\n\n你将学习如何使用 Hugging Face 的轻量级 AI 智能体框架 smolagents 来编写代码 AI 智能体。在此过程中，你将学会如何安全地运行由 LLM 生成的代码，并开发一个评估系统来优化你的代码 AI 智能体，使其适用于生产环境。\n\n具体来说，你将学习：\n- AI 智能体系统如何演变，随着时间的推移获得更高水平的自主性——以及为什么代码 AI 智能体是下一步发展方向。\n- 代码 AI 智能体如何在代码中编写其行动。\n- 代码 AI 智能体何时会优于函数调用 AI 智能体。\n- 如何在你的系统中使用受限的 Python 解释器并通过 E2B 进行沙盒隔离，从而安全地运行代码 AI 智能体。\n- 追踪、调试和评估代码 AI 智能体，以优化其处理复杂请求时的表现。\n- 如何构建一个研究型多 AI 智能体系统，该系统能够在线查找信息并将其组织成交互式报告。\n\n在本课程结束时，你将学会如何使用 smolagents 构建和运行代码 AI 智能体，并利用结构化的评估系统将其安全部署到你的项目中。\n\n请在此处报名！https://t.co/grcy0rH9Hg"
  },
  {
    "id": "1912908679344693711",
    "url": "https://x.com/AndrewYNg/status/1912908679344693711",
    "text": "I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals.\n\nI wrote previously in The Batch about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals.\n\nI encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example:\n- It’s okay to start with very few examples in the eval set, say 5, and gradually add to them over time — or subtract them if you find that some examples are too easy or too hard, and not useful for distinguishing between the performance of different versions of your system.\n- It’s okay to start with evals that measure only a subset of the dimensions of performance you care about, or measure narrow cues that you believe are correlated with, but don’t fully capture, system performance. For example if, at a certain moment in the conversation, your customer-support agent is supposed to (i) call an API to issue a refund and (ii) generate an appropriate message to the user, you might start off measuring only whether or not it calls the API correctly and not worry about the message. Or if, at a certain moment, your chatbot should recommend a specific product, a basic eval could measure whether or not the chatbot mentions that product without worrying about what it says about it.\n\nSo long as the output of the evals correlates with overall performance, it’s fine to measure only a subset of things you care about when starting.\n\nThe development process thus comprises two iterative loops, which you might execute in parallel:\n- Iterating on the system to make it perform better, as measured by a combination of automated evals and human judgment;\n- Iterating on the evals to make them correspond more closely to human judgment.\n\nAs with many things in AI, we often don’t get it right the first time. So t’s better to build an initial end-to-end system quickly and then iterate to improve it. We’re used to taking this approach to building AI systems. We can build evals the same way.\n\nTo me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B:\n- If A works significantly better than B according to a skilled human judge, the eval should give A a significantly higher score than B.\n- If A and B have similar performance, their eval scores should be similar.\n\nWhenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in “error” and we should tweak it to make it rank A and B correctly. This is a similar philosophy to error analysis in building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output — such as when it outputs an incorrect label — we focus on “errors” of the evals — such as when they incorrectly rank two systems A and B, so the evals aren’t helpful in choosing between them.\n\nRelying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress.\n\n[Original text: https://t.co/V3BZe8sRWE ]",
    "createdAt": "Thu Apr 17 16:39:03 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 176,
    "replyCount": 56,
    "likeCount": 1274,
    "quoteCount": 35,
    "viewCount": 206735,
    "bookmarkCount": 1245,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我注意到，许多生成式 AI (GenAI) 应用项目往往过晚才引入系统输出的自动化评估 (evals)，并且过度依赖人工判断输出。这是因为构建评估被视为一项巨大的投入 (比如，需要创建 100 甚至 1,000 个示例，并设计和验证相关指标)，而通常人们很难找到一个合适的时间来承担这笔前期成本。但我鼓励团队将评估的构建看作一个迭代过程。从一个快速粗糙的实现 (例如，只有 5 个示例，且指标未经优化) 开始，然后随着时间的推移逐步迭代和改进，这是完全可行的。这样做可以帮助您将评估的重担逐渐从人工判断转移到自动化评估上。\n\n我之前在 The Batch 中撰文讨论过创建评估的重要性和难度。比方说，您正在开发一个能以自由文本形式回复用户的客户服务聊天机器人。由于没有唯一的正确答案，许多团队最终在每次系统更新后，都需要让人工仔细检查数十个示例输出，以判断系统是否有所提升。尽管“大语言模型 (LLM) 作为判断者”这类技术很有用，但要让它们真正发挥作用，其具体细节 (比如使用什么提示词，给判断者提供什么上下文等) 往往非常繁琐，难以精确拿捏。所有这些因素都让人觉得构建评估需要大量前期投入，因此在任何特定的一天，团队可能会认为依赖人工判断者比摸索如何构建自动化评估更能推进项目。\n\n我鼓励您以不同的方式来构建评估。您可以从一些快速但可能只是部分、不完整且带有一定误差的系统性能衡量标准开始，并对其进行迭代改进。它们可以作为人工评估的补充，而非替代。随着时间的推移，您可以逐步调整评估方法，缩小自动化评估结果与人工判断之间的差距。例如：\n- 评估集可以先从很少的示例开始，比如 5 个，然后随着时间逐步增加——如果您发现某些示例过于简单或过于困难，对区分系统不同版本的性能没有帮助，也可以将其移除。\n- 您可以先从只衡量您所关注的性能维度中的一部分，或者只衡量那些您认为与系统性能相关但无法完全捕捉的狭窄线索的评估开始。举个例子，如果在对话的某个时刻，您的客户支持智能体 (agent) 需要 (i) 调用 API 来处理退款，并 (ii) 生成一条适当的消息给用户，那么您可能可以先只衡量它是否正确调用了 API，暂时不考虑消息内容。或者，如果在某个时刻，您的聊天机器人应该推荐特定产品，一个基本的评估可以只衡量聊天机器人是否提到了该产品，而不用担心它对该产品具体说了什么。\n\n只要评估的输出与整体性能相关，那么在项目初期只衡量您关心的部分内容是完全可以的。\n\n因此，开发过程包括两个迭代循环，您可以并行执行：\n- 迭代优化系统，使其表现更好，这通过结合自动化评估和人工判断来衡量；\n- 迭代改进评估，使其更贴近人工判断的结果。\n\n正如 AI 领域的许多事物一样，我们通常无法一步到位。因此，最好是快速构建一个初始的端到端系统，然后通过迭代来不断完善它。我们已经习惯了用这种方法来构建 AI 系统，同样地，我们也可以用这种方式来构建评估。\n\n在我看来，一个成功的评估应满足以下标准。假设我们当前有一个系统 A，并且我们可能对其进行调整以得到系统 B：\n- 如果根据经验丰富的人工判断，A 的性能明显优于 B，那么评估应该给予 A 明显高于 B 的分数。\n- 如果 A 和 B 的性能相似，那么它们的评估分数也应该相似。\n\n每当系统 A 和 B 的比较结果与这些标准相悖时，就表明评估存在“错误”。此时我们应该调整评估，使其能够正确地对 A 和 B 进行排名。这与构建机器学习算法时的错误分析理念相似，只不过我们关注的不是机器学习算法输出的错误——比如它输出了不正确的标签——而是评估本身的“错误”——比如当它们错误地对两个系统 A 和 B 进行排名，导致评估无法帮助我们在这两者之间做出选择。\n\n纯粹依赖人工判断是启动项目的一个很好的方式。但对于许多团队来说，将评估作为快速原型进行构建并逐步迭代至更成熟的版本，可以帮助您更早地引入评估，并加速您的项目进展。\n\n[Original text: https://t.co/V3BZe8sRWE ]"
  },
  {
    "id": "1912560177745994098",
    "url": "https://x.com/AndrewYNg/status/1912560177745994098",
    "text": "New Short Course: Building AI Browser Agents! \n\nLearn how to build AI agents that interact and take actions on websites in this course, created in partnership with @the_agi_company and taught by  @DivGarg_ and @namangarg0, Co-founders of AGI Inc.\n\nAI browser agents can log into websites, fill out forms, click through web pages, or even place orders online for you. They use both visual information, like screenshots, and structural data, like the HTML or Document Object Model (DOM) of a web page, to reason and take action.\n\nWith the complexity of webpages and multiple possible actions at each step, it can be challenging for an AI browser agent to complete an assigned task. Because these agents run long action sequences, a single error—like clicking the wrong button or misreading a field—can lead to unexpected outcomes or errors that compound over time.\n\nIn this course, you'll understand how autonomous web agents work, their current limitations, and how AgentQ enables them to improve through self-correction.\n\nIn detail, you'll:\n- Learn what web agents are, how they automate tasks online, their architecture, key components, limitations, and an overview of their decision-making strategies.\n- Build a web agent that can scrape https://t.co/zpIxRSuky4's website and return course recommendations in a structured output format.\n- Build an autonomous web agent that can execute multiple tasks, such as finding and summarizing webpages, filling out a form, and signing up for a newsletter.\n- Explore AgentQ, a framework that enables agents to self-correct by combining Monte Carlo Tree Search (MCTS), a self-critique mechanism for continuous improvement, and Direct Preference Optimization (DPO).\n- Deep dive into MCTS, learn how it finds an effective path, illustrated by an example of Gridworld animation, and use AgentQ to complete web tasks.\n- Understand AI agents' current state and future directions—including key factors shaping their evolution, such as hardware, algorithm innovation, and data availability.\nBy the end of this course, you will have hands-on experience building browser agents and a deeper understanding of how to make them more robust and reliable.\n\nPlease sign up here: https://t.co/kTzv4NkQ8H",
    "createdAt": "Wed Apr 16 17:34:14 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 302,
    "replyCount": 50,
    "likeCount": 1818,
    "quoteCount": 21,
    "viewCount": 183935,
    "bookmarkCount": 1881,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程: 构建 AI 浏览器智能体!\n\n本课程与 @the_agi_company 合作开发，由 AGI Inc. 的联合创始人 @DivGarg_ 和 @namangarg0 授课，旨在教您如何构建能够与网站互动并采取行动的 AI 智能体 (AI agents)。\n\nAI 浏览器智能体 (AI Browser Agents) 可以执行多种任务，例如登录网站、填写表格、点击网页，甚至为您在线下单。它们会结合视觉信息 (visual information) (如网页截图) 和结构化数据 (structural data) (如网页的 HTML 或文档对象模型 (Document Object Model, DOM)) 来进行推理并采取行动。\n\n然而，由于网页的复杂性以及每一步可能存在的多种操作，AI 浏览器智能体 (AI Browser Agents) 在完成指定任务时会面临挑战。这些智能体 (agents) 通常需要执行冗长的操作序列，单个错误 (例如点错按钮或误读某个字段) 都可能导致意外结果，甚至随时间推移积累成更大的问题。\n\n通过本课程，您将深入了解自主网络智能体 (autonomous web agents) 的工作原理、它们目前的局限性，以及 AgentQ 如何通过自我修正机制来提升它们的表现。\n\n具体来说，您将：\n- 了解什么是网络智能体 (web agents)、它们如何在线实现任务自动化、它们的架构、关键组件、局限性以及它们决策策略的概览。\n- 构建一个网络智能体 (web agent)，能够抓取 https://t.co/zpIxRSuky4 网站的内容，并以结构化的输出格式 (structured output format) 返回课程推荐。\n- 构建一个自主网络智能体 (autonomous web agent)，能够执行多项任务，例如查找和总结网页、填写表格以及注册电子报。\n- 探索 AgentQ 框架，它通过结合蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)、用于持续改进的自我批评机制 (self-critique mechanism) 以及直接偏好优化 (Direct Preference Optimization, DPO)，使智能体 (agents) 具备自我修正的能力。\n- 深入探究 MCTS，学习它如何找到有效路径，并通过 Gridworld 动画示例进行说明，然后使用 AgentQ 来完成实际的网络任务。\n- 理解 AI 智能体 (AI agents) 的当前发展状况和未来趋势——包括塑造其演进的关键因素，例如硬件进步、算法创新 (algorithm innovation) 和数据可用性 (data availability)。\n完成本课程后，您将获得构建浏览器智能体 (browser agents) 的实践经验，并对如何使它们更加健壮和可靠有更深刻的理解。\n\n请在此处注册：https://t.co/kTzv4NkQ8H"
  },
  {
    "id": "1910388768487727535",
    "url": "https://x.com/AndrewYNg/status/1910388768487727535",
    "text": "I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn’t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other.\n\nMuch has been written about why high, widespread taxes on imports are harmful. In this letter, I’d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is.\n\nHowever, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI.\n\nWith regard to data-center buildouts, another silver lining is that, with the rise of generative AI, data gravity has decreased because compute processing costs are much greater than transmission costs, meaning it’s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally.\n\nFinally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vance pointed out in 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel — or know-how, or supply chain — to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI’s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small.\n\nMy 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit — he was proud that he’s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I’ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids.\n\nI don’t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let’s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas — including specifically open source software — and keep supporting each other. Let’s all do what we can to keep the world as connected as we are able.\n\n[I had written this letter before the 90 day pause on the tariffs, but am sharing this here since many of the points are still relevant depends on what happens next.] \n\nOriginal text: https://t.co/fNyTqzABWy",
    "createdAt": "Thu Apr 10 17:45:50 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 364,
    "replyCount": 198,
    "likeCount": 2978,
    "quoteCount": 50,
    "viewCount": 333496,
    "bookmarkCount": 627,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我很抱歉，美国正在辜负我们的朋友和盟友。广泛的关税，不仅针对对手，也针对坚定的盟友实施，将损害数十亿人的生计，造成通货膨胀，使世界更加碎片化，并使美国和全球变得更加贫穷。人工智能 (AI) 并非万能药，但即使在这种充满挑战的环境中，我希望我们的社区能够团结起来，继续建立跨越国界的友谊，继续分享思想，并继续互相支持。\n\n关于为什么高额、广泛的进口税有害，此前已有诸多论述。在这封信中，我希望能重点探讨其对 AI 可能产生的影响。新关税的一个亮点是，它们主要针对实物进口，而非数字商品和服务，这其中包括 AI 研究发明和软件等知识产权 (IP)。知识产权难以征税，因为每一项知识产权都独一无二，因此难以估价，并且它可以通过互联网几乎没有阻力地跨越国界。许多国际 AI 团队跨越国界和时区进行协作，而软件，特别是开源软件，是分享思想的重要机制。我希望这种思想的自由流动能够不受阻碍，即使实物商品的流动受到限制。\n\n然而，AI 依赖硬件，而关税将通过限制硬件的获取来减缓 AI 的发展。尽管在最后一刻对半导体做出了例外处理，但对太阳能电池板、风力涡轮机以及其他发电和配电设备的进口征税，将削弱美国数据中心的供电能力。对服务器、冷却硬件、网络硬件等设备的进口征税，也将使数据中心的建设成本更高。此外，对笔记本电脑和手机等消费电子产品征税，将使公民学习和使用 AI 变得更加困难。\n\n关于数据中心建设，另一个好消息是，随着生成式 AI (Generative AI) 的兴起，数据引力 (data gravity) 有所降低，因为计算处理成本远大于传输成本。这意味着在全球任何地方放置数据中心都比仅靠近最终用户更可行。尽管许多地方缺乏足够的合格技术人员来建造和运营数据中心，但我预计关税将鼓励数据中心在全球各地建设，从而在全球创造更多就业机会。\n\n最后，关税将增加国内制造业的压力，这可能会为机器人技术和工业自动化带来一些微弱的推动作用。正如美国副总统 J.D. Vance 在 2017 年指出的那样，美国应该专注于自动化（和教育），而非关税。但美国目前缺乏足够的人员——或者说技术诀窍，或者说完善的供应链——来制造它目前依赖盟友生产的许多商品。机器人技术有助于解决这一系列巨大挑战中的一小部分。生成式 AI 在机器人技术方面的进步速度也显著慢于其在处理文本、视觉数据、音频和推理方面的速度。因此，尽管关税可能会为 AI 驱动的机器人技术带来利好，但我预计这种影响会很小。\n\n我 4 岁的儿子抱怨了好几个星期他的鞋子太紧——他为自己长大了感到自豪！所以上周日，我们去买鞋了。他的新鞋花了 25 美元，结账时，我停下来感慨自己能负担得起这些鞋子是多么幸运。但我也想到了许多靠工资勉强度日的家庭，对他们来说，如果关税导致鞋子每双高达 40 美元，就意味着他们会让孩子穿着不合脚的鞋子更长时间。我还想到了我在亚洲和拉丁美洲的服装制造厂遇到的人们，对他们来说，需求减少将意味着工作更少，带回家的钱更少，给他们的孩子。\n\n我不知道美国关税接下来会发生什么，但无论美国是否参与，许多国际贸易仍会进行。我希望我们能回到一个充满活力的全球贸易世界，伴随着强大且基于规则的美国参与。在此之前，我们所有 AI 界的同仁，请继续培养我们的国际友谊，继续保持思想的数字化流动——特别是开源软件——并继续互相支持。让我们尽我们所能，让世界保持尽可能紧密的联系。\n\n[我是在关税暂停 90 天之前写的这封信，但在此分享，因为许多观点依然相关，这取决于接下来会发生什么。]"
  },
  {
    "id": "1907843984158036137",
    "url": "https://x.com/AndrewYNg/status/1907843984158036137",
    "text": "Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it’s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I’d like to share when and how I use “lazy prompting.”\n\nWhen debugging code, many developers copy-paste error messages — sometimes pages of them — into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don’t need to explicitly tell them. With brief instructions like “Edit this: …” or “sample dotenv code” (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text.\n\nAt the other end of the spectrum, sometimes  I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer.\n\nI don’t try a lazy prompt if (i) I feel confident there’s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI’s Agentic Doc Extraction!), I should say so in the prompt, since otherwise it’s very hard for the LLM to guess my preference. I also wouldn’t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want.\n\nBy the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you’ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM’s web or app interface. It doesn’t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won’t be examining every output to clarify and iterate if the output is poor.\n\nThank you to Rohit Prsad, who has been collaborating with me on the open-source package aisuite, for suggesting the term lazy prompting. There is an analogy to lazy evaluation in computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed.\n\nOriginal text: https://t.co/Doh0TdJpO3",
    "createdAt": "Thu Apr 03 17:13:46 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 167,
    "replyCount": 77,
    "likeCount": 1419,
    "quoteCount": 38,
    "viewCount": 146030,
    "bookmarkCount": 733,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "与标准提示建议——即你应该为大语言模型 (LLM) 提供它们成功所需的所有上下文——不同的是，我发现有时偷懒反而更快：只需快速草拟一个不精确的提示，然后观察结果。判断这种做法是否明智的关键在于你能否迅速评估输出质量，从而决定是否需要提供更多上下文。在这篇文章中，我将分享我何时以及如何使用“惰性提示 (lazy prompting)”。\n\n在调试代码时，许多开发者常常直接将错误信息——有时多达好几页——复制粘贴到大语言模型中，而不附带任何额外指令。大多数大语言模型都足够智能，能理解你希望它们帮助理解问题并提出修复方案，因此你无需明确告知。只需像“编辑此内容：...”或“示例 dotenv 代码”（用来提醒你如何编写代码以使用 Python 的 dotenv 包）这样简短的指令，大语言模型通常就能生成令人满意的响应。此外，如果响应存在缺陷，你也能及时发现问题并优化提示，例如引导大语言模型如何修改你的文本。\n\n当然，在另一方面，有时我也会花费 30 分钟仔细撰写一份长达 2 页的提示，以借助 AI 系统解决一个复杂问题（例如编写多页代码），而这在没有 AI 帮助的情况下，可能会花费我更长的时间。\n\n我不会在以下两种情况下尝试惰性提示： (i) 我确信，如果没有额外上下文，大语言模型不可能提供一个好的解决方案。例如，给定一个不完整的程序规格，即使是经验丰富的人类开发者，也很难理解你到底想要什么。如果我绝对想使用某个特定的 PDF 转文本转换软件 (比如我团队 LandingAI 的 Agentic Doc Extraction!)，我就应该在提示中明确说明，否则大语言模型将很难猜到我的偏好。 (ii) 当一个有缺陷的实现需要很长时间才能被发现时，我也不会使用惰性提示。举例来说，如果我判断输出是否正确的唯一方法是费力地运行代码来检查其功能，那么最好预先花时间提供上下文，这样能大大增加大语言模型生成我所需内容的可能性。\n\n顺带一提，惰性提示是一种高级技巧。总的来说，我发现大多数人在给大语言模型提供上下文时，常常是给得太少而非太多。惰性只有在你已经学会如何提供足够上下文的基础上，再有意识地尝试“退一步”，看看在提供最少上下文的情况下是否依然能奏效时，才是一种有效的技术。此外，惰性提示仅适用于你能通过大语言模型的网页或应用界面进行快速迭代的情境。它不适用于那些为反复调用 API 而编写在代码中的提示，因为在这种情况下，如果输出质量不佳，你可能不会逐一检查每个输出并进行澄清和迭代。\n\n感谢 Rohit Prsad 提出了“惰性提示”这个术语，他一直与我合作开发开源软件包 aisuite。这与计算机科学中的惰性求值 (lazy evaluation) 有异曲同工之妙，惰性求值指的是在最晚的时刻，并且仅当特定结果被需要时才调用函数。在惰性提示中，我们也是只在需要时才向提示中添加细节。\n\n原文链接: https://t.co/Doh0TdJpO3"
  },
  {
    "id": "1907471607314133126",
    "url": "https://x.com/AndrewYNg/status/1907471607314133126",
    "text": "New Short Course: Getting Structured LLM Output!\n\nLearn how to get structured outputs from your LLM applications in this course, built in partnership with @dottxtai, and taught by @willkurt, a Founding Engineer, and @cameron_pfiffer , Developer Relations Engineer.\n\nIt's challenging for software to automatically parse through an LLM's freeform text outputs. Structured outputs—like JSON—solve this by converting natural language into consistent, clear, data that a machine can read and process. This course teaches you how to generate structured outputs while building several use cases, including a social media analysis agent.\n\nYou’ll learn about structured outputs and efficient ways to generate outputs in your defined schema or format. You’ll begin by using structured output APIs, then use re-prompting libraries like “instructor” to generate structured output. Finally, you’ll learn how constrained decoding works; this is a very clever technique in which constraints are applied on each subsequent token generated, blocking any tokens that don’t fit your defined schema.\n\nIn detail, you’ll:\n- Learn why structured outputs are important, how they allow for scalable software development, and the different approaches to generate them, including vendor-provided APIs, re-prompting libraries, and structured generation.\n- Build a simple social media agent using OpenAI’s structured output API, learn how to define a model's desired structured output using Pydantic, and perform basic programming with your outputs, such as importing structured data into a data frame using pandas.\n- Learn how to use the open-source library \"instructor,\" which checks the structured output of the model and re-prompts the model until it validates the desired output, and explore the limitations of this approach.\n- Understand how structured generation by the “outlines” library works by modifying LLM logits, on a per-generated-token basis based on the desired format, to give a particular output structure.\n- Learn how regular expressions, which outlines works with, are represented as finite-state machines, and how they can be used to develop a range of structured outputs beyond JSON.\n\nBy the end of this course, you’ll have broadened your knowledge of the approaches you can use to get structured outputs from your LLM applications.\n\nPlease sign up here: https://t.co/3k53vgEFj3",
    "createdAt": "Wed Apr 02 16:34:04 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 186,
    "replyCount": 32,
    "likeCount": 1267,
    "quoteCount": 21,
    "viewCount": 88725,
    "bookmarkCount": 860,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程：掌握大语言模型结构化输出！\n\n本课程将教您如何从大语言模型 (LLM) 应用程序中获取结构化输出。这门课程是与 @dottxtai 合作开发的，由创始工程师 @willkurt 和开发者关系工程师 @cameron_pfiffer 共同授课。\n\n让软件自动解析大语言模型的自由文本输出是一项挑战。结构化输出——例如 JSON——通过将自然语言转化为机器可读和处理的一致、清晰的数据，有效解决了这一难题。本课程将教您如何生成结构化输出，同时还会通过构建多个实际用例进行演示，其中包括一个社交媒体分析 AI 智能体 (AI agent)。\n\n您将学习什么是结构化输出，以及如何高效地按照您定义的模式或格式生成输出。课程将从使用结构化输出 API 开始，然后教您如何利用像“instructor”这样的重新提示 (re-prompting) 库来生成结构化输出。最后，您将深入了解受限解码 (constrained decoding) 的工作原理；这是一项非常巧妙的技术，它对每个后续生成的 Token 施加约束，从而阻止任何不符合您预设模式的 Token 被生成。\n\n具体来说，您将：\n- 了解结构化输出的重要性，它们如何促进可扩展的软件开发，以及生成结构化输出的各种方法，包括由厂商提供的 API、重新提示库和结构化生成技术。\n- 使用 OpenAI 的结构化输出 API 构建一个简单的社交媒体 AI 智能体，学习如何使用 Pydantic 定义模型所需的结构化输出，并利用这些输出进行基础编程，例如使用 pandas 将结构化数据导入数据框 (data frame)。\n- 学习如何使用开源库“instructor”，该库会检查模型的结构化输出，并在必要时重新提示模型，直到输出符合预期，同时也会探讨这种方法的局限性。\n- 理解“outlines”库的结构化生成原理：它如何通过根据所需格式，对大语言模型每个生成的 Token 的 logits 进行修改，从而实现特定的输出结构。\n- 学习正则表达式（“outlines”库使用的核心技术）如何表示为有限状态机 (finite-state machines)，以及如何利用它们来开发除 JSON 之外的更广泛的结构化输出。\n\n完成本课程后，您将掌握更多从大语言模型应用程序获取结构化输出的方法。\n\n请在此处注册：https://t.co/3k53vgEFj3"
  },
  {
    "id": "1907132637963260223",
    "url": "https://x.com/AndrewYNg/status/1907132637963260223",
    "text": "Major program launch: Data Analytics Professional Certificate! This large, five-course sequence takes you all the way to being job-ready as a data analyst, and shows how to use Generative AI as a thought partner to enhance your work in this role.\n\nOffered by https://t.co/zpIxRSuky4 on Coursera, this is taught by Sean Barnes, Ph.D., a Data Science & Engineering Leader at Netflix.\n\nAnalyzing data remains one of the most important skills in where the world is going with AI. This comprehensive certificate takes you all the way to being job-ready. \n\nEach course comes with practical projects demonstrated in real-world contexts, such as analyzing sales data for a Korean bakery, video game sales trends across different regions, or identifying factors impacting customer retention for a communications company. You'll also work on estimating fire distribution for forest fire prevention, analyzing how a diamond's properties affect its market value, and developing predictive models for retail sales analysis, carbon emissions, and coral reef conservation.\n\nHere's some of what you'll learn:\n- How to define data and categorize it into its many types such as discrete & continuous numerical, structured & unstructured, time series, categorical, and know what insights can be derived from the different types of data categories.\n- How to differentiate between data-related job roles and their responsibilities, and how data flows through an organization from the moment of capture to decision-making.\n- How to perform data processing functions and apply conditional formatting in spreadsheets to extract business value from your data using statistical calculations and best practices for visualizing and interpreting data.\n- How to use LLMs for stakeholder analysis, data exploration, and data visualization.\n- Best practices for using LLMs for as a thought partner to data analysis work\n\nBy the end of this professional certificate program, you will have learned core statistical concepts, analysis techniques, and visualization methodologies that will serve as the foundation for working as a data analyst.\n\nThe world needs more data analysts, especially ones who know how to use modern generative AI. With data science roles projected to grow 36% by 2033, the skills taught in this program create new professional opportunities in data.\n\nSign up here! https://t.co/R2ZiJQCn5g",
    "createdAt": "Tue Apr 01 18:07:08 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 165,
    "replyCount": 54,
    "likeCount": 849,
    "quoteCount": 3,
    "viewCount": 83413,
    "bookmarkCount": 758,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "<font color=\"#FF0000\">重磅项目上线</font>: 数据分析专业证书！这个包含五门课程的大型系列，将带您一步步成为<font color=\"#FF0000\">一名能够胜任工作</font>的数据分析师，并展示如何利用<font color=\"#FF0000\">生成式 AI (Generative AI)</font> 作为思想伙伴来提升您在该领域的工作表现。\n\n本课程由 https://t.co/zpIxRSuky4 在 Coursera 平台提供，由 Netflix 的数据科学与工程主管 Sean Barnes 博士亲自授课。\n\n分析数据依然是<font color=\"#FF0000\">在 AI 驱动的世界中</font>最重要的技能之一。这个全面的证书课程将助您<font color=\"#FF0000\">具备数据分析师的工作能力</font>。\n\n每门课程都包含在真实世界场景中演示的实践项目，例如分析一家韩国面包店的销售数据、探究不同地区视频游戏的销售趋势，或者识别影响某通信公司<font color=\"#FF0000\">客户留存</font>的因素。您还将学习估算森林火灾的分布情况以进行火灾预防、分析钻石的特性如何影响其市场价值，以及开发用于零售销售分析、碳排放和珊瑚礁保护的预测模型。\n\n以下是您将学到的一些核心内容:\n- 如何定义数据并将其归类为多种类型，例如<font color=\"#FF0000\">离散型 (discrete) 和连续型 (continuous) 数值数据、结构化 (structured) 和非结构化 (unstructured) 数据、时间序列 (time series) 数据、分类型 (categorical) 数据</font>，并了解如何从不同类型的数据中获得<font color=\"#FF0000\">洞察</font>。\n- 如何区分与数据相关的工作角色及其职责，以及数据从<font color=\"#FF0000\">获取</font>到决策制定在组织内部的流动方式。\n- 如何执行数据处理功能，并在电子表格中应用条件格式，通过统计计算和数据可视化与解读的最佳实践，从数据中提取商业价值。\n- 如何使用<font color=\"#FF0000\">大语言模型 (LLM)</font> 进行利益相关者分析、数据探索和数据可视化。\n- 将 LLM 作为数据分析工作的思想伙伴的最佳实践。\n\n完成本专业证书课程后，您将掌握核心统计概念、分析技术和<font color=\"#FF0000\">可视化方法论</font>，这些都将为您成为数据分析师<font color=\"#FF0000\">奠定坚实基础</font>。\n\n世界需要更多的数据分析师，尤其是那些懂得如何运用现代生成式 AI 的专业人才。随着数据科学职位的预计到 2033 年增长 36%，本课程中教授的技能将在数据领域为您创造全新的职业机遇。\n\n点击这里注册！https://t.co/R2ZiJQCn5g"
  },
  {
    "id": "1904929635043074478",
    "url": "https://x.com/AndrewYNg/status/1904929635043074478",
    "text": "New short course: Vibe Coding 101 with Replit! Learn to build and host applications with an AI agent in this course, built in partnership with @Replit and taught by its President @pirroh and Head of Developer Relations @mattppal.\n\nCoding agents are changing how we write code. \"Vibe coding\" refers to a growing practice where you might barely look at the generated code, and instead focus on the architecture and features of your application. However, contrary to popular belief, effectively coding this way isn't done by just prompting, accepting all recommendations, and hoping for the best. It requires structuring your work, refining your prompts, and having a systematic process that lead to a more efficient and effective workflow.\n\nI code frequently using LLMs, and asking an LLM to do everything in one shot usually does not work. I'll typically take a problem, partition it into manageable modules, spend time creating prompts to specify each module, and use the model to produce the code one module at a time, and test/debug each module before moving on. A process like this is making me and many other developers faster and more efficient.\n\nIn this video-only course, you’ll learn how to use Replit’s cloud environment--with an integrated code editor, package manager, and deployment tools--to build and deploy web applications. Along the way, you’ll learn strategies for working effectively with agents and improve your development skills.\n\nIn detail, you’ll:\n- Understand principles of agentic code development such as being precise, giving agents one task at a time, making prompts specific, keeping projects tidy, starting with fresh sessions for each new feature, and how to approach debugging.\n- Learn how to get started with Replit, and key skills for vibe coding: Thinking, using frameworks, checkpoints, debugging, and providing context.\n- Create a product requirement document (PRD) and wireframe for your agent to build a prototype of a website performance analyzer.\n- See how to use an agent to make your prototype more visually appealing, and deploy it application others to access .\n- Learn to build a head-to-head national park ranking app, from a sample dataset, with voting capabilities and persistent data storage, and refine further ask the assistant to recap and explain what it built to find room for improvement and reinforce your learning.\n\nBy the end of this course, you’ll have a solid foundation in building with coding agents, and a process you can use to keep vibe coding effectively.\n\nPlease sign up here: https://t.co/yDbX1QFTI7",
    "createdAt": "Wed Mar 26 16:13:11 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 691,
    "replyCount": 110,
    "likeCount": 4227,
    "quoteCount": 136,
    "viewCount": 743254,
    "bookmarkCount": 5174,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新短课程：Replit “Vibe Coding 101” 开课啦！这门课程与 @Replit 合作推出，由其总裁 @pirroh 和开发者关系主管 @mattppal 亲自授课，将教你如何使用 AI 智能体 (AI agent) 来构建和托管应用程序。\n\n编码智能体 (Coding agents) 正在改变我们编写代码的方式。“Vibe coding” 指的是一种日益兴盛的实践，在这种模式下，你可能几乎不用查看生成的代码，而是将重心放在应用程序的架构和功能设计上。然而，与普遍的看法不同，高效地进行 Vibe coding 并非简单地通过给出提示、接受所有建议并寄希望于最好的结果就能实现。它要求你结构化工作、优化提示，并遵循系统化的流程，才能带来更高效、更有效的工作流。\n\n我个人经常使用大语言模型 (LLM) 进行编码，而让 LLM 一次性完成所有任务通常是行不通的。我的做法是，先将问题分解成易于管理的模块，然后花时间为每个模块创建具体的提示，并利用模型逐个生成模块代码，在进入下一个模块之前，对每个模块进行测试和调试。这样的流程显著提升了我和许多其他开发者的效率。\n\n在这个纯视频课程中，你将学习如何利用 Replit 的云环境——它内置了集成代码编辑器、包管理器和部署工具——来构建和部署 Web 应用程序。在此过程中，你将掌握与 AI 智能体有效协作的策略，并全面提升你的开发技能。\n\n具体来说，你将：\n- 理解智能体辅助代码开发 (agentic code development) 的核心原则，例如保持精确性、每次只分配一个任务给智能体、使提示具体化、保持项目整洁、为每个新功能开启一个全新的会话，以及如何着手调试。\n- 学习如何开始使用 Replit，以及 Vibe coding 的关键技能：思考、运用框架、设置检查点、调试和提供上下文。\n- 为你的智能体创建一个产品需求文档 (PRD) 和线框图，以构建一个网站性能分析器的原型。\n- 了解如何使用智能体让你的原型更具视觉吸引力，并部署应用供他人访问。\n- 学习如何从样本数据集中构建一个国家公园“对战”排名应用，它将具备投票功能和持久数据存储。你还将进一步优化应用，并请 AI 助手总结和解释它所构建的内容，以发现改进空间并巩固学习成果。\n\n学完本课程后，你将为使用编码智能体进行开发打下坚实基础，并掌握一套能让你持续高效进行 Vibe coding 的流程。\n\n请点击此处报名：https://t.co/yDbX1QFTI7"
  },
  {
    "id": "1903147778097983709",
    "url": "https://x.com/AndrewYNg/status/1903147778097983709",
    "text": "Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out shortly after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event.\n\nI'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so.\n\nBased on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you!\n\nOther aspects of the event that struck me:\n- First, agentic AI continues to be a strong theme. The topic attendees most wanted to hear about (based on free text responses to our in-person survey at the start of the event) was agents!\n- Google's Paige Bailey talked about embedding AI in everything and using a wide range of models to do so. I also particularly enjoyed her demos of Astra and Deep Research agents.\n- Meta's Amit Sangani talked compellingly as usual about open models. Specifically, he described developers fine-tuning smaller models on specific data, resulting in superior performance than with large general purpose models. While there're still many companies using fine-tuning that should really just be prompting, I'm also seeing continued growth of fine-tuning in applications that are reaching scale and that are becoming valuable.\n- Many speakers also spoke about the importance of being pragmatic about what problems we are solving, as opposed to buying into the AGI hype. For example, Nebius' Roman Chernin put it simply: Focusing on solving real problems is important!\n- Lastly, I was excited to hear continued enthusiasm for the Voice Stack. Justin Uberti gave a talk about OpenAI’s realtime audio API to a packed room, with many people pulling out laptops to try things out themselves in code!\n\nhttps://t.co/zpIxRSuky4 has a strong “Learner First” mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions.\n\nI'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future.\n\n[Original text: https://t.co/iNUywKfGRx ]",
    "createdAt": "Fri Mar 21 18:12:43 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 47,
    "replyCount": 33,
    "likeCount": 268,
    "quoteCount": 7,
    "viewCount": 45778,
    "bookmarkCount": 63,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "上周五的圆周率日，我们成功举办了面向 AI 开发者的新会议——AI Dev 25。尽管门票在公布发售不久后就（很遗憾地）一抢而空，但这一天与众多 AI 开发者一起编码和进行技术交流，让我倍感振奋！现在，我来分享一下这次活动的一些观察。\n\n我决定发起 AI Dev 会议，是因为虽然有许多优秀的学术 AI 会议，专注于传播研究成果 ( 例如 NeurIPS、ICML 和 ICLR )，也有不少由大公司主办的活动，通常侧重于自家产品，但面向 AI 开发者、且保持厂商中立的会议却寥寥无几。如今，随着 AI 工具的日益丰富，开发者们拥有无数机会去创造新事物 ( 并交流创造的经验！)，因此，一个中立的交流平台来帮助他们实现这些就显得尤为必要。\n\n根据一项非正式调查，大约一半的与会者不远万里，从旧金山湾区以外的地方赶来参加这次盛会，其中许多人甚至来自海外。看到大家对融入这个 AI 开发者社区所展现出的热情，我感到非常激动。在此，向所有到场的同仁们表示衷心的感谢！\n\n本次活动中，还有其他一些方面给我留下了深刻印象：\n- 首先，AI 智能体 ( agentic AI ) 依旧是大会的热门主题。根据活动开始时我们现场调查的自由文本回复，与会者最想听到的就是关于智能体的话题！\n- Google 的 Paige Bailey 探讨了如何将 AI 融入万物，并利用各种模型来实现这一目标。我尤其喜欢她对 Astra 和 Deep Research 智能体进行的演示。\n- Meta 的 Amit Sangani 一如既往地，生动地讲述了开源模型。他具体描述了开发者如何通过在特定数据上对小型模型进行微调，从而获得比大型通用模型更出色的性能。虽然目前仍有不少公司在使用微调，而他们可能更适合使用提示词工程，但我观察到微调在那些已具规模且日益重要的应用中，呈现出持续增长的态势。\n- 许多演讲者还强调了在解决问题时保持务实的重要性，而非盲目追逐通用人工智能 ( AGI ) 的炒作。例如，Nebius 的 Roman Chernin 观点明确：专注于解决实际问题至关重要！\n- 最后，我很高兴地看到大家对 Voice Stack 持续饱满的热情。Justin Uberti 在一个座无虚席的会场里，就 OpenAI 的实时音频 API 进行了演讲，许多人纷纷拿出笔记本电脑，亲自动手在代码中实践！\n\nhttps://t.co/zpIxRSuky4 秉持着强烈的“学习者优先”理念；我们的首要目标始终是帮助学习者。我很高兴有几位与会者告诉我，他们非常喜欢会议的技术深度，并表示学到了许多能直接应用到实践中的知识。( 事实上，我自己也从这些环节中获得了一些新想法！) 同样令我惊讶的是，无论是演讲环节还是技术演示展位，会场都座无虚席，与会者整天都保持着高度的专注和投入。我很高兴我们能够举办一场充满技术和工程讨论的盛会。\n\n我非常欣喜 AI Dev 25 取得了圆满成功，并衷心感谢所有使这次活动成为可能的与会者、志愿者、演讲者、赞助商、合作伙伴以及团队成员。我唯一的遗憾是，由于活动空间的物理限制，我们这次未能接纳更多的与会者。面对面地将人们聚集在一起，分享想法、结交朋友、互相学习和帮助，这本身就是一件充满魔力的事情。我希望未来我们能够汇聚更多的人。\n\n[Original text: https://t.co/iNUywKfGRx ]"
  },
  {
    "id": "1902395485601853941",
    "url": "https://x.com/AndrewYNg/status/1902395485601853941",
    "text": "New short course: Long-Term Agentic Memory with LangGraph. Learn to build an agent with long-term memory in this course developed in collaboration with @LangChainAI taught by its Co-Founder and CEO, @hwchase17! \n\nPersonal assistance and productivity tasks have become important use cases for agents. An important feature of an AI assistant, such as a coding or calendar assistant, is its ability to keep improving over time from its experience. Agent memory is the key capability that enables this.\n\nTo add memory to an agent, you must first figure out what to store and what to retrieve when it is time to use the information. Additionally, you’ll have to decide when to update the stored information. For example, you might update in each iteration loop of the agent or perform updates in the background, with a helper agent.\n\nIn this course, you will learn a mental framework to build agents with long-term memory. You'll create a useful email assistant that can respond, ignore, and notify using writing, scheduling, and memory-management tools. You’ll develop your agent's memory by adding facts to its memory store, provide examples to learn the user's preferences, and optimize system prompts to evolve instructions based on previous responses.\n\nIn detail, you’ll:\n- Learn how the three types of memory--semantic, episodic, and procedural–and the two update mechanisms–via hot path and in the background–apply to your agents.\n- Build an email agent with writing, scheduling, and availability tools, along with a router that triages incoming email and handles it accordingly by ignoring, responding, or notifying the user.\n- Add tools to your email agent that allow it to operate on semantic memory by learning facts about the user, storing them in a long-term memory store, and searching over them in future interactions.\n- Incorporate episodic memory, in the form of few-shot examples, in the triage step of your agents to help them learn and update user preferences.\n- Add procedural memory as system prompts, optimized with feedback to improve the instructions the agent follows.\n\nLearn how to approach memory in agents, and start building agents with long-term memory with LangGraph!\n\nPlease sign up here: https://t.co/9E02gQDdiM",
    "createdAt": "Wed Mar 19 16:23:23 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 267,
    "replyCount": 50,
    "likeCount": 1619,
    "quoteCount": 18,
    "viewCount": 129706,
    "bookmarkCount": 1188,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新短期课程：LangGraph 中的 AI 智能体长效记忆。在这个与 @LangChainAI 合作开发，由其联合创始人兼首席执行官 @hwchase17 教授的课程中，学习如何构建一个具有长期记忆的 AI 智能体 (AI Agent)！\n\n个人协助和生产力任务已成为 AI 智能体的重要应用场景。一个 AI 助手，例如编码或日历助手，其一个重要特点就是能够随着时间的推移，通过学习经验不断改进。AI 智能体的记忆能力是实现这一功能的关键。\n\n要为 AI 智能体添加记忆，你首先必须明确要存储什么，以及当需要使用信息时，应该检索什么。此外，你还需要决定何时更新存储的信息。例如，你可以在 AI 智能体的每个迭代循环中进行更新，或者在后台通过一个辅助 AI 智能体执行更新。\n\n在本课程中，你将学习一个构建具有长期记忆的 AI 智能体的思维框架。你将创建一个实用的电子邮件助手，它能利用写作、日程安排和记忆管理工具来响应、忽略或通知。你将通过向其记忆存储添加事实、提供示例来学习用户的偏好，并优化系统提示 (system prompts) 以根据之前的响应调整指令，从而不断发展你的 AI 智能体的记忆能力。\n\n具体来说，你将：\n- 学习三种记忆类型——语义记忆 (semantic memory)、情景记忆 (episodic memory) 和程序记忆 (procedural memory)——以及两种更新机制——即时更新 (via hot path) 和后台更新 (in the background)——如何应用于你的 AI 智能体。\n- 构建一个电子邮件 AI 智能体，它配备写作、日程安排和可用性工具，以及一个路由器 (router)，用于分类传入的电子邮件并相应地处理，如忽略、回复或通知用户。\n- 为你的电子邮件 AI 智能体添加工具，使其能够通过学习关于用户的事实、将它们存储在长期记忆库中，并在未来的交互中搜索这些事实，以此来管理语义记忆。\n- 在你的 AI 智能体的分类步骤中，以少样本 (Few-shot) 示例的形式融入情景记忆，帮助它们学习并更新用户偏好。\n- 将程序记忆作为系统提示添加进去，并通过反馈进行优化，以改进 AI 智能体遵循指令的方式。\n\n学习如何处理 AI 智能体中的记忆，并开始使用 LangGraph 构建具有长期记忆的 AI 智能体吧！\n\n请在此处注册：https://t.co/9E02gQDdiM"
  },
  {
    "id": "1900639477279977618",
    "url": "https://x.com/AndrewYNg/status/1900639477279977618",
    "text": "OpenAI’s Justin Uberti at AI Dev 25 showing how to build a voice agent using the Realtime API. Building on the voice stack is easier than most realize - worth trying out if you have a voice idea! https://t.co/GOTyw7Vftl",
    "createdAt": "Fri Mar 14 20:05:38 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 78,
    "replyCount": 51,
    "likeCount": 472,
    "quoteCount": 3,
    "viewCount": 75035,
    "bookmarkCount": 192,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "在 AI Dev 25 大会上，OpenAI 的 Justin Uberti 展示了如何使用 Realtime API 来构建一个语音 AI 智能体 (AI Agent)。其实，在语音技术栈 (voice stack) 上开发语音应用远比大多数人想象的要简单——如果你有任何与语音相关的想法，都非常值得一试！https://t.co/GOTyw7Vftl"
  },
  {
    "id": "1900617330906067136",
    "url": "https://x.com/AndrewYNg/status/1900617330906067136",
    "text": "Good tip from Replit’s @mattppal at AI Dev 25 on debugging while vibe coding: Large part of it is looking at outputs to figure out what context you have that LLM does not, so that you can give it that context and help it get unstuck. Sometimes pasting in the error messages is enough, but also sometimes not.",
    "createdAt": "Fri Mar 14 18:37:38 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 77,
    "replyCount": 38,
    "likeCount": 468,
    "quoteCount": 15,
    "viewCount": 105433,
    "bookmarkCount": 200,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "在 AI Dev 25 大会上，Replit 公司的 @mattppal 分享了一个在“随性编码”或“凭感觉编码” (vibe coding) 时进行调试的实用技巧：很大程度上，调试过程就是要查看程序的输出，从而发现你了解而大语言模型 (LLM) 却不具备的上下文信息。这样一来，你就能将这些关键上下文提供给 LLM，帮助它解决遇到的难题。有时，简单地粘贴错误消息就足以解决问题，但有时这还远远不够。"
  },
  {
    "id": "1900610468747899142",
    "url": "https://x.com/AndrewYNg/status/1900610468747899142",
    "text": "Panel with Replit’s Michele Catasta, Stanford’s @percyliang , Nebius’ Roman Chernin and Hugging Face’s @Thom_Wolf, moderated by @lmoroney, on application building. Lots of tips on infra, open source, agentic workflows, benchmarking and code gen. Particular interest in how to take stochastic LLMs that hallucinate and nonetheless build reliable agents.",
    "createdAt": "Fri Mar 14 18:10:22 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 8,
    "replyCount": 10,
    "likeCount": 58,
    "quoteCount": 0,
    "viewCount": 33935,
    "bookmarkCount": 11,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "@lmoroney 主持了一场关于应用构建的小组讨论。参与者包括 Replit 的 Michele Catasta、斯坦福大学的 @percyliang 、Nebius 的 Roman Chernin 以及 Hugging Face 的 @Thom_Wolf。讨论分享了许多关于基础设施、开源、AI 智能体 (AI Agent) 工作流、基准测试 (benchmarking) 和代码生成 (code generation) 的实用技巧。其中一个特别引人关注的议题是，如何利用那些容易出现“幻觉”（即输出不准确信息）的、具备不确定性 (stochastic) 的大语言模型 (Large Language Model)，去构建可靠的 AI 智能体。"
  },
  {
    "id": "1900599467822510154",
    "url": "https://x.com/AndrewYNg/status/1900599467822510154",
    "text": "Meta’s Chaya Nayak talking about the open Llama models and Llama Stack, and best practices for using them. Great tips and I saw lots of people pulling out phones to take pictures of her slides! https://t.co/OGixvLRyPO",
    "createdAt": "Fri Mar 14 17:26:39 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 18,
    "replyCount": 6,
    "likeCount": 66,
    "quoteCount": 0,
    "viewCount": 18052,
    "bookmarkCount": 10,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "Meta 的 Chaya Nayak 介绍了开放的 Llama 模型和 Llama Stack，并分享了如何有效利用它们的最佳实践。这些建议非常棒，我看到许多人都纷纷拿出手机拍摄她的演示幻灯片！https://t.co/OGixvLRyPO"
  },
  {
    "id": "1900596396140671194",
    "url": "https://x.com/AndrewYNg/status/1900596396140671194",
    "text": "Great talk by Google’s Bill Jia on their GenAI work, including Astra and Deep Research agents (both of which I think are very cool). https://t.co/1yhNIQGxII",
    "createdAt": "Fri Mar 14 17:14:26 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 8,
    "replyCount": 12,
    "likeCount": 78,
    "quoteCount": 1,
    "viewCount": 32914,
    "bookmarkCount": 11,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "Google 的 Bill Jia 发表了一场精彩的演讲，分享了他们在生成式 AI (Generative AI) 方面的工作，其中包括 Astra 和 Deep Research AI 智能体 (AI Agent) （我个人认为这两者都非常酷）。 https://t.co/1yhNIQGxII"
  },
  {
    "id": "1900595885970780360",
    "url": "https://x.com/AndrewYNg/status/1900595885970780360",
    "text": "From audience poll the topic AI developers are most excited about is Agents! https://t.co/WqMCyR07H4",
    "createdAt": "Fri Mar 14 17:12:25 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 10,
    "replyCount": 9,
    "likeCount": 66,
    "quoteCount": 1,
    "viewCount": 23236,
    "bookmarkCount": 6,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "根据观众投票结果显示，AI 开发者最感兴趣的话题是 AI 智能体 (AI Agent)！ https://t.co/WqMCyR07H4"
  },
  {
    "id": "1900594063516254299",
    "url": "https://x.com/AndrewYNg/status/1900594063516254299",
    "text": "It’s starting - just kicked off AI Dev 25, the AI developer conference, in San Francisco! Happy Pi day! https://t.co/tlJvBFee0F",
    "createdAt": "Fri Mar 14 17:05:10 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 74,
    "replyCount": 120,
    "likeCount": 656,
    "quoteCount": 6,
    "viewCount": 61303,
    "bookmarkCount": 49,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "正式开始了！AI Dev 25，这场 AI (人工智能) 开发者大会，刚刚在旧金山拉开帷幕！祝大家圆周率日快乐！ https://t.co/tlJvBFee0F"
  },
  {
    "id": "1900219116822102116",
    "url": "https://x.com/AndrewYNg/status/1900219116822102116",
    "text": "Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!\n\nIn the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.\n\nAs coding becomes easier, more people should code, not fewer!\n\nOver the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step.\n\nI wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals — individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.\n\nOne question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is a great way to do that.\n\nWhen I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.\n\nSimilarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools are continuing to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.\n\n[Original text: https://t.co/HdI3Jb9HmF ]",
    "createdAt": "Thu Mar 13 16:15:16 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 2866,
    "replyCount": 524,
    "likeCount": 12179,
    "quoteCount": 514,
    "viewCount": 2121039,
    "bookmarkCount": 5550,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "如今，有些人以人工智能 (AI) 将自动化编程工作为由，劝阻他人学习编程。这条建议未来很可能会被认为是史上最糟糕的职业建议之一。我不同意某位图灵奖和诺贝尔奖得主所写的话：“编程职业与其变得无所不能，更有可能走向消亡 [...]。计算机将越来越多地自行编程。”劝阻人们学习编程的言论是十分有害的！\n\n在 20 世纪 60 年代，编程从穿孔卡片 (程序员必须费力地在物理卡片上逐字符打孔来编写代码) 转向了带终端的键盘，编程变得更加容易。这使得那段时间比以往任何时候都更适合开始编程。然而，正是在这个时代，诺贝尔奖得主赫伯·西蒙写下了第一段引用的那些话。如今，那些劝退人们学习编程的论点依然在呼应他的评论。\n\n随着编程变得越来越容易，应该有更多的人去学习编程，而不是更少！\n\n在过去的几十年里，编程从汇编语言 (assembly language) 转向了像 C 这样的高级语言，从桌面环境转向了云端，从原始文本编辑器发展到集成开发环境 (IDE)，再到人工智能 (AI) 辅助编程——有时人们甚至几乎不用看生成的代码 (一些程序员最近开始称之为“vibe coding”)，每一步都让编程变得更加简单。\n\n我之前曾写道，我看到那些精通技术的人正通过协调 AI 工具，逐步成为“10 倍专业人士”——即在各自领域中，能发挥出相当于普通人 10 倍影响力的个体。我越来越坚信，对许多人来说，实现这一目标的最佳途径不是仅仅作为 AI 应用程序的消费者，而是要学习足够的编程知识，以便能有效地使用 AI 辅助编程工具。\n\n我最常被问到的一个问题是，那些担心 AI 导致失业的人应该怎么办。我的答案是：了解 AI 并掌控它，因为未来最重要的技能之一将是能够准确地告诉计算机你想要什么，这样它就能为你完成。编程 (或者让 AI 为你编程) 是实现这一目标的绝佳方式。\n\n当我正在制作“人人可用的生成式 AI (Generative AI for Everyone)”课程并需要为背景图像生成 AI 艺术品时，我与一位研究过艺术史并了解艺术语言的合作者一起工作。他使用基于历史风格、调色板、艺术家灵感等术语——运用艺术语言——向 Midjourney 进行提示，以获得他想要的结果。我不懂这种语言，我那些微不足道的提示尝试无法产生如此有效的结果。\n\n同样，科学家、分析师、营销人员、招聘人员以及各行各业的专业人士，通过他们的编程知识理解软件语言，可以更精确地告诉大语言模型 (LLM) 或支持 AI 的集成开发环境 (IDE) 他们想要什么，并获得更好的结果。由于这些工具正在持续使编程变得更容易，现在是学习编程、掌握软件语言、并学会让计算机完全按照你的意愿行事的最佳时机。\n\n[原文链接: https://t.co/HdI3Jb9HmF ]"
  },
  {
    "id": "1897776017873465635",
    "url": "https://x.com/AndrewYNg/status/1897776017873465635",
    "text": "Continuing from last week’s post on the rise of the Voice Stack, there’s an area that today’s voice-based systems often struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.\n\nWhen communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm.\n\nA key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments.\n\nHowever, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments.\n\nIntriguingly, last year, Kyutai Labs published Moshi, a model that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user.\n\nIf you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.)\n\nJust as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year.\n\nIt feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in.\n\n[Original text: https://t.co/vwLlAnTJZT ]",
    "createdAt": "Thu Mar 06 22:27:16 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 85,
    "replyCount": 38,
    "likeCount": 385,
    "quoteCount": 9,
    "viewCount": 68182,
    "bookmarkCount": 155,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "接续上周关于语音技术栈 (Voice Stack) 兴起的文章，今天的语音系统在某些方面仍面临挑战，其中之一便是语音活动检测 (Voice Activity Detection, VAD) 技术以及对话中“轮流”沟通的模式。\n\n当我们与文本聊天机器人交流时，对话的轮次非常明确：你先输入内容，然后机器人回复，接着你再发言，如此循环。这种清晰的轮流沟通模式在文本聊天机器人中取得了巨大成功，也因此影响了语音机器人的设计，目前大多数语音机器人也沿用了这种轮流对话的范式。\n\n要构建一个这样的系统，一个核心组件就是 VAD，它的作用是识别用户何时正在说话。通过 VAD，我们的软件能够从音频流中截取出用户说话的部分，并将其传递给模型进行处理，从而完成用户的这一轮对话。它还在一定程度上支持打断功能：如果用户在 AI 系统讲话时持续打断，VAD 系统最终会检测到用户正在说话，随即停止 AI 的输出，让用户接管对话。在安静的环境中，这套机制通常运作良好。\n\n然而，当前的 VAD 系统在嘈杂环境下，尤其是当背景噪音是其他人声时，就会显得力不从心。举个例子，如果你在一个喧闹的咖啡馆里与语音聊天机器人对话，VAD（它通常被训练来检测人声）就很难准确判断究竟是你还是旁边的人在说话。（相比之下，如果你在一辆嘈杂的汽车里，它的效果会好得多，因为背景噪音明显不是人声。）VAD 可能会误以为你在打断，而那其实只是背景中有人在讲话；或者它可能没有识别出你已经停止说话。这就是为什么当今许多语音应用在嘈杂环境中表现不佳的原因。\n\n有趣的是，去年 Kyutai Labs 发布了 Moshi 模型，其中包含了多项技术创新。一个重要的突破是它实现了从用户到 Moshi，以及从 Moshi 到用户的持续双向音频流。\n\n如果你我面对面或通过电话交谈，我们会不断地向对方传输音频（通过空气或电话系统），并利用社交线索来判断何时倾听，以及在有需要时如何礼貌地打断对方。因此，音频流本身不需要明确地设定轮流模式。Moshi 正是这样运作的。它一直在持续监听，并由模型自身来决定何时保持沉默、何时进行回应。这意味着不再需要独立的 VAD 步骤。（Moshi 还包含了其他创新，比如一个“内心独白”功能，它在生成音频的同时同步生成文本，以提高响应质量，并优化了音频编码技术。）\n\n正如纯文本 Transformer 的架构经历了多次演进（例如编码器-解码器模型、仅解码器模型，以及在最终输出前生成大量“推理 Token”的推理模型），语音模型目前也正处于广泛的架构探索阶段。鉴于具备语音输入和语音输出能力的基础模型 (Foundation Models) 的重要性，许多大型公司正在大力投资开发更优秀的语音模型。我确信今年我们会看到更多出色的语音模型问世。\n\n语音技术领域的潜在创新空间依然广阔。一些棘手的技术难题，比如我上周提到的延迟问题和 VAD 错误，仍有待攻克。随着这些解决方案的不断完善，语音到语音的交互模式将继续成为构建各类应用的一个极具前景的领域。\n\n[原文链接: https://t.co/vwLlAnTJZT ]"
  },
  {
    "id": "1897389514034688313",
    "url": "https://x.com/AndrewYNg/status/1897389514034688313",
    "text": "New short course: Event-Driven Agentic Document Workflows. Event-driven workflows are a key design pattern in which many pieces of work (LLM calls, tool use, etc.) can be carried out asynchronously and in parallel, and completion of specific steps generate events that trigger other work to begin. In this course, created in partnership with @llama_index and @seldo, VP of Developer Relations, you'll learn to apply this technique to document workflows.\n\nFilling out complex forms can be tedious, time-consuming, and error-prone. Agentic workflows can automate this. This course teaches you how to build an agentic document workflow using an event-driven architecture.\n\nYou'll design an event-driven agentic workflow that fills out a PDF form based on information from a source document. The agent will use RAG to retrieve relevant data from the source document, parse the form to identify the required fields, convert the blank spaces into questions, and send those questions to the RAG system to fill the form. You'll collaborate with the agent using a human-in-the-loop feedback approach through text and with your voice.\n\nIn detail, you’ll:\n- Understand the basic concepts of event-driven workflows.\n- Build a series of LlamaIndex’s workflows that increase in complexity from branching and looping logic to concurrent executions.\n- Set up the agent’s RAG capability by parsing the source document, loading the extracted information into a vector store, and building a query engine on top of the store.\n- Implement workflow steps that enable the agent to parse the form to be filled, turn the parsed information into simple questions, and use the questions to query the RAG pipeline.\n- Incorporate human-in-the-loop into the workflow and ask the agent to re-answer when necessary to produce more accurate form responses.\n- Add multimodal capability to the agent, allowing it to process spoken feedback.\n\nBy the end of this course, you will have built an event-driven agentic workflow that fills out a document and responds to human feedback to complete the form more accurately.\n\nPlease sign up here: https://t.co/GZSZ9lXMKC",
    "createdAt": "Wed Mar 05 20:51:26 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 244,
    "replyCount": 136,
    "likeCount": 1556,
    "quoteCount": 12,
    "viewCount": 150897,
    "bookmarkCount": 1345,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程上线：事件驱动的 AI 智能体文档工作流。事件驱动的工作流是一种关键设计模式，它允许许多任务（如大语言模型 (LLM) 调用、工具使用等）异步并行地执行。当特定步骤完成后，会生成事件来触发后续工作的启动。本课程是与 LlamaIndex 和开发者关系副总裁 Seldo 合作开发的，你将学习如何将这项技术应用到文档处理工作中。\n\n填写复杂的表格可能既繁琐又耗时，还容易出错。而 AI 智能体工作流能够将这一过程自动化。本课程将教你如何利用事件驱动架构来构建一个 AI 智能体文档工作流。\n\n你将设计一个事件驱动的 AI 智能体工作流，根据源文档中的信息来填写 PDF 表格。这个智能体将运用 RAG 技术从源文档中检索相关数据，解析表格以识别必填字段，然后将空白区域转化为具体问题，并将这些问题发送给 RAG 系统以完成表格填写。你还将通过文本和语音，采用人机协作 (human-in-the-loop) 的反馈方式与智能体共同完成任务。\n\n具体来说，你将：\n- 理解事件驱动工作流的基本概念。\n- 构建一系列基于 LlamaIndex 的工作流，其复杂程度将从分支和循环逻辑逐步提升至并发执行。\n- 搭建智能体的 RAG 能力：包括解析源文档、将提取的信息加载到向量存储中，并在此存储之上构建一个查询引擎。\n- 实现工作流步骤，使智能体能够解析待填写的表格，将解析出的信息转化为简单问题，并利用这些问题查询 RAG 管道。\n- 将人机协作整合到工作流中，并在必要时要求智能体重新回答，从而生成更准确的表格响应。\n- 为智能体添加多模态能力，使其能够处理口头反馈。\n\n在本课程结束时，你将成功构建一个事件驱动的 AI 智能体工作流。它不仅能填写文档，还能响应人类反馈，从而更准确地完成表格填写。\n\n请在此处注册：https://t.co/GZSZ9lXMKC"
  },
  {
    "id": "1895183929977843970",
    "url": "https://x.com/AndrewYNg/status/1895183929977843970",
    "text": "Announcing: Agentic Document Extraction! \n\nPDF files represent information visually - via layout, charts, graphs, etc. - and are more than just text. Unlike  traditional OCR and most PDF-to-text approaches, which focus on extracting the text, an agentic approach lets us break a document down into components and reason about them, resulting in more accurate extraction of the underlying meaning for RAG and other applications. Watch the video for details.",
    "createdAt": "Thu Feb 27 18:47:14 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 894,
    "replyCount": 273,
    "likeCount": 6301,
    "quoteCount": 89,
    "viewCount": 683093,
    "bookmarkCount": 7114,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "发布：智能体文档提取 (Agentic Document Extraction)！\n\nPDF 文件通过布局、图表、图形等方式以视觉形式呈现信息，它们不只是简单的文本。与传统的 OCR (光学字符识别) 和大多数 PDF 转文本方法不同，这些方法通常只侧重于提取文本，而智能体 (Agentic) 方法能够将文档分解成更小的组件，并对这些组件进行推理，从而为检索增强生成 (RAG) 和其他应用更准确地理解文档的深层含义。观看视频了解详情。"
  },
  {
    "id": "1895146310296379419",
    "url": "https://x.com/AndrewYNg/status/1895146310296379419",
    "text": "The Voice Stack is improving rapidly. Systems that interact with users via speaking and listening will drive many new applications. Over the past year, I’ve been working closely with https://t.co/zpIxRSuky4, AI Fund, and several collaborators on voice-based applications, and I will share best practices I’ve learned in this and future posts.\n\nFoundation models that are trained to directly input, and often also directly generate, audio have contributed to this growth, but they are only part of the story. OpenAI’s RealTime API makes it easy for developers to write prompts to develop systems that deliver voice-in, voice-out experiences. This is great for building quick-and-dirty prototypes, and it also works well for low-stakes conversations where making an occasional mistake is okay. I encourage you to try it!\n\nHowever, compared to text-based generation, it is still hard to control the output of voice-in voice-out models. In contrast to directly generating audio, when we use an LLM to generate text, we have many tools for building guardrails, and we can double-check the output before showing it to users. We can also use sophisticated agentic reasoning workflows to compute high-quality outputs. Before a customer-service agent shows a user the message, “Sure, I’m happy to issue a refund,” we can make sure that (i) issuing the refund is consistent with our business policy and (ii) we will call the API to issue the refund (and not just promise a refund without issuing it).\n\nIn contrast, the tools to prevent a voice-in, voice-out model from making such mistakes are much less mature.\n\nIn my experience, the reasoning capability of voice models also seems inferior to text-based models, and they give less sophisticated answers. (Perhaps this is because voice responses have to be more brief, leaving less room for chain-of-thought reasoning to get to a more thoughtful answer.)\n\nWhen building applications where I need a high degree of control over the output, I use agentic workflows to reason at length about the user’s input. In voice applications, this means I end up using a pipeline that includes speech-to-text (STT, also known as ASR, or automatic speech recognition) to transcribe the user’s words, then processes the text using one or more LLM calls, and finally returns an audio response to the user via TTS (text-to-speech). This STT → LLM/Agentic workflow → TTS pipeline, where the reasoning is done in text, allows for more accurate responses.\n\nHowever, this process introduces latency, and users of voice applications are very sensitive to latency. When https://t.co/zpIxRSuky4 worked with RealAvatar (an AI Fund portfolio company led by Jeff Daniel) to build an avatar of me, we found that getting TTS to generate a voice that sounded like me was not very hard, but getting it to respond to questions using words similar to those I would choose was. Even after a year of tuning our system — starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows — it remains a work in progress. You can play with it at https://t.co/vMO2CM0xfb\n\nInitially, this agentic workflow incurred 5-9 seconds of latency, and having users wait that long for responses led to a bad experience. To address this, we came up with the following latency reduction technique. The system quickly generates a pre-response (short for preliminary response) that can be uttered quickly, which buys time for an agentic workflow to generate a more thoughtful, full response. (We’re grateful to LiveKit’s CEO Russ d’Sa and team for helping us get this working.) This is similar to how, if you were to ask me a complicated question, I might say “Hmm, let me think about that” or “Sure, I can help with that” — that’s the pre-response — while thinking about what my full response might be.\n\nI think generating a pre-response followed by a full response, to quickly acknowledge the user’s query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and https://t.co/zpIxRSuky4, through our efforts on the pre-response and other optimizations, have reduced the system’s latency to around 0.5-1 seconds.\n\nMonths ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT → LLM → TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize!\n\nBuilding reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you’ll start building prototypes and see how far you can get! I’ll keep building voice applications and sharing best practices and voice-related technology trends in future posts.\n\n[Original letter: https://t.co/M38Ud0UhdT ]",
    "createdAt": "Thu Feb 27 16:17:45 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 142,
    "replyCount": 132,
    "likeCount": 661,
    "quoteCount": 23,
    "viewCount": 76927,
    "bookmarkCount": 346,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "语音技术栈 (Voice Stack) 正在快速发展。那些通过语音交互与用户沟通的系统，将催生出许多全新的应用。在过去一年里，我一直与 https://t.co/zpIxRSuky4、AI Fund 以及多位合作伙伴紧密协作，致力于开发基于语音的应用。我将在本文及后续文章中，分享我所学到的最佳实践。\n\n经过训练可以直接接收并通常也直接生成音频的基础模型 (Foundation models)，对这一发展起到了推动作用，但这只是其中的一部分。OpenAI 的 RealTime API 让开发者能够轻松编写提示 (prompts)，从而开发出支持语音输入、语音输出 (voice-in, voice-out) 体验的系统。这对于快速搭建原型非常有用，也适用于那些偶尔出错也无关紧要的低风险对话。我鼓励大家亲自体验一下！\n\n然而，与基于文本的生成相比，目前仍然很难精确控制语音到语音模型的输出。与直接生成音频不同，当我们使用大语言模型 (LLM) 生成文本时，我们有许多工具来构建“护栏” (guardrails)，并且可以在向用户展示输出之前进行二次检查。我们还可以运用复杂的 AI 智能体 (agentic) 工作流来生成高质量的输出。例如，在客户服务 AI 智能体向用户发送“当然，我很乐意为您办理退款”这条消息之前，我们可以确保 (i) 办理退款符合我们的业务政策，并且 (ii) 我们会调用相应的 API 来实际执行退款操作 (而不仅仅是口头承诺)。\n\n相比之下，目前防止语音到语音模型犯这类错误的工具还远不够成熟。\n\n根据我的经验，语音模型的推理能力似乎也比基于文本的模型逊色，它们给出的答案往往不够精妙。(这可能是因为语音回复必须更简洁，留给思维链 (chain-of-thought) 推理的空间较少，从而难以得出更深思熟虑的答案。)\n\n在构建需要高度控制输出的应用时，我通常会采用 AI 智能体工作流，对用户的输入进行深入推理。在语音应用中，这意味着我最终会使用一个包含以下环节的管道：首先是语音转文本 (STT，也称为 ASR 或自动语音识别)，将用户的语音转录成文本；然后使用一个或多个 LLM 调用来处理这些文本；最后通过文本转语音 (TTS) 技术，将音频回复返回给用户。这种 STT → LLM/AI 智能体工作流 → TTS 的管道，将推理过程放在文本层面进行，可以实现更准确的回复。\n\n然而，这个过程会引入延迟，而语音应用的用户对延迟非常敏感。当 https://t.co/zpIxRSuky4 与 RealAvatar (一家由 Jeff Daniel 领导的 AI Fund 投资组合公司) 合作打造我的虚拟形象 (avatar) 时，我们发现让 TTS 生成听起来像我的声音并不太难，但要让它用我可能选择的词语来回应问题，却非常困难。即使经过一年的系统调优——我们从迭代多个冗长的大型提示 (mega-prompts) 开始，最终开发出复杂的 AI 智能体工作流——这项工作仍在进行中。您可以在 https://t.co/vMO2CM0xfb 体验它。\n\n最初，这个 AI 智能体工作流会产生 5-9 秒的延迟，让用户等待这么久才能得到回复，会严重影响用户体验。为了解决这个问题，我们提出了一种延迟降低技术：系统会快速生成一个预回应 (pre-response)，这是一种简短的初步回复，可以迅速发出，从而为 AI 智能体工作流争取时间，去生成一个更周到、更完整的回复。(我们非常感谢 LiveKit 的 CEO Russ d’Sa 及其团队帮助我们实现了这一点。) 这类似于，如果您问我一个复杂的问题，我可能会先说“嗯，让我考虑一下”或“当然，我能帮你”——这就是预回应——同时我在思考我的完整回复可能是什么。\n\n我认为，先生成一个预回应，再给出完整回复，能够迅速确认用户的查询并降低感知延迟，这将是一项重要的技术，希望很多团队都能从中受益。我们的目标是达到人面对面交谈的延迟水平，大约在 0.3-1 秒之间。通过在预回应和其他优化方面的努力，RealAvatar 和 https://t.co/zpIxRSuky4 已将系统的延迟降低到大约 0.5-1 秒。\n\n几个月前，我坐在一家咖啡店里，只用了几个小时就成功在 Twilio 上购买了一个电话号码，并将其连接到 STT → LLM → TTS 管道。这让我能够使用自定义提示与我自己的 LLM 对话。开发语音应用原型比大多数人想象的要容易得多！\n\n当然，构建可靠、可扩展的生产级应用需要更长时间，但如果您心中已经有一个语音应用的构想，我希望您能开始构建原型，看看您能走多远！我将继续构建语音应用，并在未来的文章中分享最佳实践和语音相关的技术趋势。\n\n[原始文章: https://t.co/M38Ud0UhdT ]"
  },
  {
    "id": "1894979731726180765",
    "url": "https://x.com/AndrewYNg/status/1894979731726180765",
    "text": "Transformers have dominated LLM text generation, and generate tokens sequentially. This is a cool attempt to explore diffusion models as an alternative, by generating the entire text at the same time using a coarse-to-fine process. Congrats @StefanoErmon &amp; team!",
    "createdAt": "Thu Feb 27 05:15:49 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 281,
    "replyCount": 94,
    "likeCount": 1858,
    "quoteCount": 13,
    "viewCount": 159066,
    "bookmarkCount": 537,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isQuote": true,
    "isPinned": false,
    "tranlastedContent": "在过去，Transformer 模型在 大语言模型 (LLM) 的文本生成领域一直占据主导地位，它们通常以顺序方式生成 Token。现在，有一项引人注目的尝试，探索将 扩散模型 (diffusion models) 作为一种替代方法，其创新之处在于利用一种 从粗到精 (coarse-to-fine) 的过程来同时生成整个文本。祝贺 @StefanoErmon 及团队！"
  },
  {
    "id": "1894796562829844873",
    "url": "https://x.com/AndrewYNg/status/1894796562829844873",
    "text": "New course to bring you up to state-of-the-art at using AI to help you code: Build Apps with Windsurf's AI Coding Agents, built in partnership with WIndsurf (@codeiumdev) and taught by @_anshulr!\n\nAI-assisted IDEs (Integrated Development Environments) make developers’ workflows faster, more efficient, and much more fun. Agentic tools like Windsurf are more than just code autocomplete—they are collaborative coding agents that help you break down complex applications, iterate efficiently, and generate code that spans multiple files.\n\nAlthough a lot of coding assistants share the same underlying large language models for planning and reasoning, a major point of distinction is how they handle tools, keep track of context, and stay aligned with your intent as a developer.\n\nFor instance, if you make modifications to a class definition in your code and make the same modifications to other classes in the same directory, you might tell the AI agent \"Do the same thing in similar places in this directory.\" Here, tracking your intent means understanding that “the same thing\" refers to that recent edit you just made, which must be followed by appropriate search and tool-calling to implement the changes.\n\nIn this course, you'll learn the inner workings of coding agents, their strengths and limitations, and how to use Windsurf to quickly build several applications.\n\nIn detail, you'll:\n- Build a mental model of how agents work by combining human-action tracking, tool integration, and context awareness to carry out an agentic coding workflow.\n- Learn the challenges of code search and discovery and how a multi-step retrieval approach helps coding agents address them.\n- Use Windsurf to analyze and understand a large, old codebase and update it to the latest versions of the frameworks and packages it uses.\n- Build a Wikipedia data analysis app that retrieves, parses, and analyzes word frequencies.\n- Enhance the performance of your Wikipedia analysis app by adding caching, and through this, also learn how to course-correct when the AI agent produces unexpected results.\n- Learn tips and tricks such as keyboard shortcuts, autocomplete, and @ mentions to quickly call on agentic capabilities.\n- Use image/multimodal capabilities of the AI agent to increase your development velocity; you'll see an example of uploading a mockup with sketched-out UI features, and ask the agent to use that to build new functionality to an app.\n\nBy the end of this course, you’ll understand agentic coding in-depth and know how to use it to make your development process much faster, more efficient, and enjoyable.\n\nPlease sign up here! https://t.co/IhFB3IwHAh",
    "createdAt": "Wed Feb 26 17:07:58 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 271,
    "replyCount": 143,
    "likeCount": 1565,
    "quoteCount": 24,
    "viewCount": 138836,
    "bookmarkCount": 1267,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "全新课程，助您掌握利用 AI 辅助编程的最新前沿技术：学习使用 Windsurf 的 AI 编码智能体构建应用程序！本课程由 Windsurf (@codeiumdev) 合作开发，并由 @_anshulr 亲自讲授。\n\nAI 辅助的集成开发环境 (IDEs - Integrated Development Environments) 能让开发者的工作流程更快速、更高效，也更有趣。像 Windsurf 这样的智能体 (Agentic) 工具不仅仅是简单的代码自动补全——它们是真正的协作式编程智能体，能帮助你拆解复杂的应用程序、高效迭代，并生成跨多个文件的代码。\n\n尽管许多编程助手在规划和推理时都依赖相同的底层大语言模型 (Large Language Model)，但它们之间的一个主要区别在于：它们如何处理工具、跟踪上下文，以及如何与你作为开发者的意图保持一致。\n\n举个例子，如果你在代码中修改了一个类定义，并想在同一目录的其他类中也进行相同的修改，你可能会告诉 AI 智能体 “在这个目录里类似的地方也做同样的事情。” 在这里，跟踪你的意图就意味着要理解“同样的事情”指的就是你刚刚做出的那个编辑。AI 智能体会据此进行适当的搜索和工具调用，从而帮你实现这些更改。\n\n在本课程中，你将深入学习编程智能体的工作原理、它们的优势与局限性，以及如何利用 Windsurf 快速构建多个应用程序。\n\n具体来说，你将：\n- 结合人工操作跟踪、工具集成和上下文感知，构建一个关于智能体 (Agent) 工作原理的心智模型，从而掌握智能体驱动的编程工作流程。\n- 了解代码搜索和发现的挑战，以及多步检索方法如何帮助编程智能体有效应对这些挑战。\n- 使用 Windsurf 分析并理解一个庞大且陈旧的代码库，并将其更新到所用框架和软件包的最新版本。\n- 构建一个维基百科数据分析应用程序，用于检索、解析和分析词频。\n- 通过添加缓存 (Caching) 来提升维基百科分析应用程序的性能；在此过程中，你还将学习当 AI 智能体产生意外结果时如何进行修正。\n- 学习各种实用技巧，如键盘快捷键、自动补全以及 @ 提及 (Mentions) 功能，以便快速调用智能体的强大能力。\n- 利用 AI 智能体 (AI Agent) 的图像/多模态 (Multimodal) 能力来加速开发进程；你将看到一个示例，演示如何上传带有手绘 UI 功能的原型图，并要求智能体利用它来为应用程序添加新功能。\n\n通过本课程的学习，你将深入理解智能体驱动的编程 (Agentic Coding)，并掌握如何运用它来让你的开发过程变得更快、更高效、更令人愉快。\n\n请点击这里报名！https://t.co/IhFB3IwHAh"
  },
  {
    "id": "1892628887856939236",
    "url": "https://x.com/AndrewYNg/status/1892628887856939236",
    "text": "Last month, a drone from Skyfire AI was credited with saving a police officer’s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened.\n\nSkyfire AI, an AI Fund portfolio company led by CEO Don Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers’ time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond.\n\nIn January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located.\n\nFrom the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road — they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf’s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find  them.\n\nFrom the aerial footage, it appeared that the officer still had his radio, but  was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation.\n\nFortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes.\n\nThe officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we’d probably saved the officer’s life.\n\nDemocratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we’re making. In the aftermath of Hurricane Helene last year, Skyfire AI’s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives.\n\nIt’s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these.\n\n[Original text: https://t.co/xtBdJgqIqc ]",
    "createdAt": "Thu Feb 20 17:34:24 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 107,
    "replyCount": 167,
    "likeCount": 591,
    "quoteCount": 10,
    "viewCount": 64457,
    "bookmarkCount": 122,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "上个月，Skyfire AI 的一架无人机在一次惊心动魄的凌晨两点交通拦截中，成功挽救了一名警官的生命。许多统计数据都表明，AI (人工智能) 正影响着数十亿人的生活，但有时一些真实的故事依然会深深触动我的内心。下面我就来分享一下这个事件的始末。\n\nSkyfire AI 是一家由 CEO Don Mathis 领导的 AI Fund 投资组合公司，其运营着一项公共安全计划，让无人机作为 911 紧急呼叫的第一响应者。尤其是在警力受限的部门，无人机不仅能为警官节省宝贵时间，还能显著增强他们的态势感知 (Situational Awareness) 能力。举例来说，许多防盗警报都属于误报，可能只是由潮湿或小动物触发的。这时，与其派巡逻警官开车前往现场确认，无人机能更快抵达，并判断是否真的需要警官介入。如果警报属实，无人机还能帮助警官了解现场情况、嫌疑人的位置以及最佳的应对策略。\n\n今年一月，一架 Skyfire AI 无人机在响应了一次误报后正准备返航基地，这时警方调度员请求我们改变航线，协助定位一名巡逻警官。这名警官在几分钟前曾通过无线电报告，他拦下了一辆可疑车辆，但此后便失去了联系。警官停车的位置是两条主要高速公路在一个复杂的苜蓿叶形立交桥处交汇，调度中心不确定他们的确切位置。\n\n从空中俯瞰，无人机迅速锁定了警官以及他所拦截车辆的司机。原来，这名司机是从当地一所看守所越狱的逃犯。从地面上看，两人都无法被发现——他们正在高速公路下方的一个排水沟里激烈搏斗。由于苜蓿叶形立交桥的复杂几何结构，值班警官 (负责协调当班警务活动) 后来估计，如果由警车内的警官驾车寻找，至少需要 5 到 7 分钟才能找到他们。\n\n从无人机传回的空中画面显示，警官的无线电虽然仍在身边，但他在这场搏斗中正处于劣势，根本无暇顾及去呼叫支援。更糟糕的是，看起来袭击者似乎有可能夺走他的配枪，并用来攻击警官。这无疑是一个极其危急和危险的局面。\n\n幸运的是，由于无人机精确锁定了警官和袭击者的位置，调度中心能够立即指示增援警力前往协助。第一批支援单位不是在 5 到 7 分钟后到达，而是在仅仅 45 秒内便赶到现场。随后的四辆警车也在几分钟内陆续抵达。\n\n在增援警力的帮助下，警官们成功控制了局面并逮捕了司机。最终，这名逃犯被捕，更重要的是，警官本人安然无恙。事后，值班警官表示，我们很可能挽救了这名警官的生命。\n\n民主国家在无人机技术方面仍有许多工作要做，我们必须在开发这项技术时，同步建立完善的保障机制，以确保公民自由和人权得到充分维护。但我对我们目前所取得的进展感到非常鼓舞。去年飓风 Helene 席卷之后，Skyfire AI 的无人机就在北卡罗来纳州应急管理办公室的指挥下，参与了搜救行动。它们响应具体请求，帮助定位失踪人员，并引导救援资源 (例如直升机和船只) 前往指定位置，最终成功挽救了 13 条生命。\n\nAI 并非每天都能直接挽救生命。但随着我们技术的不断进步，我相信这样的故事将会越来越多。\n\n[原文链接: https://t.co/xtBdJgqIqc ]"
  },
  {
    "id": "1892258190546653392",
    "url": "https://x.com/AndrewYNg/status/1892258190546653392",
    "text": "New short course: Evaluating AI Agents! Evals are important for driving AI system improvements, and in this course you'll learn to systematically assess and improve an AI agent’s performance. This is built in partnership with @arizeai and taught by @JohnGilhuly, Head of Developer Relations, and @_amankhan, Director of Product.\n\nI've often found evals to be a critical tool in the agent development process - they can be the difference between picking the right thing to work on vs. wasting weeks of effort. Whether you’re building a shopping assistant, coding agent, or research assistant, having a structured evaluation process helps you refine its performance systematically, rather than relying on random trial and error. \n\nThis course shows you how to structure your evals to assess the performance of each component of an agent and its end-to-end performance. For each component, you select the appropriate evaluators, test examples, and performance metrics. This helps you identify areas for improvement both during development and in production. (If you're familiar with error analysis in supervised learning, think of this as adapting those ideas to agentic workflows.) \n\nIn this course, you'll build an AI agent, and add observability to visualize and debug its steps. You’ll learn about code-based evals, in which you write code explicitly to test a certain step, as well as LLM-as-a-Judge evals, in which you prompt an LLM to efficiently come up with ways to evaluate more open-ended outputs.\n\nIn detail, you’ll:\n- Understand key differences between evaluating LLM-based systems and traditional software testing.\n- Add observability to an agent by collecting traces of the steps taken by the agent and visualizing them\n- Choose the appropriate evaluator - code-based, LLM-as-a-Judge, human-annotation based - for each component.\n- Compute a convergence score to evaluate if your agent can respond to a query in an efficient number of steps. \n- Run structured experiments to improve the agent’s performance by exploring changes to the prompt, LLM model, or the agent’s logic.\n- Understand how to deploy these evaluation techniques to monitor the agent’s performance in production.\n\nBy the end of this course, you’ll know how to trace AI agents, systematically evaluate them, and improve their performance.\n\nPlease sign up here: https://t.co/hTNCM8xuYn",
    "createdAt": "Wed Feb 19 17:01:23 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 241,
    "replyCount": 137,
    "likeCount": 1352,
    "quoteCount": 23,
    "viewCount": 124425,
    "bookmarkCount": 1064,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新课程上线：评估 AI 智能体 (AI Agents)！评估 (Evaluation) 对于推动人工智能系统进步至关重要，在本课程中，您将学习如何系统地评估和提升 AI 智能体的性能。本课程由 @arizeai 合作开发，并由开发者关系负责人 @JohnGilhuly 和产品总监 @_amankhan 共同授课。\n\n我常常发现评估是智能体开发过程中的一项关键工具——它可能决定您的团队是选对了工作方向，还是白白浪费数周精力。无论您正在构建购物助手、编程智能体，还是研究助手，拥有一个结构化的评估流程都能帮助您系统地优化其性能，而不是仅仅依靠盲目的试错。\n\n本课程将指导您如何构建评估体系，以便同时评估智能体的各个组件和其端到端 (end-to-end) 的整体性能。对于每个组件，您将学会选择合适的评估器、测试用例和性能指标。这能帮助您在开发阶段和实际生产环境中精准定位改进点。(如果您熟悉监督学习中的错误分析，可以把这门课看作是将那些思路巧妙地应用到智能体工作流程中。)\n\n在这门课程中，您将亲手搭建一个 AI 智能体，并为其添加可观测性 (observability) 功能，从而实现其运行步骤的可视化和调试。您将了解基于代码的评估方法——即通过编写代码来明确测试某个步骤；以及 大语言模型 即法官 (LLM-as-a-Judge) 评估方法——即通过向 大语言模型 提问，来高效地评估那些更开放式的输出结果。\n\n具体来说，您将：\n- 理解评估基于 大语言模型 的系统与传统软件测试之间的主要区别。\n- 通过收集并可视化智能体执行步骤的轨迹，为智能体添加可观测性。\n- 为每个组件选择最合适的评估器——包括基于代码的、 大语言模型 即法官，以及基于人工标注的方法。\n- 计算收敛分数 (convergence score)，以评估您的智能体能否以高效的步数响应查询。\n- 运行结构化实验，通过调整提示 (prompt)、更换 大语言模型 模型或修改智能体逻辑，来提升智能体的性能。\n- 掌握如何在生产环境中应用这些评估技术，持续监控智能体的表现。\n\n学完本课程，您将能够追踪 AI 智能体的运行过程，系统地评估它们，并有效提升它们的性能。\n\n请点击此处报名：https://t.co/hTNCM8xuYn"
  },
  {
    "id": "1891885809722327138",
    "url": "https://x.com/AndrewYNg/status/1891885809722327138",
    "text": "Credit also goes to Matthew Carrigan for the neat idea of getting function descriptions from docstrings:  https://t.co/CPLL1KxHE4",
    "createdAt": "Tue Feb 18 16:21:41 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 14,
    "replyCount": 27,
    "likeCount": 85,
    "quoteCount": 2,
    "viewCount": 35821,
    "bookmarkCount": 27,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "此外，这项巧妙的设想还要归功于 Matthew Carrigan，他提出了从 docstrings (docstrings) 中获取函数描述的独到想法： https://t.co/CPLL1KxHE4"
  },
  {
    "id": "1891885332058210787",
    "url": "https://x.com/AndrewYNg/status/1891885332058210787",
    "text": "Announcing new aisuite capability: Easy function calling with LLMs! Function calling (tool use) is an important capability for agentic workflows and other LLM applications, but is cumbersome for developers to use (left column in image). Our open-source aisuite package simplifies it to just one command (right column), and works for multiple LLM providers.\n\nHope this makes implementing agents easier for developers, and thanks Rohit Prsad & team for working with me on this! \n\nhttps://t.co/gwz9oKTCFx",
    "createdAt": "Tue Feb 18 16:19:47 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 244,
    "replyCount": 94,
    "likeCount": 1419,
    "quoteCount": 19,
    "viewCount": 111097,
    "bookmarkCount": 1006,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "隆重推出 aisuite 的新功能：让大语言模型 (LLM) 的函数调用 (Function calling) 变得轻而易举！函数调用（也称为工具使用 (Tool use)）是实现 AI 智能体 (AI Agent) 工作流和其他大语言模型应用的关键能力。然而，对于开发者来说，现有的函数调用过程往往十分繁琐（如图片左栏所示）。为了解决这一痛点，我们的开源 (open-source) aisuite 软件包将其操作简化为只需一个命令（如图片右栏所示），而且它能兼容多个大语言模型提供商。\n\n我们希望这一改进能让开发者更轻松地实现各种 AI 智能体。同时，非常感谢 Rohit Prsad 和他的团队与我共同完成了这项工作！\n\nhttps://t.co/gwz9oKTCFx"
  },
  {
    "id": "1890858116574839241",
    "url": "https://x.com/AndrewYNg/status/1890858116574839241",
    "text": "Among people in non-technical roles (recruiter, marketer, sales, ...) I notice the more technical ones being more effective, and the gap is increasing. E.g., the ones that took a coding course are outperforming the ones that didn't. Has anyone else noticed this? \n\nOne obvious theory is that they are better at using AI, but would love to hear if you have other theories.",
    "createdAt": "Sat Feb 15 20:18:00 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 173,
    "replyCount": 212,
    "likeCount": 1618,
    "quoteCount": 41,
    "viewCount": 262527,
    "bookmarkCount": 385,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我注意到，在非技术岗位的人员中 ( 例如招聘人员、市场人员、销售人员等 )，那些技术能力更强的人工作效率更高，而且这种差距还在不断扩大。举个例子，上过编程课程的人比没上过编程课的人表现得更出色。大家有没有注意到这个现象？\n\n一个显而易见的解释是，他们更擅长使用 AI，但我很想听听大家是否有其他看法。"
  },
  {
    "id": "1890452971747479715",
    "url": "https://x.com/AndrewYNg/status/1890452971747479715",
    "text": "To all my AI friends: You must be a good prompt, because whenever we chat, you complete me. \n\nHappy Valentine's Day! ❤️",
    "createdAt": "Fri Feb 14 17:28:06 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 135,
    "replyCount": 176,
    "likeCount": 1804,
    "quoteCount": 20,
    "viewCount": 104753,
    "bookmarkCount": 69,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "送给我所有的 AI 朋友：你肯定是个优秀的提示词 (prompt)，因为每当我们聊天时，你都让我变得更完整。\n\n情人节快乐！❤️"
  },
  {
    "id": "1890076882391167317",
    "url": "https://x.com/AndrewYNg/status/1890076882391167317",
    "text": "At the Artificial Intelligence Action Summit in Paris this week, U.S. Vice President J.D. Vance said, “I’m not here to talk about AI safety.... I’m here to talk about AI opportunity.” I’m thrilled to see the U.S. government focus on opportunities in AI. Further, while it is important to use AI responsibly and try to stamp out harmful applications, I feel “AI safety” is not the right terminology for addressing this important problem. Language shapes thought, so using the right words is important. I’d rather talk about “responsible AI” than “AI safety.” Let me explain.\n\nFirst, there are clearly harmful applications of AI, such as non-consensual deepfake porn (which creates sexually explicit images of real people without their consent), the use of AI in misinformation, potentially unsafe medical diagnoses, addictive applications, and so on. We definitely want to stamp these out! There are many ways to apply AI in harmful or irresponsible ways, and we should discourage and prevent such uses.\n\nHowever, the concept of “AI safety” tries to make AI — as a technology — safe, rather than making safe applications of it. Consider the similar, obviously flawed notion of “laptop safety.” There are great ways to use a laptop and many irresponsible ways, but I don’t consider laptops to be intrinsically either safe or unsafe. It is the application, or usage, that determines if a laptop is safe. Similarly, AI, a general-purpose technology with numerous applications, is neither safe nor unsafe. How someone chooses to use it determines whether it is harmful or beneficial.\n\nNow, safety isn’t always a function only of how something is used. An unsafe airplane is one that, even in the hands of an attentive and skilled pilot, has a large chance of mishap. So we definitely should strive to build safe airplanes (and make sure they are operated responsibly)! The risk factors are associated with the construction of the aircraft rather than merely its application. Similarly, we want safe automobiles, blenders, dialysis machines, food, buildings, power plants, and much more.\n\n“AI safety” presupposes that AI, the underlying technology, can be unsafe. I find it more useful to think about how applications of AI can be unsafe.\n\nFurther, the term “responsible AI” emphasizes that it is our responsibility to avoid building applications that are unsafe or harmful and to discourage people from using even beneficial products in harmful ways.\n\nIf we shift the terminology for AI risks from “AI safety” to “responsible AI,” we can have more thoughtful conversations about what to do and what not to do.\n\nI believe the 2023 Bletchley AI Safety Summit slowed down European AI development — without making anyone safer — by wasting time considering science-fiction AI fears rather than focusing on opportunities. Last month, at Davos, business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting, unlike the one at Bletchley, will result in acceleration rather than deceleration.\n\nIn a world where AI is becoming pervasive, if we can shift the conversation away from “AI safety” toward responsible [use of] AI, we will speed up AI’s benefits and do a better job of addressing actual problems. That will actually make people safer.\n\n[Original text: https://t.co/uvjfNwXq4c ]",
    "createdAt": "Thu Feb 13 16:33:39 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 168,
    "replyCount": 113,
    "likeCount": 703,
    "quoteCount": 32,
    "viewCount": 75275,
    "bookmarkCount": 128,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "本周在巴黎举行的人工智能行动峰会上，美国副总统 J.D. Vance 表示：“我不是来谈论 AI 安全的……我来这里是谈论人工智能带来的机会的。” 我很高兴看到美国政府关注 AI (Artificial Intelligence) 带来的机会。此外，虽然负责任地使用 AI 并努力杜绝有害应用很重要，但我认为“AI 安全”不是解决这个重要问题的正确术语。语言会影响我们的思维方式，因此使用正确的词语至关重要。我宁愿谈论“负责任的 AI”而不是“AI 安全”。请允许我解释一下。\n\n首先，AI 显然存在有害应用，例如未经同意的深度伪造色情内容 (deepfake porn)（即未经真实人物同意，利用 AI 技术合成的性露骨图像）、AI 在虚假信息中的使用、可能不安全的医疗诊断、成瘾性应用等等。我们肯定希望杜绝这类问题！AI 有许多可能被用于有害或不负责任的方式，我们应该阻止并预防此类使用。\n\n然而，“AI 安全”的概念试图让 AI —— 作为一种技术 —— 本身变得安全，而不是使其应用变得安全。我们可以类比一下“笔记本电脑安全”这个显然有缺陷的说法。笔记本电脑有许多优良的用途，也有许多不负责任的用法，但我认为笔记本电脑本身既非本质安全也非本质不安全。是它的应用或使用方式决定了笔记本电脑是否安全。同样，AI 是一种具有众多应用的通用技术，它本身既不安全也不不安全。人们选择如何使用它，决定了它是有害还是有益。\n\n当然，安全并不总是仅仅取决于一项事物如何被使用。一架不安全的飞机，即使在细心熟练的飞行员手中，也有很大概率发生事故。所以我们绝对应该努力建造安全的飞机（并确保它们被负责任地操作）！这里的风险因素与飞机的建造有关，而不仅仅是它的应用方式。同样，我们希望汽车、搅拌机、透析机、食物、建筑物、发电厂等都能是安全的。\n\n“AI 安全”这个词预设了 AI，即其底层技术，可能是不安全的。我认为更重要的是思考 AI 的应用如何可能是不安全的。\n\n此外，“负责任的 AI”这一术语强调，我们有责任避免构建不安全或有害的应用，并劝阻人们即使在使用有益产品时，也要避免以有害的方式进行。\n\n如果我们将 AI 风险的术语从“AI 安全”转向“负责任的 AI”，我们就可以就应该做什么和不应该做什么进行更深入的讨论。\n\n我相信 2023 年布莱切利 AI 安全峰会通过浪费时间担忧科幻小说中的 AI 威胁，而非关注 AI 带来的机遇，减缓了欧洲 AI 的发展，并且没有让任何人变得更安全。上个月在达沃斯，商界和政策领导者也对欧洲能否摆脱当前的监管困境并专注于 AI 建设表达了强烈担忧。我希望巴黎会议与布莱切利会议不同，能推动其发展而非阻碍。\n\n在一个 AI 日益普及的世界中，如果我们将对话从“AI 安全”转向负责任地使用 AI，我们将加速 AI 带来益处，并更好地解决实际问题。这才会真正让人们更安全。\n\n[原文链接：https://t.co/uvjfNwXq4c]"
  },
  {
    "id": "1889766176059994166",
    "url": "https://x.com/AndrewYNg/status/1889766176059994166",
    "text": "New short course: Attention in Transformers: Concepts and Code in PyTorch.\n\nLast week we released a course on how LLM transformers work. This week, go deeper and learn about the technical ideas behind the attention mechanism, and see how to code it in PyTorch. This course is built with @joshuastarmer, Founder and CEO of StatQuest.\n\nThe attention mechanism was a breakthrough that led to transformers, the architecture powering large language models like ChatGPT. Transformers, introduced in the 2017 paper: \"Attention is All You Need\" by Viswani and others, took off because of its highly scalable design. \n\nIn this course, you’ll learn how the attention mechanism, a key element of transformer-based LLMs, works and implement it in PyTorch. You'll develop deep intuition about building reliable, functional, and scalable AI applications.\n\nWhat you will do:\n- Understand the evolution of the attention mechanism, a key breakthrough that led to transformers.\n- Learn the relationships between word embeddings, positional embeddings, and attention.\n- Learn about the Query, Key, and Value matrices, and how to produce and use them in attention.\n- Walk through the math required to calculate self-attention and masked self-attention to learn why and how they work.\n- Understand the difference between self-attention and masked self-attention and how one is used in the encoder to build context-aware embeddings and the other is used in the decoder for generative outputs.\n- Learn the details of the encoder-decoder architecture, cross-attention, and multi-head attention and how they are all incorporated into a transformer.\n- Use PyTorch to code a class that implements self-attention, masked self-attention, and multi-head attention.\n\nThere're lots of exciting technical details in this course.  Please sign up here: https://t.co/aAeNblXcYo",
    "createdAt": "Wed Feb 12 19:59:01 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 262,
    "replyCount": 54,
    "likeCount": 1799,
    "quoteCount": 19,
    "viewCount": 130865,
    "bookmarkCount": 1269,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新短期课程：Transformer 中的注意力机制：PyTorch 实践与核心概念。\n\n上周我们发布了一个关于大语言模型 (Large Language Model, LLM) Transformer 如何工作的课程。本周，我们将深入探讨注意力机制 (Attention Mechanism) 背后的技术思想，并学习如何在 PyTorch (PyTorch) 中实现代码。本课程与 StatQuest 的创始人兼 CEO @joshuastarmer 共同打造。\n\n注意力机制是一项突破性技术，它催生了 Transformer (Transformer) 架构，为 ChatGPT 等大语言模型提供了核心支持。Transformer 架构由 Viswani 等人在 2017 年的论文“Attention is All You Need”中提出，因其高度可扩展的设计而迅速普及。\n\n在本课程中，你将学习作为基于 Transformer 的大语言模型关键元素的注意力机制如何运作，并在 PyTorch 中亲手实现它。你将培养关于构建可靠、功能强大且可扩展的 AI 应用程序的深刻直觉。\n\n你将学习什么：\n- 了解注意力机制的演变，这项关键突破如何引领了 Transformer 的诞生。\n- 学习词嵌入 (Word Embeddings)、位置嵌入 (Positional Embeddings) 和注意力机制之间的关系。\n- 了解查询矩阵 (Query Matrix)、键矩阵 (Key Matrix) 和值矩阵 (Value Matrix)，以及如何在注意力机制中生成和使用它们。\n- 深入探索计算自注意力 (Self-Attention) 和掩码自注意力 (Masked Self-Attention) 所需的数学知识，从而理解它们的工作原理和原因。\n- 理解自注意力与掩码自注意力之间的区别，以及前者如何在编码器 (Encoder) 中用于构建上下文感知嵌入，后者如何在解码器 (Decoder) 中用于生成输出。\n- 学习编码器-解码器架构、交叉注意力 (Cross-Attention) 和多头注意力 (Multi-Head Attention) 的细节，以及它们如何共同整合到 Transformer 中。\n- 使用 PyTorch 编写一个类，实现自注意力、掩码自注意力和多头注意力。\n\n本课程包含许多精彩的技术细节。请在此处注册：https://t.co/aAeNblXcYo"
  },
  {
    "id": "1889380742263890238",
    "url": "https://x.com/AndrewYNg/status/1889380742263890238",
    "text": "VP @JDVance at the Paris AI Summit: \"I'm not here to talk about AI Safety... I'm here to talk about AI Opportunity.\" This is excellent! Thrilled to see the US gov   focus on opportunities in AI.",
    "createdAt": "Tue Feb 11 18:27:26 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 270,
    "replyCount": 209,
    "likeCount": 3366,
    "quoteCount": 42,
    "viewCount": 264091,
    "bookmarkCount": 163,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "副总裁 @JDVance 在巴黎 AI 峰会上表示：“我来这里不是为了谈论 AI 安全 (AI Safety)... 我来这里是为了谈论 AI 机会 (AI Opportunity)。” 这太棒了！ 很高兴看到美国政府聚焦于 AI 带来的机遇。"
  },
  {
    "id": "1889369284482351280",
    "url": "https://x.com/AndrewYNg/status/1889369284482351280",
    "text": "The U.S. imports over $3 trillion/year of goods. With Trump imposing new tariffs, import compliance is getting more complex. Fortunately, we have an AI agentic solution to help! \n\nLast summer, we saw the possibility of new tariffs in 2025, and partnered with Emil Stefanutti to build a solution. When importing a bicycle from Mexico, whether its tires are 20-24 inches or 25-28 inches changes the classification code and duty rate required in the import paperwork. That’s why tariff compliance can require specialized brokers pouring over thousands of pages of regulations. And if a product is described inaccurately in the paperwork, it can get stuck at the border for weeks. Now multiply this by the thousands of products traded worldwide on any given day. \n\nWith Gaia Dynamics, importers can enter a product name and description, answer targeted clarifying questions (such as the bicycle tire size), and get a recommendation for the best way to describe the product and also possible classification codes. Gaia also tracks changing tariffs, including rumored changes, to help with planning.\n\nGaia Dynamics is available at https://t.co/d9BKBVH8pz.",
    "createdAt": "Tue Feb 11 17:41:54 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 85,
    "replyCount": 83,
    "likeCount": 670,
    "quoteCount": 10,
    "viewCount": 74941,
    "bookmarkCount": 273,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "美国每年进口的商品总额超过 3 万亿美元。随着特朗普政府实施新关税政策，进口商品的合规审查变得日益复杂。幸运的是，我们有了一个 AI 智能体解决方案 (AI Agentic Solution) 来提供帮助！\n\n去年夏天，我们预见到 2025 年可能会有新的关税政策出台，因此与 Emil Stefanutti 合作开发了一个解决方案。举例来说，从墨西哥进口一辆自行车，仅仅是轮胎尺寸从 20-24 英寸变为 25-28 英寸，就可能导致进口报关单上的分类代码和关税税率发生变化。正因如此，关税合规工作往往需要专业的报关员仔细研究数千页的法规。如果产品在报关单中的描述不够准确，商品就可能在边境被扣留数周。而全球每天贸易的商品成千上万，上述问题的影响将被无限放大。\n\n通过 Gaia Dynamics，进口商可以输入产品名称和描述，然后回答一些有针对性的澄清问题 ( 例如：自行车轮胎的尺寸 )，系统便能推荐最佳的产品描述方式以及可能适用的分类代码。此外，Gaia 还会跟踪不断变化的关税信息，包括潜在的关税变动传闻，从而帮助进口商提前做好规划。\n\n您可以访问 Gaia Dynamics：https://t.co/d9BKBVH8pz。"
  },
  {
    "id": "1889003138612650081",
    "url": "https://x.com/AndrewYNg/status/1889003138612650081",
    "text": "Since DeepSeek R1's release, very quickly AWS, Azure, Fireworks AI, Groq, Hugging Face, SambaNova and Together AI all started to host R1 variants. What's the \"best\" model changes frequently, and so developers often want to try out new ones. The aisuite package, which helps developers do this quickly with minimal code changes.\n\nThanks Rohit Prsad & team for working with me on this!\n\nhttps://t.co/gwz9oKTCFx",
    "createdAt": "Mon Feb 10 17:26:58 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 280,
    "replyCount": 140,
    "likeCount": 1548,
    "quoteCount": 12,
    "viewCount": 145119,
    "bookmarkCount": 752,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "自从 DeepSeek R1 发布以来，AWS、Azure、Fireworks AI、Groq、Hugging Face、SambaNova 和 Together AI 等平台都迅速开始托管 R1 的各种变体。由于“最好”的模型定义经常变化，开发者们总是希望尝试新的模型。这时，aisuite 包就能派上用场，它能帮助开发者用最少的代码改动快速实现这一目标。\n\n感谢 Rohit Prsad 及其团队与我在此项目上的合作！\n\nhttps://t.co/gwz9oKTCFx"
  },
  {
    "id": "1887919658201960807",
    "url": "https://x.com/AndrewYNg/status/1887919658201960807",
    "text": "A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to talk about 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.”\n\nThere aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job).\n\nBut for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow.\n\n10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done.\n\nI think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact.\n\nSimilarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way.\n\nA 2023 Harvard/BCG study estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves.\n\nHere in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.”\n\n[Original text: https://t.co/svQYHp3XVW ]",
    "createdAt": "Fri Feb 07 17:41:37 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 233,
    "replyCount": 161,
    "likeCount": 1302,
    "quoteCount": 61,
    "viewCount": 183554,
    "bookmarkCount": 849,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "“10倍工程师”——一个在科技界广为接受的概念——指的是影响力相当于普通工程师10倍的人。但我们似乎很少提及10倍营销人员、10倍招聘人员或10倍财务分析师。随着越来越多的工作变得AI赋能 (AI enabled)，我认为这种局面将会改变，并且会出现更多“10倍专业人士”。\n\n目前没有更多“10倍专业人士”的原因在于，在许多岗位上，最优秀的员工和普通员工之间的表现差距存在上限。无论超市收银员动作多么敏捷，他们都不太可能让顾客结账速度快10倍。同样，即使是最好的医生，也不太可能让病人比普通医生快10倍康复 (不过对于生病的患者而言，即使是很小的差异也弥足珍贵 )。在许多工作中，物理定律限制了任何人类或AI的能力 (除非我们彻底重塑这份工作 )。\n\n然而，对于许多主要涉及应用知识或处理信息的工作，AI将带来变革性的影响。在少数岗位上，我开始看到那些精通技术的人，他们能够整合一套技术工具，以不同的方式开展工作，并开始产生——即便尚未达到10倍影响力——也已轻松实现2倍的影响力。我预计这种差距会持续扩大。\n\n10倍工程师并非编写代码的速度快10倍。相反，他们做出能够带来显著更好后续影响的技术架构决策；他们更有效地发现问题并确定任务优先级；而且，他们可能不是重写10,000行代码 (或标记10,000个训练样本 (training examples) )，而是想方设法只写100行 (或收集100个样本 ) 就能完成任务。\n\n我认为10倍营销人员、招聘人员和分析师，也将以类似的方式有所作为。例如，传统的营销人员可能只是反复撰写社交媒体帖子。而10倍营销人员或许会利用AI辅助写作，但这种变革将远不止于此。如果他们对如何应用AI有深入的理解——理想情况下能够自己编写代码来测试想法、自动化任务或分析数据——他们最终可能会进行更多实验，更深入地洞察客户需求，并生成比传统营销人员更精确或更个性化的信息，从而最终产生10倍的影响。\n\n同样，10倍招聘人员也不会仅仅使用生成式 AI 来帮助撰写给候选人的邮件或总结面试。(这种基于提示的AI使用水平很快就会成为许多知识型岗位的基本标配 )。他们可能会整合一套AI工具，高效地识别并对大量候选人进行研究，从而比普通招聘人员产生显著更大的影响。而10倍分析师也不会仅仅使用生成式 AI 来编辑他们的报告。他们可能会编写代码来编排一套AI智能体 (AI Agent)，对产品、市场和公司进行深入研究，从而得出比传统研究方式远为更有价值的结论。\n\n2023年哈佛大学/波士顿咨询公司 (BCG) 的一项研究估计，在提供GPT-4的情况下，顾问能够多完成12%的任务，并且完成任务的速度快25%。这仅仅是平均水平，而且是基于2023年的技术。通过复杂方式运用AI所能获得的最大优势将远超于此，并且只会随着技术的进步而不断增长。\n\n在硅谷，我看到越来越多的AI原生团队正在重塑工作流程，并以截然不同的方式开展工作。在软件工程领域，我们一直推崇最优秀的工程师，因为他们能够产生巨大的影响力。这激励了一代又一代的工程师不断学习和努力工作，因为这样做会增加他们从事高影响力工作的机会。我相信，随着AI在更多工作岗位上变得更具助力，我们将为更多人成为“10倍专业人士”开辟类似的道路。\n\n[原文链接: https://t.co/svQYHp3XVW ]"
  },
  {
    "id": "1887542467173753282",
    "url": "https://x.com/AndrewYNg/status/1887542467173753282",
    "text": "@Nimaano_ Thanks!",
    "createdAt": "Thu Feb 06 16:42:47 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 0,
    "replyCount": 2,
    "likeCount": 13,
    "quoteCount": 0,
    "viewCount": 5651,
    "bookmarkCount": 0,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "@Nimaano_ 谢谢！"
  },
  {
    "id": "1887533748205592656",
    "url": "https://x.com/AndrewYNg/status/1887533748205592656",
    "text": "You can also play with the demo here: https://t.co/3kZJPmwUD4",
    "createdAt": "Thu Feb 06 16:08:08 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 31,
    "replyCount": 26,
    "likeCount": 135,
    "quoteCount": 0,
    "viewCount": 37615,
    "bookmarkCount": 109,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "你也可以在这里体验这个演示：https://t.co/3kZJPmwUD4"
  },
  {
    "id": "1887533627275419690",
    "url": "https://x.com/AndrewYNg/status/1887533627275419690",
    "text": "Introducing Agentic Object Detection!\n\nGiven a text prompt like “unripe strawberries” or “Kellogg’s branded cereal” and an image, we use an agentic workflow to reason at length and detect the specified objects. No need to label any training data. Watch the video for details.",
    "createdAt": "Thu Feb 06 16:07:40 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 722,
    "replyCount": 199,
    "likeCount": 4560,
    "quoteCount": 101,
    "viewCount": 395018,
    "bookmarkCount": 3677,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "隆重推出 AI 智能体目标检测 (Agentic Object Detection)!\n\n给定一个文本提示（例如“未成熟的草莓”或“家乐氏品牌的麦片”）和一张图像，我们采用一种 AI 智能体工作流 (agentic workflow) 来进行深入推理，进而检测出指定的物体。更棒的是，它无需标注任何训练数据。观看视频了解更多详情。"
  },
  {
    "id": "1887184924165492940",
    "url": "https://x.com/AndrewYNg/status/1887184924165492940",
    "text": "Announcing How Transformer LLMs Work, created with @JayAlammar and @MaartenGr, co-authors of the beautifully illustrated book, “Hands-On Large Language Models.”\n\nThis course offers a deep dive into the inner workings of the transformer architecture that powers large language models (LLMs).\n\nThe transformer architecture revolutionized generative AI; in fact, the \"GPT\" in ChatGPT stands for \"Generative Pre-Trained Transformer.\" Originally introduced in the Google Brain team's groundbreaking 2017 paper \"Attention Is All You Need,\" by Vaswani and others, transformers were a highly scalable model for machine translation tasks. Variants of this architecture now power today’s LLMs such as those from OpenAI, Google, Meta, Cohere, Anthropic and DeepSeek.\n\nIn this course, you’ll learn in detail how LLMs process text. You'll also work through code examples that illustrate that transformer's individual components.\n\nIn details, you’ll learn:\n- How the representation of language has evolved, from  Bag-of-Words to Word2Vec embeddings to the transformer architecture that captures a word's meanings taking into account the context of other words in the input.\n- How inputs are broken down into tokens before they are sent to the language model.\n- The details of a transformer's main stages: Tokenization and embedding, the stack of transformer blocks, and the language model head.\n- The inner workings of the transformer block, including attention, which calculates relevance scores, and the feedforward layer, which incorporates stored information learned in training.\n- How cached calculations make transformers faster.\n- Some of the most recent ideas in the latest models such as Mixture-of-Experts (MoE) which uses multiple sub-models and a router on each layer to improve the quality of LLMs.\n\nBy the end of this course, you’ll have a deep understanding of how LLMs actually process text and  be able to read through papers describing the latest models and understand the details.\n\nGaining this intuition will improve your approach to building LLM applications.\n\nPlease sign up here: https://t.co/hdTUASuEbb",
    "createdAt": "Wed Feb 05 17:02:02 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 258,
    "replyCount": 44,
    "likeCount": 1594,
    "quoteCount": 19,
    "viewCount": 236843,
    "bookmarkCount": 1308,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "推出“Transformer 大语言模型工作原理”课程，由精美插图书籍《Hands-On Large Language Models》的合著者 Jay Alammar 和 MaartenGr 共同创作。\n\n本课程将深入剖析为大语言模型 (LLMs) 提供核心动力的 Transformer 架构的内部运行机制。\n\nTransformer 架构彻底改变了生成式 AI (Generative AI) 领域；事实上，ChatGPT 中的“GPT”就代表着“Generative Pre-Trained Transformer”（生成式预训练 Transformer）。该架构最初由 Google Brain 团队在 Vaswani 等人于 2017 年发表的开创性论文《Attention Is All You Need》中提出，它是一种扩展性很强的模型，最初主要用于机器翻译任务。如今，OpenAI、Google、Meta、Cohere、Anthropic 和 DeepSeek 等公司的大语言模型都由这种架构的变体所驱动。\n\n在本课程中，您将详细了解大语言模型如何处理文本。您还将通过代码示例，深入探究 Transformer 的各个组成部分。\n\n具体而言，您将学习：\n- 语言表示方式的演变历程：从词袋模型 (Bag-of-Words) 到 Word2Vec 嵌入，再到 Transformer 架构，后者在捕获词义时，能够充分考虑输入中其他词的上下文信息。\n- 输入文本是如何被分解成 Token (Token) ，然后发送给语言模型的。\n- Transformer 主要阶段的详细过程：Token 化和嵌入、堆叠的 Transformer 模块，以及语言模型头部。\n- Transformer 模块的内部工作原理，包括计算相关性得分的注意力机制 (attention)，以及整合在训练中习得的存储信息的全连接层 (feedforward layer)。\n- 计算结果的缓存如何提高 Transformer 的运行速度。\n- 最新模型中的一些前沿思想，例如专家混合模型 (Mixture-of-Experts, MoE)，它在每一层利用多个子模型和一个路由器来提升大语言模型的质量。\n\n通过本课程的学习，您将对大语言模型处理文本的实际方式有深刻的理解，并能够阅读描述最新模型的学术论文，并透彻理解其细节。\n\n掌握这种直观认识将有效提升您构建大语言模型应用程序的方法和效率。\n\n请在此处注册：https://t.co/hdTUASuEbb"
  },
  {
    "id": "1886871011578273818",
    "url": "https://x.com/AndrewYNg/status/1886871011578273818",
    "text": "Thank you @NYSE for highlighting Coursera on your trading floor to help us celebrate Greg Hart joining as our CEO! 🎉 https://t.co/JNe3wvPyNY",
    "createdAt": "Tue Feb 04 20:14:40 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 43,
    "replyCount": 24,
    "likeCount": 1083,
    "quoteCount": 3,
    "viewCount": 70298,
    "bookmarkCount": 37,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "感谢 @NYSE 在你们的交易大厅宣传 Coursera，和我们一同庆祝 Greg Hart 出任公司首席执行官！🎉 https://t.co/JNe3wvPyNY"
  },
  {
    "id": "1886833904235241753",
    "url": "https://x.com/AndrewYNg/status/1886833904235241753",
    "text": "Announcing AI Dev 25: A conference for AI developers, this Pi day (3/14/2025)!\n\nThere're great AI academic conferences for researchers (NeurIPS, ICLR, ICML, etc.) and some companies hold great meetings around their products (Google I/O, OpenAI DevDay, etc.). But we need more vendor-neutral meetings for AI developers, so I decided to organize this. \n\nThis is a technical meeting, and we'll have >400 developers gathering in-person in San Francisco to build, share ideas, and network.\n\nThis will be fun! https://t.co/i4bQevDG4i",
    "createdAt": "Tue Feb 04 17:47:13 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 94,
    "replyCount": 160,
    "likeCount": 630,
    "quoteCount": 14,
    "viewCount": 72526,
    "bookmarkCount": 148,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "隆重宣布 AI Dev 25：一场专为 AI 开发者举办的会议，定于今年的圆周率日 (3/14/2025) 举行！\n\n目前，我们有很多面向研究人员的优秀 AI 学术会议 (例如 NeurIPS, ICLR, ICML 等)，也有一些公司围绕其产品举办的精彩活动 (如 Google I/O, OpenAI DevDay 等)。然而，我们还需要更多面向 AI 开发者、且“供应商中立”的聚会，因此我决定组织此次盛会。\n\n这将是一场纯技术性的会议，届时将有超过 400 名开发者齐聚旧金山，大家将面对面地聚在一起，共同构建、分享创意并进行交流。\n\n这场活动一定会非常有趣！https://t.co/i4bQevDG4i"
  },
  {
    "id": "1885522069301211562",
    "url": "https://x.com/AndrewYNg/status/1885522069301211562",
    "text": "@StanfordHAI @landay @erikbryn @alex_pentland @YejinChoinka Fun event, and great to see @StanfordHAI have such a strong presence at WEF. Thank you @landay for organizing this!",
    "createdAt": "Sat Feb 01 02:54:27 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1,
    "replyCount": 4,
    "likeCount": 14,
    "quoteCount": 0,
    "viewCount": 6579,
    "bookmarkCount": 3,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "@StanfordHAI @landay @erikbryn @alex_pentland @YejinChoinka 这次活动很有趣，很高兴看到 @StanfordHAI 在 WEF 有如此强大的影响力。感谢 @landay 的组织！"
  },
  {
    "id": "1885033810552905814",
    "url": "https://x.com/AndrewYNg/status/1885033810552905814",
    "text": "The buzz over DeepSeek this week crystallized, for many people, a few important trends that have been happening in plain sight: (i) China is catching up to the U.S. in generative AI, with implications for the AI supply chain. (ii) Open weight models are commoditizing the foundation-model layer, which creates opportunities for application builders. (iii) Scaling up isn’t the only path to AI progress. Despite the massive focus on and hype around processing power, algorithmic innovations are rapidly pushing down training costs.\n\nAbout a week ago, DeepSeek, a company based in China, released DeepSeek-R1, a remarkable model whose performance on benchmarks is comparable to OpenAI’s o1. Further, it was released as an open weight model with a permissive MIT license. At Davos last week, I got a lot of questions about it from non-technical business leaders. And on Monday, the stock market saw a “DeepSeek selloff”: The share prices of Nvidia and a number of other U.S. tech companies plunged. (As of the time of writing, some have recovered somewhat.)\n\nHere’s what I think DeepSeek has caused many people to realize:\n\nChina is catching up to the U.S. in generative AI. When ChatGPT was launched in November 2022, the U.S. was significantly ahead of China in generative AI. Impressions change slowly, and so even recently I heard friends in both the U.S. and China say they thought China was behind. But in reality, this gap has rapidly eroded over the past two years. With models from China such as Qwen (which my teams have used for months), Kimi, InternVL, and DeepSeek, China had clearly been closing the gap, and in areas such as video generation there were already moments where China seemed to be in the lead.\n\nI’m thrilled that DeepSeek-R1 was released as an open weight model, with a technical report that shares many details. In contrast, a number of U.S. companies have pushed for regulation to stifle open source by hyping up hypothetical AI dangers such as human extinction. It is now clear that open source/open weight models are a key part of the AI supply chain: Many companies will use them. If the U.S. continues to stymie open source, China will come to dominate this part of the supply chain and many businesses will end up using models that reflect China’s values much more than America’s.\n\nOpen weight models are commoditizing the foundation-model layer. As I wrote previously, LLM token prices have been falling rapidly, and open weights have contributed to this trend and given developers more choice. OpenAI’s o1 costs $60 per million output tokens; DeepSeek R1 costs $2.19. This nearly 30x difference brought the trend of falling prices to the attention of many people.\n\nThe business of training foundation models and selling API access is tough. Many companies in this area are still looking for a path to recouping the massive cost of model training. Sequoia’s article “AI’s $600B Question” lays out the challenge well (but, to be clear, I think the foundation model companies are doing great work, and I hope they succeed). In contrast, building applications on top of foundation models presents many great business opportunities. Now that others have spent billions training such models, you can access these models for mere dollars to build customer service chatbots, email summarizers, AI doctors, legal document assistants, and much more.\n\nScaling up isn’t the only path to AI progress. There’s been a lot of hype around scaling up models as a way to drive progress. To be fair, I was an early proponent of scaling up models. A number of companies raised billions of dollars by generating buzz around the narrative that, with more capital, they could (i) scale up and (ii) predictably drive improvements. Consequently, there has been a huge focus on scaling up, as opposed to a more nuanced view that gives due attention to the many different ways we can make progress. Driven in part by the U.S. AI chip embargo, the DeepSeek team had to innovate on many optimizations to run on less-capable H800 GPUs rather than H100s, leading ultimately to a model trained (omitting research costs) for under $6M of compute.\n\nIt remains to be seen if this will actually reduce demand for compute. Sometimes making each unit of a good cheaper can result in more dollars in total going to buy that good. I think the demand for intelligence and compute has practically no ceiling over the long term, so I remain bullish that humanity will use more intelligence even as it gets cheaper.\n\nI saw many different interpretations of DeepSeek’s progress here in X, as if it was a Rorschach test that allowed many people to project their own meaning onto it. I think DeepSeek-R1 has geopolitical implications that are yet to be worked out. And it’s also great for AI application builders. My team has already been brainstorming ideas that are newly possible only because we have easy access to an open advanced reasoning model. This continues to be a great time to build!\n\n[Original text: https://t.co/yiOHeGJgLZ ]",
    "createdAt": "Thu Jan 30 18:34:17 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1048,
    "replyCount": 290,
    "likeCount": 4405,
    "quoteCount": 128,
    "viewCount": 614217,
    "bookmarkCount": 1983,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "本周围绕 DeepSeek 产生的热议，让许多人清楚地认识到几个一直在悄然发生的重要趋势：(i) 中国在生成式 AI (Generative AI) 方面正在追赶美国，这对 AI 供应链有着深远影响。(ii) 开放权重模型 (Open weight models) 正在让基础模型层商品化，这为应用程序开发者创造了新的机遇。(iii) 一味追求规模化并非 AI 进步的唯一途径。尽管算力备受关注并被大肆炒作，但算法创新正在迅速降低模型训练成本。\n\n大约一周前，一家中国公司 DeepSeek 发布了 DeepSeek-R1，这是一款卓越的模型，其在基准测试上的表现足以与 OpenAI 的 o1 相媲美。更令人振奋的是，它以开放权重模型形式发布，并采用了宽松的 MIT 许可证。上周在达沃斯，我收到了许多非技术商业领袖关于它的提问。本周一，股市甚至出现了“DeepSeek 抛售”：Nvidia 和许多其他美国科技公司的股价应声暴跌。（截至本文撰写时，部分股价已有所回升。）\n\n以下是我认为 DeepSeek 让许多人意识到的几点：\n\n中国在生成式 AI 方面正在追赶美国。当 ChatGPT 于 2022 年 11 月问世时，美国在生成式 AI 领域确实显著领先于中国。人们的固有印象改变缓慢，因此即使最近，我仍听到美国和中国的朋友说他们认为中国在这方面仍处于劣势。但实际上，这个差距在过去两年中已迅速弥合。随着来自中国的 Qwen (我的团队已使用数月)、Kimi、InternVL 和 DeepSeek 等模型的相继问世，中国显然一直在拉近与美国的距离，在视频生成等某些领域，中国甚至一度展现出领先的态势。\n\n我很高兴 DeepSeek-R1 能以开放权重模型形式发布，同时还附带了一份详尽的技术报告。相比之下，一些美国公司却通过夸大“人类灭绝”等假设的 AI 危险，来推动监管以打压开源。如今，开源/开放权重模型已明确成为 AI 供应链的关键一环：未来会有大量公司使用它们。如果美国继续阻碍开源发展，中国将主导这部分供应链，届时许多企业最终将不得不使用那些更反映中国价值观而非美国价值观的模型。\n\n开放权重模型正在让基础模型层商品化。正如我之前所写，大语言模型 (LLM) token (Token) 的价格一直在迅速下跌，开放权重模型对此趋势起到了推动作用，并为开发者提供了更多选择。OpenAI 的 o1 每百万输出 token 成本为 60 美元；而 DeepSeek R1 仅为 2.19 美元。这近 30 倍的巨大差异，让价格持续下降的趋势受到了更多人的关注。\n\n训练基础模型并销售 API 访问并非易事。许多身处这个领域的公司仍在探索如何收回巨额的模型训练成本。Sequoia 的文章“AI 的 6000 亿美元问题”很好地阐述了这一挑战（但需要澄清的是，我认为基础模型公司正在做着了不起的工作，我希望它们能成功）。相比之下，在基础模型之上构建应用程序则带来了许多巨大的商业机遇。既然已有公司投入数十亿美元训练出这些模型，你现在只需花费区区几美元，就能访问这些模型，从而构建客户服务聊天机器人、电子邮件摘要器、AI 医生、法律文档助手等等。\n\n规模化并非 AI 进步的唯一途径。此前，将模型规模化作为推动 AI 进步的唯一途径被大肆炒作。平心而论，我曾是模型规模化的早期倡导者。一些公司通过宣扬这样的论调——只要有更多资金，就能 (i) 扩大规模并 (ii) 持续实现可预测的改进——从而制造轰动效应，筹集了数十亿美元。因此，人们对规模化给予了巨大关注，而忽视了对取得进步的多种不同途径的更细致考量。部分受美国 AI 芯片禁运的影响，DeepSeek 团队不得不进行大量优化创新，以使模型能在性能相对较低的 H800 GPU 而非 H100 上运行，最终使得该模型的训练计算成本 (不包括研究成本) 低于 600 万美元。\n\n这是否真的会减少对算力的需求，还有待观察。有时，商品单位价格的降低反而会促使人们购买更多，从而增加总支出。我认为从长远来看，对智能和算力的需求几乎没有上限，所以我仍然乐观地认为，即使智能变得更便宜，人类也会使用更多的智能。\n\n我在这里的 X 上看到了许多对 DeepSeek 进展的不同解读，就好像它是一面罗夏测试 (Rorschach test) 的镜子，让许多人将自己的意义投射到上面。我认为 DeepSeek-R1 具有尚未明朗的地缘政治影响。同时，它对 AI 应用程序开发者也是一个重大利好。我的团队已经开始集思广益，提出了许多以前无法实现，现在因我们能轻松访问一个开放的高级推理模型而变为可能的新想法。这仍然是创造的绝佳时机！\n\n[原文链接: https://t.co/yiOHeGJgLZ ]"
  },
  {
    "id": "1884723330839961664",
    "url": "https://x.com/AndrewYNg/status/1884723330839961664",
    "text": "Welcome Greg Hart as Coursera’s incoming CEO!\n\nI’m thrilled to announce that Greg will be joining Coursera as CEO, succeeding Jeff Maggioncalda after seven remarkable years of leadership.\n\nJeff has been an extraordinary leader. Under his guidance, Coursera has grown into a global platform serving over 160 million learners, expanded our partnerships to over 350 world-class universities and industry leaders, and debuted as a public company. Coursera exists to serve learners, and Jeff’s unwavering commitment to our goal of transforming lives through learning leaves an enduring legacy. I’m deeply grateful for his leadership, dedication, and partnership throughout this journey.\n\nAnd, I’m thrilled to welcome Greg Hart as our next CEO. Greg brings over 25 years of experience in technology-driven innovation and operational excellence, including leading the development and launch of Amazon Alexa and scaling Prime Video globally. At Compass, the leading real estate brokerage in the US, Greg served as Chief Product Officer and later Chief Operating Officer, where he helped take the company public and led the development of its industry-leading technology platform. His ability to combine strategic vision with operational excellence, coupled with his passion for education, makes him the ideal choice to lead Coursera into its next chapter.\n\nCoursera was founded with a mission to provide universal access to world-class learning. I’m grateful for everyone who has contributed to this journey – learners, educators, our team, and our many partners who have helped us advance this vision in countless ways. Yet, demand for high-quality training continues to grow, and our mission is far from complete. As we enter this next chapter, I’m excited about Greg’s leadership and what the exceptional Coursera team will do. We will keep working hard to serve learners everywhere!\n\n https://t.co/chvmwiqVGi",
    "createdAt": "Wed Jan 29 22:00:33 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 41,
    "replyCount": 34,
    "likeCount": 552,
    "quoteCount": 2,
    "viewCount": 61656,
    "bookmarkCount": 41,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "欢迎 Greg Hart 出任 Coursera 新任首席执行官！\n\n我非常高兴地宣布，Greg 将加入 Coursera 担任首席执行官，接替 Jeff Maggioncalda 卓越领导七年后的职位。\n\nJeff 是一位杰出的领导者。在他的指导下，Coursera 已发展成为一个服务全球超过 1.6 亿学习者的平台，将我们的合作伙伴关系扩展到超过 350 所世界一流大学和行业领导者，并成功上市。Coursera 致力于服务学习者，Jeff 始终坚定地致力于通过学习改变生活的目标，留下了深远影响。我深切感谢他在此期间的领导、奉献与合作。\n\n同时，我非常高兴地欢迎 Greg Hart 担任我们的下一任首席执行官。Greg 带来了超过 25 年的技术创新和卓越运营经验，其中包括领导 Amazon Alexa 的开发与发布，以及在全球范围内推广 Prime Video。在美国领先的房地产经纪公司 Compass，Greg 曾担任首席产品官，随后升任首席运营官，在此期间，他帮助公司成功上市，并主导开发了其行业领先的技术平台。他不仅能将战略愿景与卓越的运营能力相结合，还对教育充满热情，这使他成为带领 Coursera 开启新篇章的理想人选。\n\nCoursera 的创立使命是普及世界一流的教育。我感谢所有为这段旅程做出贡献的人——学习者、教育工作者、我们的团队，以及无数以各种方式帮助我们推进这一愿景的合作伙伴。然而，对高质量培训的需求仍在持续增长，我们的使命远未完成。当我们迈入下一个篇章时，我为 Greg 的领导以及卓越的 Coursera 团队即将创造的成就感到兴奋。我们将继续努力，为世界各地的学习者服务！\n\n https://t.co/chvmwiqVGi"
  },
  {
    "id": "1883972263177072730",
    "url": "https://x.com/AndrewYNg/status/1883972263177072730",
    "text": "Today's \"DeepSeek selloff\" in the stock market -- attributed to DeepSeek V3/R1 disrupting the tech ecosystem -- is another sign that the application layer is a great place to be. The foundation model layer being  hyper-competitive is great for people building applications.",
    "createdAt": "Mon Jan 27 20:16:04 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1027,
    "replyCount": 243,
    "likeCount": 7160,
    "quoteCount": 194,
    "viewCount": 789390,
    "bookmarkCount": 1230,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "今天股市中的“DeepSeek 大跌”（DeepSeek selloff）——这被归因于 DeepSeek V3/R1 正在颠覆（disrupting）科技生态系统——再次表明，应用层（application layer）是一个极具潜力的领域。基础模型层（foundation model layer）的超高竞争度对于那些开发应用程序的人来说，反而是一件好事。"
  },
  {
    "id": "1882828225979760651",
    "url": "https://x.com/AndrewYNg/status/1882828225979760651",
    "text": "Discussion at Davos with @Yoshua_Bengio, @YejinChoinka, @JonathanRoss321, @Thom_Wolf moderated by @nxthompson. We share excitement for the future of AI, the science to be done, and the many things yet to be built. Take a look!",
    "createdAt": "Fri Jan 24 16:30:04 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 57,
    "replyCount": 49,
    "likeCount": 274,
    "quoteCount": 7,
    "viewCount": 75434,
    "bookmarkCount": 106,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isQuote": true,
    "isPinned": false,
    "tranlastedContent": "在达沃斯与 @Yoshua_Bengio、@YejinChoinka、@JonathanRoss321、@Thom_Wolf 进行了讨论，会议由 @nxthompson 主持。我们都对人工智能 (AI) 的未来充满期待，对需要深入进行的科学研究感到兴奋，也对未来有待构建的众多事物充满憧憬。快来看看吧！"
  },
  {
    "id": "1882125891821822398",
    "url": "https://x.com/AndrewYNg/status/1882125891821822398",
    "text": "Our first short course with @AnthropicAI! Building Towards Computer Use with Anthropic. This teaches you to build an LLM-based agent that uses a computer interface by generating mouse clicks and keystrokes. Computer Use is an important, emerging capability for LLMs that will let AI agents do many more tasks than were possible before, since it lets them interact with interfaces designed for humans to use, rather than only tools that provide explicit API access. I hope you will enjoy learning about it!\n\nThis course is taught by Anthropic's Head of Curriculum,  @Colt_Steele. You'll learn to apply image reasoning and tool use to \"use\" a computer as follows: a model processes an image of the screen, analyzes it to understand what's going on, and navigates the computer via mouse clicks and keystrokes.\n\nThis course goes through the key building blocks, and culminates in a demo of an AI assistant that uses a web browser to search for a research paper, downloads the PDF, and finally summarizes the paper for you.\n\nIn detail, you’ll:\n- Learn about Anthropic's family of models, when to use which one, and make API requests to Claude\n- Use multi-modal prompts that combine text and image content blocks, and also work with streaming responses\n- Improve your prompting by using prompt templates, using XML to structure prompts, and providing examples\n- Implement prompt caching to reduce cost and latency\n- Apply tool-use to build a chatbot that can call different tools to respond to queries\n- See all these building blocks come together in Computer Use demo\n\nPlease sign up here: https://t.co/lM3m6zsJ40",
    "createdAt": "Wed Jan 22 17:59:15 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 321,
    "replyCount": 48,
    "likeCount": 2181,
    "quoteCount": 17,
    "viewCount": 168406,
    "bookmarkCount": 1973,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "我们与 @AnthropicAI 联合推出的首个短期课程来啦！本课程名为“与 Anthropic 共探计算机使用能力”。它将教你如何构建一个基于大语言模型 (Large Language Model, LLM) 的 AI 智能体 (AI Agent)，该智能体能够通过模拟鼠标点击和键盘输入来操作计算机界面。对 LLM 来说，“计算机使用能力”是一项重要的新兴技能，它能让 AI 智能体完成远超以往的任务，因为这使得它们可以与专为人类设计的界面进行交互，而不再局限于那些只提供明确 API (应用程序编程接口) 访问的工具。希望大家享受学习过程！\n\n本课程由 Anthropic 的课程负责人 @Colt_Steele 亲自讲授。你将学到如何运用图像推理和工具使用 (Tool Use) 来“操作”计算机，具体流程是：模型处理屏幕图像，对其进行分析以理解当前情况，然后通过模拟鼠标点击和键盘输入来导航计算机。\n\n本课程将详细介绍这项能力的关键组成部分，并最终展示一个 AI 助手演示，该助手能够利用网络浏览器搜索研究论文，下载 PDF 文档，并最终为你总结论文内容。\n\n具体来说，你将：\n- 了解 Anthropic 的模型家族，学习何时选择使用哪种模型，并向 Claude 发出 API 请求\n- 使用结合文本和图像内容块的多模态提示，并学习如何处理流式响应\n- 通过使用提示模板、利用 XML 结构化提示以及提供示例来优化你的提示词\n- 实现提示缓存 (Prompt Caching) 以降低成本和延迟\n- 应用工具使用 (Tool Use) 来构建一个能够调用不同工具响应查询的聊天机器人\n- 在“计算机使用”演示中，亲眼见证所有这些构建模块如何协同运作\n\n请在此处注册：https://t.co/lM3m6zsJ40"
  },
  {
    "id": "1880728653329514606",
    "url": "https://x.com/AndrewYNg/status/1880728653329514606",
    "text": "Save the date! Pi day (3.14) is coming soon and I'm thinking of organizing something fun and in-person for AI developers in the San Francisco area. More details to come!",
    "createdAt": "Sat Jan 18 21:27:07 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 55,
    "replyCount": 53,
    "likeCount": 739,
    "quoteCount": 6,
    "viewCount": 68313,
    "bookmarkCount": 51,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "日期定啦！圆周率日 (3.14) 即将到来，我正计划为旧金山地区的 AI 开发者举办一场有趣的线下活动。更多详情即将公布，敬请期待！"
  },
  {
    "id": "1879939058211971420",
    "url": "https://x.com/AndrewYNg/status/1879939058211971420",
    "text": "Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!\n\nSoftware is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.\n\nThis is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products.\n\nMany companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.\n\nThis change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow.\n\nFurther, AI Product Management requires a different set of skills than traditional software Product Management. It requires:\n- Technical proficiency in AI. PMs need to understand what products might be technically feasible to build. They also need to understand the lifecycle of AI projects, such as data collection, building, then monitoring, and maintenance of AI models.\n- Iterative development. Because AI development is much more iterative than traditional software and requires more course corrections along the way, PMs need be able to manage such a process.\n- Data proficiency. AI products often learn from data, and they can be designed to generate richer forms of data than traditional software.\n- Skill in managing ambiguity. Because AI’s performance is hard to predict in advance, PMs need to be comfortable with this and have tactics to manage it.\n- Ongoing learning. AI technology is advancing rapidly. PMs, like everyone else who aims to make best use of the technology, need to keep up with the latest technology advances, product ideas, and how they fit into users’ lives.\n\nFinally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled at gathering feedback fast to keep projects moving. Increasingly, I also expect strong product managers to be able to build prototypes for themselves.\n\nThe demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.\n\nThe variety of valuable things we can build is nearly unlimited. What a great time to build!\n\n[Original text: https://t.co/OIeAQXpriK ]",
    "createdAt": "Thu Jan 16 17:09:33 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1023,
    "replyCount": 187,
    "likeCount": 5439,
    "quoteCount": 198,
    "viewCount": 875043,
    "bookmarkCount": 4685,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "开发软件，尤其是制作原型，如今成本越来越低。这意味着对那些能决定“做什么产品”的人的需求将大幅增加。因此，AI 产品管理 (AI Product Management) 的未来一片光明！\n\n软件开发通常由团队协作完成，其中包含产品经理 (PMs)，他们负责决定产品方向（比如为哪些用户开发哪些功能），以及软件开发人员 (Software Developers)，他们编写代码来将产品变为现实。经济学原理告诉我们，当两种商品是互补品时——比如汽车（内燃机）和汽油——其中一种商品价格的下降，会导致对另一种商品需求的增长。举例来说，随着汽车价格变得更亲民，购买汽车的人就越多，这自然推高了对汽油的需求。类似的情况也会发生在软件领域。在有了清晰明确的产品规格后，人工智能 (AI) 正在让软件构建过程变得更快、更便宜。这将极大地增加对那些能为有价值的产品或功能提出清晰规格的人才的需求。\n\n正因如此，我对产品管理 (Product Management) 的未来充满期待，它是一门关于开发和管理软件产品的学问。我尤其看好 AI 产品管理的发展前景，它专注于开发和管理 AI 软件产品。\n\n许多公司工程师与产品经理的比例可能在 6:1 左右。（这个比例因公司和行业而异，通常在 4:1 到 10:1 之间。）随着编码效率的提升，团队所需的总劳动力中，产品管理工作（以及设计工作）所占的比例将会增加。或许有些工程师会承担一部分这类工作，但如果这仍然是专业产品经理的职责范畴，那么对这些角色的需求将只增不减。\n\n目前，软件开发团队构成上的这种转变尚未完全加速。减缓这一转变的一个主要因素，尤其是在 AI 产品管理领域，是软件工程师凭借其技术背景，比产品经理更快地理解和接受 AI。即使在今天，大多数公司都很难找到既懂得产品开发又理解 AI 的人才，我预计这种人才短缺将日益严重。\n\n此外，AI 产品管理需要一套不同于传统软件产品管理的技能。它要求：\n-  **AI 技术熟练度** 产品经理需要了解哪些产品在技术上是可行的。他们还需要熟悉 AI 项目的生命周期，例如数据收集、模型构建，以及后续的 AI 模型 (AI Models) 监控和维护。\n-  **迭代开发能力** AI 开发比传统软件开发更具迭代性，过程中需要更多次的方向调整，因此产品经理需要具备管理这种流程的能力。\n-  **数据素养** AI 产品通常依赖数据进行学习，并且可以设计成生成比传统软件更丰富的数据形式。\n-  **管理不确定性的技能** 由于 AI 性能难以提前预测，产品经理需要适应这种不确定性，并掌握应对策略。\n-  **持续学习精神** AI 技术正飞速发展。产品经理，像所有希望充分利用这项技术的人一样，需要不断跟进最新的技术进展、产品理念，以及它们如何融入用户生活。\n\n最后，AI 产品经理还需要懂得如何确保 AI 得到负责任的实施（例如，何时需要设置防护措施以防止不良结果），并且要擅长快速收集反馈，从而推动项目进展。我越来越期待优秀的产品经理也能亲自动手构建原型。\n\n市场对优秀的 AI 产品经理的需求将非常旺盛。除了将 AI 产品管理作为一门独立学科发展之外，也许一些工程师最终也将承担更多的产品管理工作。\n\n我们能创造的有价值的事物几乎没有限制。这是一个多么激动人心的创造时代啊！\n\n[原始文本: https://t.co/OIeAQXpriK ]"
  },
  {
    "id": "1879641293401502163",
    "url": "https://x.com/AndrewYNg/status/1879641293401502163",
    "text": "@nedteneva @realavatarai @DeepLearningAI I've really enjoyed working with you @nedteneva on the tech powering this -- thank you!",
    "createdAt": "Wed Jan 15 21:26:21 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 0,
    "replyCount": 0,
    "likeCount": 1,
    "quoteCount": 0,
    "viewCount": 1559,
    "bookmarkCount": 0,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "@nedteneva @realavatarai @DeepLearningAI 我非常荣幸能与你 @nedteneva 一起，为这项技术提供支持 -- 谢谢你！"
  },
  {
    "id": "1879641073880076513",
    "url": "https://x.com/AndrewYNg/status/1879641073880076513",
    "text": "@dimapyanov @realavatarai It has been great fun working with you on the product  @dimapyanov -- thank you!",
    "createdAt": "Wed Jan 15 21:25:28 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 1,
    "replyCount": 1,
    "likeCount": 13,
    "quoteCount": 0,
    "viewCount": 5991,
    "bookmarkCount": 0,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "@dimapyanov @realavatarai 能与你在这个产品上合作，我感到非常愉快 @dimapyanov -- 谢谢你！"
  },
  {
    "id": "1879590674561110219",
    "url": "https://x.com/AndrewYNg/status/1879590674561110219",
    "text": "Something fun: AI Avatar of me built, by https://t.co/zpIxRSuky4 and @realavatarai. \n\nVideo has details. This is a work in progress, but please come chat with me in avatar form, and let me know what you think!\n\nhttps://t.co/vMO2CM0xfb\n\nThank you Jeff Daniel @consciouspilot and team for working with us on this!",
    "createdAt": "Wed Jan 15 18:05:12 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 158,
    "replyCount": 65,
    "likeCount": 997,
    "quoteCount": 22,
    "viewCount": 107480,
    "bookmarkCount": 491,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "一个有趣的消息：我的 AI 头像 (AI Avatar) 已经由 https://t.co/zpIxRSuky4 和 @realavatarai 打造完成。\n\n视频中包含详细信息。这项工作仍在进行中，欢迎大家以我的 AI 头像形式与我交流，并告诉我你们的想法！\n\nhttps://t.co/vMO2CM0xfb\n\n感谢 Jeff Daniel @consciouspilot 和团队与我们合作完成这项工作！"
  },
  {
    "id": "1879253685232144487",
    "url": "https://x.com/AndrewYNg/status/1879253685232144487",
    "text": "Just released: New AI Climate Simulator that you can play with. Visualize how geoengineering can slow global warming. \n\nThere is no longer any path to limiting warming to 1.5 degrees Celsius (Paris Agreement), unless we use geoengineering. Reflecting 1% of sunlight away from earth would lead to an extra ~1 degree of cooling.\n\nOur simulator lets you explore how geoengineering via Stratospheric Aerosol Injection (SAI) gives us new paths to keep warming to 1.5 degrees. I think SAI is a promising technology worth serious exploration. Check out the simulator here: https://t.co/OxtaQMyDuL\n\nBig thanks to collaborators @jeremy_irvin16, Jake Dexheimer, @dakotagruener, Charlotte DeWald, @DanVisioni, @DWatsonParris, @DougMacMartin, Joshua Elliott, Juerg Luterbacher, Kion Yaghoobzadeh",
    "createdAt": "Tue Jan 14 19:46:08 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 349,
    "replyCount": 166,
    "likeCount": 1853,
    "quoteCount": 69,
    "viewCount": 169765,
    "bookmarkCount": 778,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新发布：你可以亲身体验的新 AI 气候模拟器！直观地了解地球工程 (geoengineering) 如何减缓全球变暖。\n\n除非我们采取地球工程措施，否则已没有任何途径能将全球升温限制在《巴黎协定》设定的 1.5 摄氏度以内。如果能将 1% 的太阳光反射出地球，就能额外实现大约 1 摄氏度的降温。\n\n我们的模拟器能让你探索，通过平流层气溶胶注入 (Stratospheric Aerosol Injection, SAI) 这种地球工程技术，我们如何找到新的路径来将全球升温控制在 1.5 摄氏度。我认为 SAI 是一项前景广阔、值得认真探索的技术。请在这里查看模拟器：https://t.co/OxtaQMyDuL\n\n衷心感谢各位合作者：@jeremy_irvin16, Jake Dexheimer, @dakotagruener, Charlotte DeWald, @DanVisioni, @DWatsonParris, @DougMacMartin, Joshua Elliott, Juerg Luterbacher, Kion Yaghoobzadeh"
  },
  {
    "id": "1877405010893619238",
    "url": "https://x.com/AndrewYNg/status/1877405010893619238",
    "text": "Using AI-assisted coding to build software prototypes is an important way to quickly explore many ideas and invent new things. In this and future posts, I’d like to share with you some best practices for prototyping simple web apps. This post will focus on one idea: being opinionated about the software stack.\n\nThe software stack I personally use changes every few weeks. There are many good alternatives to these choices, and if you pick a preferred software stack and become familiar with its components, you’ll be able to develop more quickly. But as an illustration, here’s my current default:\n- Python with FastAPI for building web-hosted APIs: I develop primarily in Python, so that’s a natural choice for me. If you’re a JavaScript/TypeScript developer, you’ll likely make a different choice. I’ve found FastAPI really easy to use and scalable for deploying web services (APIs) hosted in Python.\n- Uvicorn to run the backend application server (to execute code and serve web pages) for local testing on my laptop.\n- If deploying on the cloud, then either Heroku for small apps or AWS Elastic Beanstalk for larger ones (disclosure: I serve on Amazon’s board of directors): There are many services for deploying jobs, including HuggingFace Spaces, Railway, Google’s Firebase, Vercel, and others. Many of these work fine, and becoming familiar with just 1 or 2 will simplify your development process.\n- MongoDB for NoSQL database: While traditional SQL databases are amazing feats of engineering that result in highly efficient and reliable data storage, the need to define the database structure (or schema) slows down prototyping. If you really need speed and ease of implementation, then dumping most of your data into a NoSQL (unstructured or semi-structured) database such as MongoDB lets you write code quickly and sort out later exactly what you want to do with the data. This is sometimes called schema-on-write, as opposed to schema-on-read. Mind you, if an application goes to scaled production, there are many use cases where a more structured SQL database is significantly more reliable and scalable.\n- OpenAI’s o1 and Anthropic’s Claude 3.5 Sonnet for coding assistance, often by prompting directly (when operating at the conceptual/design level). Also occasionally Cursor (when operating at the code level).  I hope never to have to code again without AI assistance! Claude 3.5 Sonnet is widely regarded as one of the best coding models. And o1 is incredible at planning and building more complex software modules, but you do have to learn to prompt it differently.\n\nOn top of all this, of course, I use many AI tools to manage agentic workflows, data ingestion, retrieval augmented generation, and so on. https://t.co/zpIxRSuky4 and our wonderful partners offer courses on many of these tools.\n\nMy personal software stack continues to evolve regularly. Components enter or fall out of my default stack every few weeks as I learn new ways to do things. So please don’t feel obliged to use the components I do, but perhaps some of them can be a helpful starting point if you are still deciding what to use. Interestingly, I have found most LLMs not very good at recommending a software stack. I suspect their training sets include too much “hype” on specific choices, so I don’t fully trust them to tell me what to use. And if you can be opinionated and give your LLM directions on the software stack you want it to build on, I think you’ll get better results.\n\nA lot of the software stack is still maturing, and I think many of these components will continue to improve. With my stack, I regularly build prototypes in hours that, without AI assistance, would have taken me days or longer. I hope you, too, will have fun building many prototypes!\n\n[Original text: https://t.co/cfQkXolEJk ]",
    "createdAt": "Thu Jan 09 17:20:09 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 453,
    "replyCount": 122,
    "likeCount": 3106,
    "quoteCount": 47,
    "viewCount": 290616,
    "bookmarkCount": 3192,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "利用 AI 辅助编码来构建软件原型 (software prototypes)，是快速探索大量想法和实现创新发明的重要途径。在这篇文章以及未来的系列文章中，我将与大家分享一些构建简单网络应用原型的最佳实践。本文的重点将围绕一个核心理念：在软件堆栈 (software stack) 的选择上要有自己的主张。\n\n我个人使用的软件堆栈每隔几周就会更新。当然，我的选择有许多优秀的替代方案，如果您能选定一套自己偏好的软件堆栈并熟悉其组件，您的开发效率将会大大提升。不过，作为示例，以下是我目前默认采用的技术栈：\n- **Python 和 FastAPI 用于构建网络托管 API**：我主要使用 Python 进行开发，所以这自然是我的首选。如果您是 JavaScript/TypeScript 开发者，您可能会做出不同的选择。我发现 FastAPI 非常易于使用，并且在部署基于 Python 的 Web 服务 (API) 时，其可扩展性表现出色。\n- **Uvicorn 用于在我的笔记本电脑上进行本地测试时运行后端应用程序服务器** (用于执行代码和提供网页)。\n- **云端部署方案**：小型应用可选择 Heroku，大型应用则使用 AWS Elastic Beanstalk (披露：我目前是 Amazon 的董事会成员)。市场上还有许多用于部署任务的服务，包括 HuggingFace Spaces、Railway、Google 的 Firebase、Vercel 等。其中大多数都运行良好，熟悉其中一两个将能显著简化您的开发流程。\n- **MongoDB 用于 NoSQL 数据库**：虽然传统的 SQL 数据库是卓越的工程成就，能够实现高效可靠的数据存储，但需要预先定义数据库结构 (或模式 Schema) 这一点，会减缓原型开发的进程。如果您确实追求开发速度和实现便捷性，那么将大部分数据存储到 NoSQL (非结构化或半结构化) 数据库（例如 MongoDB）中，能让您快速编写代码，并随后再详细规划如何使用这些数据。这种方式有时被称为**写入时模式** (schema-on-write)，与**读取时模式** (schema-on-read) 相对应，意味着在数据写入时才确定其结构，而非提前定义。但请注意，如果应用程序投入大规模生产，在许多场景下，结构更清晰的 SQL 数据库会表现得更为可靠和可扩展。\n- **OpenAI 的 o1 和 Anthropic 的 Claude 3.5 Sonnet 用于编码辅助**：通常在概念/设计层面通过直接提示 (prompting) 使用，偶尔也会在代码层面使用 Cursor。我真心希望今后 coding 都能有 AI 智能体的协助！Claude 3.5 Sonnet 被普遍认为是最好的编码模型之一。而 o1 在规划和构建更复杂的软件模块方面表现非凡，但您确实需要学习如何以不同的方式对其进行提示。\n\n当然，除此之外，我还使用了许多 AI 工具来管理 AI 智能体 (AI Agent) 工作流、数据摄入 (data ingestion)、检索增强生成 (Retrieval Augmented Generation) 等等。https://t.co/zpIxRSuky4 和我们出色的合作伙伴提供了许多关于这些工具的课程。\n\n我个人的软件堆栈会定期持续演进。每隔几周，随着我学习到新的做事方式，一些组件会进入或退出我的默认技术栈。因此，请大家不必拘泥于我所使用的组件，但如果您仍在选择，其中的某些或许能为您提供一个有益的起点。有趣的是，我发现大多数大语言模型 (Large Language Model, LLM) 在推荐软件堆栈方面表现并不理想。我怀疑它们的训练集包含了太多关于特定选择的“过度宣传”，所以我并不完全信任它们给出的建议。而如果您能有自己的主见，并向大语言模型明确指示您希望它基于的软件堆栈，我相信您会获得更好的结果。\n\n许多软件堆栈仍在不断成熟，我相信这些组件中的许多都将持续改进。借助我这套技术栈，我经常能在几小时内构建出原型，而如果缺少 AI 辅助，这可能需要数天甚至更长的时间。我也希望您能在构建众多原型的过程中享受乐趣！\n\n[原文链接: https://t.co/cfQkXolEJk ]"
  },
  {
    "id": "1877075439283482815",
    "url": "https://x.com/AndrewYNg/status/1877075439283482815",
    "text": "New short course: Build Long-Context AI Apps with Jamba. Learn about state space models (SSMs), which have emerged as an alternative to transformers!  Specifically, Jamba is a hybrid transformer-Mamba architecture that combines strengths of the transformer with ideas from SSMs. This course is built with  @AI21Labs and taught by @chenwai21 and @AlmagorChen.\n\nThe transformer architecture is computationally expensive when handling very long input contexts. But there's an alternative called Mamba, a selective state space model that can process very long contexts with a much lower computational cost. However, researchers found that the pure Mamba architecture underperforms in understanding the context, and gives lower-quality responses. To overcome this, AI21 developed the Jamba model, which combines Mamba's computational efficiency with the transformer's attention mechanism to help with the output quality.\n\nIn this course, you’ll learn about how state space models, and Jamba, work. You’ll also learn how to prompt Jamba, use it to process long documents, and build long-context RAG apps.\n\n- Learn how Jamba combines transformer and state space model architectures to achieve high performance and quality  \n- Use the AI21 SDK, with an example of prompting over a large 200k-token annual financial report of Nvidia \n- Use Jamba for tool-calling, with hands-on examples from calling simple arithmetic calculations to a function that returns quarterly company financial reports.\n- Learn how training for long context is done, and the metrics used for its evaluation \n- Create a RAG app using the AI21 Conversational RAG tool and build your own RAG pipeline that uses Jamba and LangChain.\n\nBy the end of this course, you'll learn how to build applications that can handle context as long as an entire book.\n\nPlease sign up here: https://t.co/qc6St7zK9g",
    "createdAt": "Wed Jan 08 19:30:33 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 149,
    "replyCount": 45,
    "likeCount": 803,
    "quoteCount": 11,
    "viewCount": 76546,
    "bookmarkCount": 586,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "新短期课程：使用 Jamba 构建长上下文 AI 应用。了解状态空间模型 (SSMs)，它已成为 Transformer 的一种替代方案。具体来说，Jamba 是一种混合式的 Transformer-Mamba 架构，它结合了 Transformer 的优势和 SSMs 的核心理念。本课程由 @AI21Labs 打造，并由 @chenwai21 和 @AlmagorChen 授课。\n\nTransformer 架构在处理超长输入上下文时计算成本高昂。不过，有一种名为 Mamba 的替代方案，它是一种选择性状态空间模型 (selective state space model)，能以低得多的计算成本处理极长的上下文。然而，研究人员发现纯 Mamba 架构在理解上下文方面的表现不尽如人意，并会生成质量较低的响应。为了克服这一问题，AI21 开发了 Jamba 模型，它将 Mamba 的计算效率与 Transformer 的注意力机制相结合，从而提高了输出质量。\n\n在本课程中，您将学习状态空间模型以及 Jamba 的工作原理。您还将学习如何向 Jamba 提示 (prompt)、使用它处理长文档，并构建长上下文的检索增强生成 (RAG) 应用。\n\n-   学习 Jamba 如何结合 Transformer 和状态空间模型架构，以实现高性能和高质量\n-   使用 AI21 SDK，例如通过对 Nvidia 包含 200k Token 的大型年度财务报告进行提示\n-   利用 Jamba 进行工具调用，从调用简单的算术计算到调用返回季度公司财务报告的函数，都提供实际操作示例\n-   了解长上下文模型的训练过程及其评估指标\n-   使用 AI21 对话式 RAG 工具创建 RAG 应用，并构建自己的基于 Jamba 和 LangChain 的 RAG 管道\n\n在本课程结束时，您将学会如何构建能够处理相当于一整本书长度的上下文的应用程序。\n\n请在此处注册：https://t.co/qc6St7zK9g"
  },
  {
    "id": "1876701823840776521",
    "url": "https://x.com/AndrewYNg/status/1876701823840776521",
    "text": "Where is AI going? Six leaders share their hopes for AI in the coming year, in The Batch:\n- Hanno Basse: Generative AI for Artists\n- David Ding: Generated Video With Music, Sound Effects, and Dialogue\n- Joseph Gonzalez: General Intelligence\n- Albert Gu: More Learning, Less Data\n- Mustafa Suleyman: Agents of Action\n- Audrey Tang: AI That Unites Us\n\nThank you @BasseHanno , @DavidDingAI, @profjoeyg, @_albertgu, @mustafasuleyman and @audreyt for writing these!  \n\nRead them here: https://t.co/YgfCpE6FL8",
    "createdAt": "Tue Jan 07 18:45:56 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 219,
    "replyCount": 57,
    "likeCount": 1158,
    "quoteCount": 13,
    "viewCount": 135246,
    "bookmarkCount": 706,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "AI (人工智能) 将走向何方？六位业界领袖在《The Batch》杂志中分享了他们对未来一年 AI 发展的展望：\n- Hanno Basse: 面向艺术家的生成式 AI (Generative AI)\n- David Ding: 生成式视频，包含音乐、音效和对话\n- Joseph Gonzalez: 通用智能\n- Albert Gu: 更高效的学习，更少的数据需求\n- Mustafa Suleyman: AI 智能体 (AI Agent) 的行动力\n- Audrey Tang: 能够凝聚人心的 AI\n\n感谢 Hanno Basse ( @BasseHanno )、 David Ding ( @DavidDingAI )、 Joseph Gonzalez ( @profjoeyg )、 Albert Gu ( @_albertgu )、 Mustafa Suleyman ( @mustafasuleyman ) 和 Audrey Tang ( @audreyt ) 撰写的这些文章！\n\n点击此处阅读全文：https://t.co/YgfCpE6FL8"
  },
  {
    "id": "1876402210931867825",
    "url": "https://x.com/AndrewYNg/status/1876402210931867825",
    "text": "Hanging out ⁦⁦with @astroteller⁩ at Google X reminiscing about the early days of Google Brain! https://t.co/2j2QWnHiTz",
    "createdAt": "Mon Jan 06 22:55:23 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 41,
    "replyCount": 23,
    "likeCount": 440,
    "quoteCount": 3,
    "viewCount": 45532,
    "bookmarkCount": 21,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "和 @astroteller 在 Google X 聚会，一起回忆 Google Brain 的早期时光！https://t.co/2j2QWnHiTz"
  },
  {
    "id": "1874923856835772811",
    "url": "https://x.com/AndrewYNg/status/1874923856835772811",
    "text": "@nickclegg Thank you for your work championing open source to policymakers @nickclegg -- this has made a real difference!",
    "createdAt": "Thu Jan 02 21:00:56 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 0,
    "replyCount": 3,
    "likeCount": 40,
    "quoteCount": 0,
    "viewCount": 32247,
    "bookmarkCount": 7,
    "source": "",
    "lang": "en",
    "isReply": true,
    "isPinned": false,
    "tranlastedContent": "@nickclegg 感谢您向政策制定者们大力推广开源的工作，这真的发挥了巨大作用！"
  },
  {
    "id": "1874922734444236810",
    "url": "https://x.com/AndrewYNg/status/1874922734444236810",
    "text": "Despite having worked on AI since I was a teenager, I’m now more excited than ever about what we can do with it, especially in building AI applications. Sparks are flying in our field, and 2025 will be a great year for building!\n\nOne aspect of AI that I’m particularly excited about is how easy it is to build software prototypes. AI is lowering the cost of software development and expanding the set of possible applications. While it can help extend or maintain large software systems, it shines particularly in building prototypes and other simple applications quickly.\n\nIf you want to build an app to print out flash cards for your kids (I just did this in a couple of hours with o1’s help), or write an application that monitors foreign exchange rates to manage international bank accounts (a real example from https://t.co/zpIxRSuky4’s finance team), or analyzes user reviews automatically to quickly flag problems with your products (https://t.co/zpIxRSuky4's content team does this), it is now possible to build these applications quickly through AI-assisted coding.\n\nI find AI-assisted coding especially effective for prototyping because (i) stand-alone prototypes require relatively little context and software integration and (ii) prototypes in alpha testing usually don’t have to be reliable. While generative AI also helps with engineering large, mission-critical software systems, the improvements in productivity there aren't as dramatic, because it’s challenging to give the AI system all the context it needs to navigate a large codebase and also to make sure the generated code is reliable (for example, covering all important corner cases).\n\nUntil now, a huge friction point for getting a prototype into users’ hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed — sometimes using an AI-enabled IDE like Cursor — and finally assemble the modules.)\n\nBuilding prototypes quickly is an efficient way to test ideas and get tasks done. It’s also a great way to learn. Perhaps most importantly, it’s really fun! (At least I think it is. 😄)\n\nHow can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:\n- Make a learning plan! To be effective builders, we all need to keep up with the exciting changes that continue to unfold. How many short courses a month do you want to take in 2025? If you discuss your learning plan with friends, you can help each other along. For instance, we launched a learning summary page that shows what short courses people have taken. A few https://t.co/zpIxRSuky4 team members have agreed to a friendly competition to see who can take more courses in 2025!\n- Go build! If you already know how to code, I encourage you to build prototypes whenever inspiration strikes and you have a spare moment. And if you don’t yet code, it will be well worth your while to learn. Even small wins — like the flash cards I printed out, which inspired my daughter to spend an extra 20 minutes practicing her multiplication table — make life better. Perhaps you’ll invent something that really takes off. And even if you don’t, you’ll have fun and learn a lot along the way.\n\n[Original text: https://t.co/YgfCpE6FL8 ]",
    "createdAt": "Thu Jan 02 20:56:28 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 523,
    "replyCount": 126,
    "likeCount": 3360,
    "quoteCount": 40,
    "viewCount": 287241,
    "bookmarkCount": 1916,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "尽管我从少年时代就开始接触人工智能 (AI) ，但现在，我对 AI 能带来的可能性，尤其是在构建 AI 应用程序方面，比以往任何时候都更加兴奋。我们的领域正迸发出耀眼的火花，2025 年必将是大展宏图的一年！\n\nAI 让我特别激动的一个方面，是它能让软件原型的开发变得如此便捷。AI 正在显著降低软件开发的成本，并极大地拓宽了应用程序的应用范围。虽然 AI 可以帮助扩展或维护大型软件系统，但它在快速构建原型和其他简单应用方面表现得尤为出色。\n\n如果你想开发一个应用程序，为你的孩子打印抽认卡（我最近就在 o1 的帮助下，只用了几个小时就做好了），或者编写一个程序来监控外汇汇率以管理国际银行账户（这是来自 https://t.co/zpIxRSuky4 财务团队的一个真实案例），又或是自动分析用户评论来快速发现产品问题（https://t.co/zpIxRSuky4 的内容团队就是这样做的），现在，借助 AI 辅助编码，这些应用程序都可以快速构建出来。\n\n我发现 AI 辅助编码对原型开发特别有效，原因有二：(i) 独立的（指不需要与现有复杂系统深度集成的）原型通常只需要相对较少的上下文和软件集成；(ii) 处于内部测试 (alpha testing) 阶段的原型通常对可靠性要求不高。虽然生成式 AI (Generative AI) 也能辅助开发大型、任务关键型软件系统，但在这方面的生产力提升就没有那么显著了。这是因为要为 AI 系统提供导航庞大代码库所需的所有上下文，并确保生成代码的可靠性（例如，覆盖所有重要的边缘情况），仍然是一个巨大的挑战。\n\n在此之前，将原型交付给用户的一大障碍是部署。像 Bolt、Replit Agent、Vercel V0 这样的平台，正利用生成式 AI 结合 AI 智能体 (AI Agent) 工作流来提升代码质量，更重要的是，它们还能直接部署所生成的应用程序。（虽然我发现这些系统很有用，但我自己的工作流程通常会使用一个大语言模型 (LLM) 来设计系统架构，然后生成代码——如果涉及多个大型模块，我会一次生成一个。接着，我逐个测试每个模块，如果需要还会进一步编辑代码——有时会借助像 Cursor 这样集成 AI 功能的集成开发环境 (IDE)——最后将这些模块组装起来。）\n\n快速构建原型是验证想法和完成任务的有效途径。同时，它也是一个绝佳的学习方式。或许最重要的是，它真的很有趣！（至少我个人是这么认为的。😄）\n\n那么，在新的一年里，你该如何抓住这些机遇呢？当你制定新年计划时，我希望你能：\n- **制定学习计划！** 要想成为高效的构建者，我们都需要不断学习，跟上持续涌现的激动人心的变化。你希望在 2025 年每月参加多少短期课程？如果你与朋友讨论学习计划，你们可以互相督促、共同进步。例如，我们推出了一项学习总结页面，展示了人们参加过的短期课程。一些 https://t.co/zpIxRSuky4 团队成员已经同意进行一场友好的比赛，看看谁能在 2025 年参加更多课程！\n- **动手构建！** 如果你已经懂得编码，我鼓励你在灵感来袭、有空闲时间时，就去构建原型。如果你还没有开始编码，那么学习它将是非常值得的。即使是微小的成功——比如我为女儿打印的那些抽认卡，它激励她额外花 20 分钟练习乘法口诀——也能让生活变得更美好。也许你会发明出一些真正风靡起来的东西。即便没有，你也会在这个过程中收获乐趣并学到很多东西。\n\n[原文：https://t.co/YgfCpE6FL8 ]"
  },
  {
    "id": "1874495593827156120",
    "url": "https://x.com/AndrewYNg/status/1874495593827156120",
    "text": "Happy sum(i**3 for i in range(10)) !",
    "createdAt": "Wed Jan 01 16:39:10 +0000 2025",
    "author.profilePicture": "https://pbs.twimg.com/profile_images/733174243714682880/oyG30NEH_normal.jpg",
    "retweetCount": 307,
    "replyCount": 98,
    "likeCount": 3050,
    "quoteCount": 50,
    "viewCount": 266970,
    "bookmarkCount": 133,
    "source": "",
    "lang": "en",
    "isReply": false,
    "isPinned": false,
    "tranlastedContent": "祝贺 sum(i**3 for i in range(10)) ！"
  },
  {
    "id": -1,
    "text": "Since you are a free user, you can only access a maximum of 15 tweets. Please upgrade to a paid user to unlock access to all tweets.",
    "tranlastedContent": "由于您是免费用户，因此最多只能访问 15 条推文。请升级为付费用户，以解锁并查看所有推文。"
  }
]