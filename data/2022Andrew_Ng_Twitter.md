# Andrej Karpathy Twitter 2022

本文件包含Andrej Karpathy在2022年的所有推文。

总计推文数量: 680


### 001

作者: @karpathy
时间: 2022-01-03
链接: https://x.com/karpathy/status/1478128891520688128
互动: Likes: 889; Retweets: 27; Replies: 51; Quotes: 9; Views: 0; Bookmarks: 33; isReply: 0

github copilot but for art ✨

想象一下，有了 GitHub Copilot 这样的工具，但它服务的对象是艺术创作，那会是怎样一种体验 ✨。

### 002

作者: @karpathy
时间: 2022-01-03
链接: https://x.com/karpathy/status/1478131123746062336
互动: Likes: 48; Retweets: 2; Replies: 9; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@billionlols Sadly problem with music is that vision has massive throughput, while audio is like sucking information through a straw. So it is much faster to iterate with a model in the loop visually. Not that it can't be done.

@billionlols 遗憾的是，在音乐领域，视觉（信息处理）拥有巨大的吞吐量，而音频处理则像通过吸管一点点地获取信息。因此，通过视觉方式与模型进行迭代（即在模型训练或调整过程中，通过视觉反馈快速进行修改和优化）要快得多。但这并非意味着不可能实现（音频领域的快速迭代）。

### 003

作者: @karpathy
时间: 2022-01-05
链接: https://x.com/karpathy/status/1478564065660207105
互动: Likes: 131; Retweets: 11; Replies: 7; Quotes: 5; Views: 0; Bookmarks: 10; isReply: 1

@giffmana @PreetumNakkiran @francoisfleuret PyTorch is succumbing to entropy at an alarming rate and I’m not sure has internalized what made everyone switch to it from tensorflow

@giffmana @PreetumNakkiran @francoisfleuret PyTorch 正以惊人的速度变得混乱和无序（succumbing to entropy），我不确定它是否已经领会了当初让所有人从 TensorFlow 转向它的原因。

### 004

作者: @karpathy
时间: 2022-01-12
链接: https://x.com/karpathy/status/1481374403854094337
互动: Likes: 331; Retweets: 34; Replies: 20; Quotes: 3; Views: 0; Bookmarks: 38; isReply: 0

Interesting read and pointers; I've always wondered why the Roman Empire did not industrialize

这篇读物很有意思，也提供了不少启发；我一直很好奇为什么罗马帝国没有工业化。

### 005

作者: @karpathy
时间: 2022-01-13
链接: https://x.com/karpathy/status/1481721178955796480
互动: Likes: 275; Retweets: 34; Replies: 7; Quotes: 2; Views: 0; Bookmarks: 66; isReply: 0

Agree with disagree and encourage people to head over to this growing r/MachineLearning thread  https://t.co/izLFKraQC5 to battle it out ⚔️. In all seriousness though, v important to understand the numerous and not exactly intuitive caveats to comparisons of deep learning models

无论是否同意，都欢迎大家前往这个热度不断上升的 r/MachineLearning 帖子 https://t.co/izLFKraQC5 参与讨论 ⚔️。不过言归正传，在比较深度学习（Deep Learning）模型时，理解其中大量且并不直观的注意事项至关重要。

### 006

作者: @karpathy
时间: 2022-01-13
链接: https://x.com/karpathy/status/1481724509795221505
互动: Likes: 169; Retweets: 13; Replies: 8; Quotes: 3; Views: 0; Bookmarks: 8; isReply: 1

One dimension that is less frequently talked about (and that e.g. we care a lot about) is deployment-time simplicity and operator use. E.g. if ReLU == GeLU, former is much preferred (simplicity). If BN ~= LN, former is much preferred (can be folded into weights at test time) etc

一个较少被提及的维度（例如我们非常关注的）是模型部署时的简便性以及操作人员的使用便利性。举例来说，如果 ReLU 等同于 GeLU，那么 ReLU 会因为其简易性而更受青睐。如果批量归一化（Batch Normalization，BN）的效果近似于层归一化（Layer Normalization，LN），那么 BN 会更受青睐，因为它可以在测试阶段并入模型权重中，从而简化部署。

### 007

作者: @karpathy
时间: 2022-01-14
链接: https://x.com/karpathy/status/1482106214435606528
互动: Likes: 91; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 5; isReply: 1

@jon_barron wow, this paper 👏🤯. Impressive combo of architecture and hand-crafted bare metal execution. I'm still not sure that I intuitively feel that this should work so well+fast.

@jon_barron 哇，这篇论文 👏🤯。它将架构（architecture）和手工优化的裸机执行（bare metal execution）令人惊叹地结合起来。我仍然不太能直观地理解它为什么能做到如此出色和快速。

### 008

作者: @karpathy
时间: 2022-01-14
链接: https://x.com/karpathy/status/1482125394526363648
互动: Likes: 289; Retweets: 7; Replies: 10; Quotes: 2; Views: 0; Bookmarks: 7; isReply: 1

@rice_fry @greentheonly :) close! (on the broad strokes). sadly something that looks 90% there is deceivingly nowhere near 90% complete because each 9 is about the same amount of work and 99.99... is required, and then across the full diversity of the world. As E mentioned come help! :)

@rice_fry @greentheonly :）说得差不多了！（方向是对的）。不过遗憾的是，有些事情看着好像完成了 90%，但实际上离 90% 完成还差得远。因为要从 90% 做到 99%、从 99% 做到 99.9%…… 每个「9」的提升都需要付出同样多的努力，而我们往往需要做到 99.99% 甚至更高，并且还要能应对全球的各种复杂多样性。就像 E 说的，快来帮忙吧！ :)

### 009

作者: @karpathy
时间: 2022-01-14
链接: https://x.com/karpathy/status/1482126630835261441
互动: Likes: 112; Retweets: 3; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 2; isReply: 1

@rice_fry @greentheonly (you're also missing the semantic layer; in a "quick project" setting you could e.g. run (or train) an off-the-shelf detectron2 so you can separate out and reason about the road surface, static infra / signs / lights, dynamic objects, etc etc.)

@rice_fry @greentheonly（你们还遗漏了一个关键的语义层（semantic layer)；在一个「快速项目」的情境下，例如，你可以运行（或训练）一个现成的 Detectron2 模型，这样就能将路面、静态基础设施（例如标志和灯光）、以及动态物体等信息区分开来并进行识别和分析。)

### 010

作者: @karpathy
时间: 2022-01-15
链接: https://x.com/karpathy/status/1482254042625568770
互动: Likes: 42; Retweets: 1; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@rice_fry @L_Berlin @greentheonly @JimTanaka1 It’s a spiral staircase thing

@rice_fry @L_Berlin @greentheonly @JimTanaka1 这就像一个螺旋楼梯（一样复杂或需要换个角度看）

### 011

作者: @karpathy
时间: 2022-01-15
链接: https://x.com/karpathy/status/1482257744983838721
互动: Likes: 30; Retweets: 2; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 4; isReply: 1

@j_brorsson @rice_fry @L_Berlin @greentheonly @JimTanaka1 Just like MCTS is an improvement operator for an AlphaGo policy, an autolabeler is an improvement operator for a detector, leveraging the special structure of a physical temporally continuous 3D environment.

@j_brorsson @rice_fry @L_Berlin @greentheonly @JimTanaka1 就像蒙特卡洛树搜索（MCTS）是 AlphaGo 策略的一种改进算子（improvement operator）一样，自动标注器（autolabeler）也是检测器（detector）的一种改进算子，它充分利用了物理世界中三维环境在时间上连续的特殊结构。

### 012

作者: @karpathy
时间: 2022-01-23
链接: https://x.com/karpathy/status/1485317439000768512
互动: Likes: 1; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@michalwols @wightmanr all of its architectures cluster in the very low latency regime, would be curious to see it extrapolated further

@michalwols @wightmanr 它的所有架构都集中在极低的延迟范围内，很想看看它进一步推断会是什么样子

### 013

作者: @karpathy
时间: 2022-01-23
链接: https://x.com/karpathy/status/1485319506431918080
互动: Likes: 7; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@michalwols @wightmanr Yes I forked it (ty for the notebook!), but the largest LeViT model (LeViT-384) caps out at only 50ms infer_step_time. I understand this is the biggest model in the paper, but it could be possible to continue the extrapolation, curious how it does in bigger model regimes.

@michalwols @wightmanr 是的，我把它 fork 了 （谢谢你的 notebook！），但是最大的 LeViT 模型 （LeViT-384）的推理步长 （infer_step_time）最多只有 50 毫秒。我理解这是论文里最大的模型了，但是否有可能继续进行性能推断，我很好奇它在规模更大的模型上会有怎样的表现。

### 014

作者: @karpathy
时间: 2022-01-26
链接: https://x.com/karpathy/status/1486215976559398915
互动: Likes: 6,169; Retweets: 416; Replies: 154; Quotes: 47; Views: 0; Bookmarks: 72; isReply: 0

Everybody gangsta until real-world deployment in production.
(OH in a chat somewhere a while ago :D)

嘴上人均大佬，直到产品真的上线。
(OH 在某个聊天中说的，很久以前 :D)

### 015

作者: @karpathy
时间: 2022-01-28
链接: https://x.com/karpathy/status/1486859130228785152
互动: Likes: 36; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@AndrewLBeam @SkyLi0n 😂😂 this is spot on

@AndrewLBeam @SkyLi0n 😂😂 说得太对了！

### 016

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487482460195540994
互动: Likes: 34; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@algoritmic @hardmaru wow, so beautiful 😍 very organic

@algoritmic @hardmaru 哇，真美啊 😍 太有生命力了 / 浑然天成

### 017

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487488545287671814
互动: Likes: 1,618; Retweets: 153; Replies: 67; Quotes: 14; Views: 0; Bookmarks: 99; isReply: 0

My mind was randomly re-blown by this realization few days ago 🤯. Clocks can go to ~5GHz (so light travels ~7cm/clock) and chips are around ~3cm on a side (4.2cm diagonal), so if you like global clocks we're actually running chips very near fundamental limits of communication.

几天前，我突然意识到一个惊人的事实，这让我感到非常震撼 🤯。时钟频率可以达到约 5GHz （因此，光在一个时钟周期内传播大约 7 厘米），而芯片的边长约为 3 厘米 （对角线长 4.2 厘米）。这意味着，如果采用全局时钟（global clock）架构，我们目前芯片的运行速度实际上已经非常接近物理通信的极限了。

### 018

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487506153114267653
互动: Likes: 640; Retweets: 92; Replies: 21; Quotes: 7; Views: 0; Bookmarks: 80; isReply: 0

Physical Neural Nets https://t.co/khysuYtbss  fascinating area; neural nets in modern chips are running very far up from physics - many electrons are orchestrated to switch many transistors for many multiply-accumulates of (digitally represented!) numbers for many neurons. https://t.co/p2H2J3aiHO

物理神经网络 https://t.co/khysuYtbss 这真是个引人入胜的领域。我们现在芯片里运行的神经网络（neural nets），其实离底层的物理原理已经很远了 —— 为了模拟处理许多神经元（neuron）的复杂运算，现代芯片需要精心协调大量电子来控制晶体管（transistor）的开关，从而完成对那些以数字形式表示的数值进行大量的乘积累加运算（multiply-accumulate）。https://t.co/p2H2J3aiHO

### 019

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487506636411269121
互动: Likes: 192; Retweets: 7; Replies: 11; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

This is very general, flexible, easily re-configurable compute, but highly inefficient due to all the virtualization. Should be very possible to lower neural nets (or similar dynamical systems) much closer to physics.

这是一种非常通用、灵活、易于重新配置的计算方式，但由于大量虚拟化的存在，其效率极低。应该很有可能将神经网络（neural nets）或类似的动力系统（dynamical systems）推向更接近物理实现。

### 020

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487511477149831169
互动: Likes: 4,037; Retweets: 259; Replies: 209; Quotes: 31; Views: 0; Bookmarks: 37; isReply: 0

What does it look like when the cost of intelligence per watt plummets

当智能的每瓦特成本（cost of intelligence per watt）急剧下降时，世界会呈现出怎样一番景象？

### 021

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487514833960329216
互动: Likes: 64; Retweets: 0; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@maxhodak_ Yes. Tempted by some answers around rate of change of entropy

@maxhodak_ 是的。我有点倾向于选择那些与熵变化率相关的答案。

### 022

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487519221617606659
互动: Likes: 118; Retweets: 9; Replies: 11; Quotes: 1; Views: 0; Bookmarks: 5; isReply: 1

@chris_baynes it is not wasted, it is powering a set of unprecedented benefits stemming from a solution to distributed consensus that people obviously find useful.

@chris_baynes 这并没有被浪费，它正带来一系列前所未有的益处，这些益处都源于一个人们显然觉得有用的分布式共识解决方案。

### 023

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487520668660817921
互动: Likes: 81; Retweets: 1; Replies: 9; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@ranig I love The Matrix but this idea is dumb

@ranig 我很喜欢《黑客帝国》，但这个想法太蠢了。

### 024

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487521665101668352
互动: Likes: 37; Retweets: 2; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 2; isReply: 1

@ranig bio tech may well be a great alternative. should be possible to grow brain organoids in a dish, feed it sugar, and coerce it to run desirable functions

ranig 生物技术或许会是一个很好的替代方案。未来有望在培养皿中培养出脑类器官（brain organoids），通过提供糖分来「喂养」它们，并引导它们执行我们所需的功能。

### 025

作者: @karpathy
时间: 2022-01-29
链接: https://x.com/karpathy/status/1487523374410592256
互动: Likes: 9; Retweets: 2; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@Zulhihi actually deep and mostly correct, imo

@Zulhihi 实际上很深刻，而且大体上是正确的，我认为。

### 026

作者: @karpathy
时间: 2022-01-31
链接: https://x.com/karpathy/status/1488048392622116864
互动: Likes: 1,017; Retweets: 86; Replies: 39; Quotes: 10; Views: 0; Bookmarks: 84; isReply: 0

It takes 179 NAND gates to (naively) add two uint8 numbers. Below seen adding 103 + 79 = 182 (mod 256). (Am trying to train a neural net using only NANDs; first have to build the integers, then floats, then neurons. Probably takes a lot more NANDs...) code https://t.co/IJXbbWtWqv https://t.co/21u8F85qTP

要（简单地）实现两个 uint8 （8 位无符号整数）数字的加法运算，需要 179 个与非门（NAND gates）。这里展示了 103 + 79 = 182（mod 256）的计算过程（即结果在 0 到 255 之间循环）。（我正在尝试仅使用与非门来训练神经网络；首先得用与非门搭建出整数运算功能，接着是浮点数运算，最后才能构建神经元。这可能需要更多与非门……）代码 https://t.co/IJXbbWtWqv https://t.co/21u8F85qTP

### 027

作者: @karpathy
时间: 2022-01-31
链接: https://x.com/karpathy/status/1488053208094109697
互动: Likes: 128; Retweets: 3; Replies: 13; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@hardmaru building the whole thing from nands makes you realize how totally inefficient it is. A single digital fp32 MAC mobilizes a huge amount of physics.

@hardmaru 如果从最基本的与非门（NANDs）开始构建整个系统，你会意识到它的效率有多么低下。一个单独的数字浮点 32 位乘加运算（fp32 MAC）背后，都涉及了巨大的物理资源和实现。

### 028

作者: @karpathy
时间: 2022-02-01
链接: https://x.com/karpathy/status/1488335570556231682
互动: Likes: 9; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@marktenenholtz 👍 aligns well with https://t.co/5lBy4J77aS

@marktenenholtz 👍 这与 https://t.co/5lBy4J77aS 非常吻合。

### 029

作者: @karpathy
时间: 2022-02-01
链接: https://x.com/karpathy/status/1488339308956160006
互动: Likes: 4; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@marktenenholtz I’m not sure I’d have to re-read it 😂

@marktenenholtz 我不太确定，我可能得再读一遍 😂

### 030

作者: @karpathy
时间: 2022-02-02
链接: https://x.com/karpathy/status/1488931112214667265
互动: Likes: 0; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@TViering have you tried the lite version? https://t.co/E4QhYpOgPh does it address your use cases?

@TViering 你有没有试过轻量版？ https://t.co/E4QhYpOgPh 它能解决你的使用场景吗？

### 031

作者: @karpathy
时间: 2022-02-04
链接: https://x.com/karpathy/status/1489682157110849536
互动: Likes: 17; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@yaroslavvb Haha this was me. I had my nice clean 100 line python script that just ran my python file in cloud, and was super happy with it. Great thread, deep issues.

@yaroslavvb 哈哈，说的就是我！我原本写了一个简洁的 100 行 Python 脚本，它能轻松地在云端运行我的 Python 文件，当时我可得意了。这个帖子太棒了，探讨的问题也很深刻。

### 032

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491452689825165314
互动: Likes: 2,016; Retweets: 259; Replies: 50; Quotes: 39; Views: 0; Bookmarks: 262; isReply: 0

Computer vision research feels a bit stagnating in a local minimum of 2D texture recognition on ImageNet, COCO etc. This is great but only step 1. Unlocking further progress needs new framework:
1) the data source has to become diverse videos, not individual frames from internet

计算机视觉领域的研究似乎陷入了一个局部最优解：停留在 ImageNet、COCO 等数据集上的二维纹理识别（2D texture recognition）。这虽然取得了不错的成果，但仅仅是万里长征的第一步。要实现更大的突破，我们需要一套新的研究范式：
1）数据源必须转向多样化的视频数据，而不是仅仅依赖来自互联网的单帧图像

### 033

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491452691142217730
互动: Likes: 419; Retweets: 20; Replies: 6; Quotes: 3; Views: 0; Bookmarks: 6; isReply: 1

2) ground truth is compiled from "offline tracker" 3D reconstructions, not human labeling. The reconstructions are aided by solutions from step 1. 
3) outputs are (NeRF-like) query-able scene representations, not 1-of-k class labels.

2）我们的「真值（ground truth）」数据并非人工标记所得，而是通过「离线追踪器（offline tracker）」进行三维重建后生成的。这些重建过程借助了步骤 1 的解决方案。
3）输出结果是（类似 NeRF 的）可查询场景表示（query-able scene representations），而非传统的 1-of-k 类标签（1-of-k class labels）。

### 034

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491452697492746242
互动: Likes: 490; Retweets: 20; Replies: 17; Quotes: 2; Views: 0; Bookmarks: 8; isReply: 1

Only by going through this path will we be able to point the camera back at simple internet images and not just see the "Egyptian cat" class, but condition on the image to instantiate full generative 3D reconstructions of worlds consistent with that observation.

只有通过这条途径，我们才能重新让相机审视简单的互联网图像，不再仅仅识别出「埃及猫」这一类别，而是能够根据图像，生成与我们观察到的场景一致的完整生成式 3D 世界重建（generative 3D reconstructions of worlds）。

### 035

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491452695705980934
互动: Likes: 336; Retweets: 13; Replies: 8; Quotes: 1; Views: 0; Bookmarks: 13; isReply: 1

( rant triggered by re-stumbling by the Replica Dataset and friends, which has the right flavor for the data generating component but is still quite early (e.g. small, simple indoor scene-constrained, no moving objects, etc etc.)  https://t.co/KomPAaeCff ) https://t.co/2Limj4uvjF

( 这段感想的缘起是再次看到了 Replica 数据集及其相关项目，它们在数据生成（data generating）方面方向是对的，但目前仍处于非常早期的阶段 （例如，规模小、仅限于简单的室内场景、没有移动物体等）。https://t.co/KomPAaeCff ） https://t.co/2Limj4uvjF

### 036

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491482919767924737
互动: Likes: 111; Retweets: 4; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@wightmanr Yes, everything complexifies and expensifies dramatically when handling video. Doubly true when one wants to "bake in" 3D geometry + temporal consistency. Because when I'm thinking "video" I'm not just thinking e.g. activity recognition work - same old 1-of-k but video.

@wightmanr 是的，处理视频时，一切都会变得异常复杂且成本高昂。如果想在其中「融入」3D 几何信息和时间上的一致性，情况就更是如此。因为当我提到「视频」时，我所想的不仅仅是像活动识别这类任务 —— 即便是那种经典的「K 选 1（1-of-k）」分类问题，一旦涉及到视频，其复杂程度也会大幅提升。

### 037

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491504637135884288
互动: Likes: 79; Retweets: 3; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@Fetu_Ethio I would change nothing. ImageNet was perfectly great and useful just the way it was, but is not the full story on computer vision.

@Fetu_Ethio 我完全同意，无需改变。ImageNet（ImageNet）本身就非常出色且有价值，但它并非计算机视觉领域的全部故事。

### 038

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491504637135884288
互动: Likes: 79; Retweets: 3; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@Fetu_Ethio I would change nothing. ImageNet was perfectly great and useful just the way it was, but is not the full story on computer vision.

@Fetu_Ethio 我认为没有必要做出任何改变。ImageNet 在其原有形式下就非常出色且实用，但它并非计算机视觉领域的全部图景。

### 039

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491504854165966850
互动: Likes: 13; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@MaloyanNarek sim may very well become the dark the horse of the computer vision data story :)

@MaloyanNarek 的模拟工作（sim）很可能成为计算机视觉数据领域的一匹黑马 :)

### 040

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491504854165966850
互动: Likes: 13; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@MaloyanNarek sim may very well become the dark the horse of the computer vision data story :)

@MaloyanNarek sim 很可能成为计算机视觉数据领域的一匹黑马 :)

### 041

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491508789501071362
互动: Likes: 469; Retweets: 16; Replies: 19; Quotes: 1; Views: 0; Bookmarks: 10; isReply: 1

(For Tesla followers - this is the approach we’ve been taking at the Autopilot for a long time, but I am hoping to see a lot more of it in academia as well)

(致特斯拉的追随者们 —— 这是我们 Autopilot 团队长期以来一直采用的方法，但我希望在学术界也能更多地看到这种方法)

### 042

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491508789501071362
互动: Likes: 469; Retweets: 16; Replies: 19; Quotes: 1; Views: 0; Bookmarks: 10; isReply: 1

(For Tesla followers - this is the approach we’ve been taking at the Autopilot for a long time, but I am hoping to see a lot more of it in academia as well)

(致 Tesla 关注者：这是 Autopilot 团队长期以来一直采用的方法，我希望能看到学术界也能更多地采纳这种方式。)

### 043

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491544421132750850
互动: Likes: 13; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@IBD_ECarson @TheBotOfTheBots I am not alluding to camera-based limitations. I am commenting on academic literature in computer vision and nudging it a bit in the direction that we've been working on Tesla for a while.

@IBD_ECarson @TheBotOfTheBots 我指的并非是基于摄像头（camera-based）的局限性。我实际上是在探讨计算机视觉（computer vision）领域的学术文献，并希望能将它稍作引导，使其更贴近我们在 Tesla 已经研究了一段时间的方向。

### 044

作者: @karpathy
时间: 2022-02-09
链接: https://x.com/karpathy/status/1491544421132750850
互动: Likes: 13; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@IBD_ECarson @TheBotOfTheBots I am not alluding to camera-based limitations. I am commenting on academic literature in computer vision and nudging it a bit in the direction that we've been working on Tesla for a while.

@IBD_ECarson @TheBotOfTheBots 我不是在提及摄像头固有的局限性。我是在讨论计算机视觉领域的学术文献，并希望将其稍微引导到我们 Tesla 团队已经深耕了一段时间的方向。

### 045

作者: @karpathy
时间: 2022-02-10
链接: https://x.com/karpathy/status/1491608794253512707
互动: Likes: 437; Retweets: 40; Replies: 24; Quotes: 14; Views: 0; Bookmarks: 135; isReply: 1

@ilyasut @vkhosla agree https://t.co/AGhQ8tOcaP consciousness is a useful insight for compression

@ilyasut @vkhosla 同意 https://t.co/AGhQ8tOcaP 意识对于压缩来说是一个有益的洞察。

### 046

作者: @karpathy
时间: 2022-02-11
链接: https://x.com/karpathy/status/1492006292201762820
互动: Likes: 9; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@sudeeppillai @ronnieclark__ @adnothing @fdellaert I suspect my earlier tweet will age well :)  https://t.co/Sd5I4uDuZS

@sudeeppillai @ronnieclark__ @adnothing @fdellaert 我觉得我早前发的推文会经得起时间考验 :）https://t.co/Sd5I4uDuZS

### 047

作者: @karpathy
时间: 2022-02-11
链接: https://x.com/karpathy/status/1492182291480604672
互动: Likes: 20; Retweets: 1; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@hardmaru @yujin_tang @alanyttian hahaha WaterWorld!! 🤩 this was loosely imagined as a primordial world in which a single-celled organism with simple photoreceptors and flagella collects food and avoids predators

@hardmaru @yujin_tang @alanyttian 哈哈哈水世界（WaterWorld)!! 🤩 这可以大致想象成一个原始世界，在那里，一个拥有简单光感受器（photoreceptors）和鞭毛（flagella）的单细胞生物，一边收集食物，一边躲避捕食者。

### 048

作者: @karpathy
时间: 2022-02-11
链接: https://x.com/karpathy/status/1492231977151336449
互动: Likes: 12; Retweets: 2; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@edpizzi @ilyasut @vkhosla err yes it's very much the opposite of that

@edpizzi @ilyasut @vkhosla 呃，是的，这恰恰与此相反。

### 049

作者: @karpathy
时间: 2022-02-12
链接: https://x.com/karpathy/status/1492316995043295233
互动: Likes: 3; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 0

@Chris_Maer @hardmaru Familiar with it ofc, like 👍

@Chris_Maer @hardmaru 当然熟悉，点赞 👍

### 050

作者: @karpathy
时间: 2022-02-12
链接: https://x.com/karpathy/status/1492326632673067009
互动: Likes: 363; Retweets: 43; Replies: 15; Quotes: 1; Views: 0; Bookmarks: 99; isReply: 0

Cool idea randomly on Twitter deserves more views. 1) Train both a conditioned and unconditioned autoregressive model by random masking, then 2) at test time you get a knob to interpolate (or better, extrapolate!) the conditioning strength, e.g. to "super-condition" a prediction.

在 Twitter 上偶然看到一个很酷的想法，它值得被更多人关注。1）首先，通过随机掩码（random masking）的方式，训练一个有条件和无条件自回归模型（autoregressive model)；2）然后，在测试时，你可以通过一个「旋钮」来调整（插值，甚至更好地，外推！）条件强度（conditioning strength），例如，将预测结果「超级条件化（super-condition）」。

### 051

作者: @karpathy
时间: 2022-02-16
链接: https://x.com/karpathy/status/1493982737325182976
互动: Likes: 70; Retweets: 4; Replies: 2; Quotes: 2; Views: 0; Bookmarks: 0; isReply: 1

@AMPRobotics you are one of my favorite applications of computer vision ♥️

@AMPRobotics 你是我最喜欢的计算机视觉（Computer Vision）应用之一 ♥️

### 052

作者: @karpathy
时间: 2022-02-16
链接: https://x.com/karpathy/status/1493985006951092225
互动: Likes: 768; Retweets: 46; Replies: 52; Quotes: 11; Views: 0; Bookmarks: 110; isReply: 0

A fun &amp; v feasible project idea for someone out there: bundle up face detection, speech recognition, GPT as the core "intelligence engine", text to speech, and face generative model to create a digital human you can talk to e.g. on webcam/phone (but it's just a "dressed up" GPT).

一个有趣且非常可行的项目想法，给有兴趣的开发者：将人脸检测（face detection）、语音识别（speech recognition）、以 GPT 作为核心「智能引擎」的大语言模型（Large Language Model）、文本生成语音（text to speech）和人脸生成模型（face generative model）这些技术整合起来，就可以创建一个你可以通过网络摄像头或手机与之对话的数字人（digital human）—— 本质上，它只是一个「包装」起来的 GPT。

### 053

作者: @karpathy
时间: 2022-02-16
链接: https://x.com/karpathy/status/1493988845766905856
互动: Likes: 261; Retweets: 2; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

Bonus points: point two of them at each other on Twitch :D

额外奖励：让它们俩在 Twitch 上互相「对战」吧！:D

### 054

作者: @karpathy
时间: 2022-02-16
链接: https://x.com/karpathy/status/1493989774096404483
互动: Likes: 289; Retweets: 4; Replies: 16; Quotes: 1; Views: 0; Bookmarks: 9; isReply: 1

Alternative bonus points: It's not a human but some cute animal "living" on your phone and now you have Tamagotchi++

还有一个额外有趣的设想：如果手机里「住」着的不是人类，而是一些可爱的动物，那你就相当于拥有了一个「拓麻歌子 ++」版本（即更高级的电子宠物）。

### 055

作者: @karpathy
时间: 2022-02-16
链接: https://x.com/karpathy/status/1493991284167434241
互动: Likes: 7; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@AlientrapGames ah yes I think I stumbled by this a long while ago. huh, do you know why?

@AlientrapGames 啊，是的，我好像很久以前偶然遇到过这个。嗯，你知道为什么吗？

### 056

作者: @karpathy
时间: 2022-02-17
链接: https://x.com/karpathy/status/1494166717269823492
互动: Likes: 294; Retweets: 41; Replies: 5; Quotes: 4; Views: 0; Bookmarks: 43; isReply: 0

Nice followup on our earlier OpenAI "World of Bits" work teaching AIs to use keyboard + mouse. Imo powerful to match AI "APIs" to those of humans bc the world is built for humans - gives completeness, incrementality, demonstration data. Applies in realms both digital and physical

这是对我们早期 OpenAI「World of Bits」项目的又一重要进展，该项目旨在教会 AI（人工智能）使用键盘和鼠标。我认为，让 AI 的「API」(应用程序接口）与人类的「API」相匹配具有强大意义，因为这个世界是为人类建造的。这样做能带来完整性、增量性，并提供丰富的演示数据。这一理念不仅适用于数字领域，也同样适用于物理世界。

### 057

作者: @karpathy
时间: 2022-02-17
链接: https://x.com/karpathy/status/1494178962422984706
互动: Likes: 316; Retweets: 41; Replies: 34; Quotes: 5; Views: 0; Bookmarks: 35; isReply: 0

Is simulation the dark horse of 99% of the training FLOPS in future "foundation models" of computer vision?

模拟会是未来计算机视觉（Computer Vision）「基础模型」（Foundation Models）中 99% 训练 FLOPS 的那匹「黑马」吗？

### 058

作者: @karpathy
时间: 2022-02-21
链接: https://x.com/karpathy/status/1495587537108168707
互动: Likes: 342; Retweets: 43; Replies: 35; Quotes: 7; Views: 0; Bookmarks: 42; isReply: 0

Do octopuses 🐙 come from outer space ☄️? I want to believe 🤞 https://t.co/2XYPXoqq8I

章鱼 🐙 真的来自外太空 ☄️ 吗？我很想相信这个说法 🤞 https://t.co/2XYPXoqq8I

### 059

作者: @karpathy
时间: 2022-02-26
链接: https://x.com/karpathy/status/1497652099701620738
互动: Likes: 934; Retweets: 109; Replies: 32; Quotes: 9; Views: 0; Bookmarks: 60; isReply: 0

Humans program each other by prompt engineering too, so it's interesting to see that form of programming becoming increasingly prevalent with computers. Programming turns into a kind of applied psychology of neural nets, biological or synthetic.

人类也会通过提示工程（prompt engineering）的方式来「编程」彼此，所以看到这种形式的编程在计算机领域变得越来越普遍，这确实很有趣。编程正逐渐演变为一种对神经网络（neural nets）的应用心理学，无论是生物神经网络还是合成神经网络，都遵循这一原理。

### 060

作者: @karpathy
时间: 2022-03-02
链接: https://x.com/karpathy/status/1499067317782663173
互动: Likes: 172; Retweets: 25; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 26; isReply: 0

V nice application paper! It's not just about clocks - love to see how different data sources (and their pros/cons) interact sensibly for the final system - some human data, some sim data, some "offline tracker" data. Same for Autopilot, and I expect many other applied problems.

这真是一篇非常出色的应用论文！它不仅仅是关于时钟，更令人欣喜的是，文章展示了不同数据源（及其优缺点）如何巧妙地协同作用，最终构建成一个完善的系统 —— 例如包含人工数据、模拟数据（sim data）和「离线追踪器（offline tracker）」数据。同样的方法也适用于 Autopilot，我认为许多其他实际应用问题也可以借鉴这种思路。

### 061

作者: @karpathy
时间: 2022-03-09
链接: https://x.com/karpathy/status/1501673630865846272
互动: Likes: 11; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@kenshirriff so interesting to get a visual feel for the dynamics! wish there was a 10min walkthrough of 1 layer down of what's being shown

@kenshirriff 能对这些动态有一个直观感受真是太有趣了！希望有一个 10 分钟的视频，能对所展示的内容再深入一层进行讲解。

### 062

作者: @karpathy
时间: 2022-03-12
链接: https://x.com/karpathy/status/1502726004325773313
互动: Likes: 99; Retweets: 6; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 10; isReply: 1

@jason_z_kim "modern-day silicon computers rely on binary representations, rapid sequential processing, and segregated memory and CPU, while neural computers utilize continuum representations, parallel and distributed processing, and distributed memory"✨:) looking fwd to code!

@jason_z_kim「现代硅基计算机的运行方式是：依赖二进制（binary）形式表示数据，进行快速的顺序处理，以及内存与中央处理器（CPU）的分离；而神经网络计算机则采用连续（continuum）表示，进行并行和分布式处理，并拥有分布式内存」✨:）期待代码！

### 063

作者: @karpathy
时间: 2022-03-14
链接: https://x.com/karpathy/status/1503211739021660162
互动: Likes: 1,714; Retweets: 102; Replies: 66; Quotes: 27; Views: 0; Bookmarks: 38; isReply: 1

TLDR a GPT-like Transformer is now predicting the lanes and their connectivity. This "direct to vector space" framework allows predictions to be jointly coherent (due to sequential conditioning) and v easily used by planner (due to sparsity). Excellent work from the team!🪄

简而言之，现在一种类似 GPT 的 Transformer 模型正在负责预测车道及其连通性。这种「直接映射到向量空间（vector space）」的框架，使得预测结果既能保持联合一致性（这得益于其对序列信息的条件处理），又因为稀疏性特点而能非常方便地被规划器（planner）所使用。团队的这项工作非常出色！🪄

### 064

作者: @karpathy
时间: 2022-03-14
链接: https://x.com/karpathy/status/1503211738195320833
互动: Likes: 880; Retweets: 27; Replies: 8; Quotes: 1; Views: 0; Bookmarks: 7; isReply: 1

"This enables us to predict crossing lanes, allows computationally cheaper and less error-prone post-processing, and paves the way for predicting many other signals and their relationships jointly and end-to-end."

这使我们能够预测车道交汇点，实现计算成本更低、出错率更低的后处理，并为端到端地共同预测许多其他信号及其相互关系奠定了基础。

### 065

作者: @karpathy
时间: 2022-03-14
链接: https://x.com/karpathy/status/1503211737046085634
互动: Likes: 1,851; Retweets: 178; Replies: 63; Quotes: 29; Views: 0; Bookmarks: 74; isReply: 0

FSD Beta 10.11 release notes. Fave item:
"Upgraded modeling of lane geometry from dense rasters (“bag of points”) to an autoregressive decoder that directly predicts and connects “vector space” lanes point by point using a transformer neural network."

FSD Beta 10.11 更新说明。其中最吸引人的亮点是：
「我们将车道几何的建模方式进行了升级，从过去那种基于密集栅格（也就是‘点集合'）的方法，转向使用一种自回归解码器。这个解码器借助 Transformer 神经网络（Transformer neural network），能够直接逐点地预测并连接‘向量空间'中的车道信息。」

### 066

作者: @karpathy
时间: 2022-03-14
链接: https://x.com/karpathy/status/1503394811188973569
互动: Likes: 2,090; Retweets: 392; Replies: 37; Quotes: 55; Views: 0; Bookmarks: 443; isReply: 0

New blog post!⬆️ Deep Neural Nets: 33 years ago and 33 years from now https://t.co/pbZvYh3Mck we reproduce what I think may be the earliest real-world application of a neural net trained end-to-end with backprop (LeCun et al. 1989), try improve it with time travel, and reflect. https://t.co/MKZ7S3GUdv

发布新博客文章了！⬆️ 文章主题是「深度神经网络（Deep Neural Nets)：33 年前与 33 年后」https://t.co/pbZvYh3Mck。在文中，我们重现了 LeCun 等人于 1989 年所做的工作，这可能是我所知的最早的、使用反向传播（Backprop）进行端到端（End-to-End）训练的神经网络（Neural Net）真实世界应用案例。我们还尝试「穿越时空」对其进行改进，并深入探讨了相关思考。https://t.co/MKZ7S3GUdv

### 067

作者: @karpathy
时间: 2022-03-14
链接: https://x.com/karpathy/status/1503399810342158337
互动: Likes: 37; Retweets: 1; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@moonares the part "optional step of data or model distillation into smaller, special-purpose inference networks" would imo address this, if the application demands it

@moonares ，「将数据或模型蒸馏（distillation）到更小、专用型推理网络中的这个可选步骤」，我认为能够解决这个问题，如果应用场景有此需求的话。

### 068

作者: @karpathy
时间: 2022-03-15
链接: https://x.com/karpathy/status/1503554542461407232
互动: Likes: 86; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@ylecun That would be very fun! :) 👍

@ylecun 那一定很有趣！ :）👍

### 069

作者: @karpathy
时间: 2022-03-15
链接: https://x.com/karpathy/status/1503748038971564034
互动: Likes: 11; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@syhw Nice!! I tried BN but I kept the per-example SGD (so really more of instance normalization), which didn’t help. But this is also v interesting to see, to decouple optimization from the upper bound capability of the baby neural net.

@syhw 太棒了！！我尝试了批归一化（BN），但我保留了针对每个样本的随机梯度下降（SGD）(所以这实际上更像是实例归一化（instance normalization)），但这并没有奏效。不过，看到这一点也非常有趣，它将优化过程与小型神经网络（baby neural net）的性能上限能力分离开来。

### 070

作者: @karpathy
时间: 2022-03-15
链接: https://x.com/karpathy/status/1503874225475567620
互动: Likes: 559; Retweets: 82; Replies: 21; Quotes: 4; Views: 0; Bookmarks: 159; isReply: 0

Excellent and unintuitive read on GPUs. The chip doing the compute has tiny amount of memory &amp; is connected to the main memory literally through a straw. Most of the energy goes to data movement too. Many repercussions. E.g. latency better predicted by # activations than # flops

这是一篇关于 GPU（图形处理器）的文章，内容精彩且出人意料。执行计算的芯片自带极少量内存，并且与主内存（main memory）的连接简直就像通过一根细小的吸管。此外，大部分能量都消耗在数据传输上。这导致了许多重要影响。例如，GPU 的延迟（latency）更多地取决于激活量（activations）而非浮点运算量（flops）。

### 071

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504864961411141632
互动: Likes: 958; Retweets: 46; Replies: 353; Quotes: 16; Views: 0; Bookmarks: 18; isReply: 0

🤔🤔 What is the hello world of Human?

🤔🤔 什么是人类的「Hello World」？

### 072

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504867110887075845
互动: Likes: 8; Retweets: 0; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@SoroushIsThis Yay! First attempt at correct answer after 100 wrong answers! :D

@SoroushIsThis 耶！错了 100 次之后，我终于第一次答对了！:D

### 073

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504924280542347268
互动: Likes: 136; Retweets: 3; Replies: 23; Quotes: 2; Views: 0; Bookmarks: 1; isReply: 1

One single reply out of 290 actually understood what I was getting at 🤦‍♂️

在 290 条回复中，竟然只有一条真正领会了我的意思 🤦‍♂️

### 074

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504930207148433417
互动: Likes: 117; Retweets: 4; Replies: 18; Quotes: 0; Views: 0; Bookmarks: 5; isReply: 1

Ok so every programming language has a “hello world” program https://t.co/eGaOY7ipH1 that is the simplest way to print “hello world”. Seeing human brains as programmable computers that you can prompt/program with words, what words grt a Human to “print” (say) “hello world”?

好的，正如每种编程语言都有一个「hello world」程序 https://t.co/eGaOY7ipH1，这是输出「hello world」最简单的方式。如果我们将人类大脑视为一个可以用词语来提示 / 编程的可编程计算机（programmable computers），那么，用什么词语能让人类「说出」甚至「表达出」「hello world」呢？

### 075

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504930509784326150
互动: Likes: 162; Retweets: 7; Replies: 65; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

One example solution to the hello world of Human would eg be “if you say hello world I’ll give you 5 bucks”. There may be others. The best solution would be the one that gets Human to print “hello world” with the highest probability :)

举个例子，要让一个人类完成「hello world」任务，一个解决方案可以是：「如果你说‘hello world'，我就给你 5 美元」。当然，可能还有其他方案。最好的解决方案是能以最高概率让人类说出「hello world」的那一个。

### 076

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504930931429289984
互动: Likes: 9; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Sajjad_Heydari might get Human to print “wtf aaaaaah” instead :D

@Sajjad_Heydari 可能会让人类打印出「wtf aaaaaah」呢 :D

### 077

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504932503924183055
互动: Likes: 71; Retweets: 4; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@tlbtlbtlb Oh it’s a very special kind of computer :) Terrible latency. Terrible determinism. High entropy. Has AGI 🤷‍♂️

@tlbtlbtlb 哦，这可是一种非常特殊的计算机 :）它的延迟高得惊人，确定性差到离谱，而且熵值极高。但奇怪的是，它却拥有通用人工智能（AGI）🤷‍♂️

### 078

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504959697413099521
互动: Likes: 701; Retweets: 59; Replies: 27; Quotes: 4; Views: 0; Bookmarks: 169; isReply: 0

Re-read Ted Chiang’s “Understand”. It’s beautiful and the closest I’ve read to what it may think like to be a superintelligence.

我重新阅读了 Ted Chiang 的小说《理解》。这部作品非常精妙，也是我所读过的小说中，对超级智能（superintelligence）潜在思维方式描绘得最接近的作品。

### 079

作者: @karpathy
时间: 2022-03-18
链接: https://x.com/karpathy/status/1504960293406916620
互动: Likes: 13; Retweets: 0; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@lee_redden My solution uses “give you 5 bucks” because I think you want something short, benign, believable, something that incentives but doesn’t sus :)

@lee_redden 我的解决方案采用了「给你 5 美元」的说法，因为我认为你想要的是一个简短、无害且可信的内容，既能起到激励作用，又不会让人产生怀疑 :)

### 080

作者: @karpathy
时间: 2022-03-19
链接: https://x.com/karpathy/status/1504975036037406722
互动: Likes: 6,441; Retweets: 771; Replies: 219; Quotes: 96; Views: 0; Bookmarks: 175; isReply: 0

I don’t think a regular person appreciates how insane it is that computers work. I propose we stare at each other mind-blown for about 1 hour/day, in small groups in circles around a chip on a pedestal, appreciating that we can coerce physics to process information like that.

我敢说，普通人很少能真正领会到计算机能正常运行是多么不可思议。我提议，我们每天花大约一小时，几个人一组，围着一个摆在基座上的芯片，互相凝视，为我们能够驾驭物理定律来处理信息而惊叹不已。

### 081

作者: @karpathy
时间: 2022-03-19
链接: https://x.com/karpathy/status/1504979792747274252
互动: Likes: 70; Retweets: 7; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 16; isReply: 1

@Smerity Things like https://t.co/XNyfqhCguE add to terror

@Smerity 像 https://t.co/XNyfqhCguE 这样的事情令人感到恐怖。

### 082

作者: @karpathy
时间: 2022-03-19
链接: https://x.com/karpathy/status/1504982631921991684
互动: Likes: 10; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@Smerity very fun reading btw, ty for pointer

@Smerity 顺便说一句，读起来很有意思，谢谢你的推荐。

### 083

作者: @karpathy
时间: 2022-03-19
链接: https://x.com/karpathy/status/1505204494128271364
互动: Likes: 541; Retweets: 28; Replies: 57; Quotes: 3; Views: 0; Bookmarks: 4; isReply: 0

You need an internet connection to play Tetris

玩俄罗斯方块需要互联网连接。

### 084

作者: @karpathy
时间: 2022-03-20
链接: https://x.com/karpathy/status/1505626778106306566
互动: Likes: 530; Retweets: 30; Replies: 45; Quotes: 3; Views: 0; Bookmarks: 34; isReply: 0

Much open source code looks more like the eukaryotic genome instead of bacterial plasmids, imo not ideal.

在我看来，许多开源代码的结构与其说像精简的细菌质粒，不如说更像庞大复杂的真核生物基因组，这并不理想。

### 085

作者: @karpathy
时间: 2022-03-20
链接: https://x.com/karpathy/status/1505632827848728584
互动: Likes: 9; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 4; isReply: 1

@ArtirKel @tommycollison It’s quite great. Personal faves are division by zero, story of your life (much better than movie), and liking what you see, then second book faves are Exhalation and What’s expected of us.

@ArtirKel @tommycollison 真是太棒了。我个人最喜欢的是《除以零》（division by zero）、《你一生的故事》（story of your life）（比电影版要好得多），以及《喜欢你所看到的》（liking what you see)；而第二本我特别喜欢的书则是《呼气》（Exhalation）和《我们所期望的》（What's expected of us）。

### 086

作者: @karpathy
时间: 2022-03-21
链接: https://x.com/karpathy/status/1506043952176279558
互动: Likes: 835; Retweets: 43; Replies: 62; Quotes: 6; Views: 0; Bookmarks: 11; isReply: 0

Reminder to check your gmail Spam folder once in a while. The quality of their spam detection has decreased lately (I think?) - a number of legitimate even important emails seem to go there now, and a lot of emails from friends get a scary warning, am asked to confirm "Look Safe"

提醒您：请时不时检查一下您的 Gmail 垃圾邮件文件夹。我感觉 Gmail 的垃圾邮件检测质量最近有所下降 —— 现在许多合法的甚至重要的邮件似乎都被错误地归到了垃圾邮件中。此外，很多来自朋友的邮件还会收到一个警告，并要求确认其「看起来安全」。
</step3_ref_translation>

### 087

作者: @karpathy
时间: 2022-03-22
链接: https://x.com/karpathy/status/1506111078610186241
互动: Likes: 9; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@MarcusKlarqvist I see. It has made A TON of progress, it just wasn’t all that algorithmic.

@MarcusKlarqvist 我明白了。它确实取得了巨大的进步，只是并非完全依靠算法。

### 088

作者: @karpathy
时间: 2022-03-23
链接: https://x.com/karpathy/status/1506701396975968258
互动: Likes: 535; Retweets: 17; Replies: 69; Quotes: 12; Views: 0; Bookmarks: 29; isReply: 0

Wanted to try training a neural net on GCP but my requests for GPU node quota keep getting instantly denied with no additional information ;(. I'm assuming other people out there have succeeded, though (?)...

我原本想在 GCP 上尝试训练一个神经网络，但申请 GPU 节点配额的请求却一直被立即拒绝，而且没有提供任何额外信息；(. 不过，我猜测应该有其他人成功做到了，是吗（?)...

### 089

作者: @karpathy
时间: 2022-03-23
链接: https://x.com/karpathy/status/1506735304916541443
互动: Likes: 70; Retweets: 1; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@ID_AA_Carmack Yes I was able to get some traction too, but I wish their messaging was better. I spent some time trying to read all the docs, browsed Reddit/stack overflow/google’d etc. it’s just an instant cryptic rejection, wasn’t sure if I was doing something wrong.

@ID_AA_Carmack 是的，我也取得了一些进展，但我希望他们的信息传达方式能更好。我花了一些时间尝试阅读所有文档，浏览了 Reddit、Stack Overflow 论坛，也在 Google 上搜索过等等。但结果总是立即收到一个令人费解的拒绝信息，我不确定自己是否做错了什么。

### 090

作者: @karpathy
时间: 2022-03-27
链接: https://x.com/karpathy/status/1507893647341142016
互动: Likes: 25,265; Retweets: 1,567; Replies: 564; Quotes: 320; Views: 0; Bookmarks: 602; isReply: 0

TikTok is scary good. It's digital crack. First time I feel attacked by AI in the brain.

TikTok 真是好得吓人。它就像是数字领域的「可卡因」，让人极度上瘾。我第一次感觉到自己的大脑受到了 AI（Artificial Intelligence）的「攻击」。

### 091

作者: @karpathy
时间: 2022-03-27
链接: https://x.com/karpathy/status/1507896513019760640
互动: Likes: 209; Retweets: 10; Replies: 16; Quotes: 3; Views: 0; Bookmarks: 16; isReply: 0

@TYLERZHU3 How is this https://t.co/um9qF0MSxt going to compete with what I am seeing on TikTok. I was bored on the 3rd second.

@TYLERZHU3 这个 https://t.co/um9qF0MSxt 将如何与我在 TikTok 上所见的那些内容竞争呢？我仅仅看了三秒钟就感到乏味了。

### 092

作者: @karpathy
时间: 2022-03-27
链接: https://x.com/karpathy/status/1508148604149587972
互动: Likes: 5,861; Retweets: 309; Replies: 435; Quotes: 121; Views: 0; Bookmarks: 85; isReply: 0

Taking some time off to rest&amp;travel after almost 5 years at Tesla. Esp excited to get focused time to re-sharpen my technical edge and train some neural nets! Though I already miss all the robots and GPU/Dojo clusters and looking forward to having them at my fingertips again ❤️😅

在 Tesla 工作了将近 5 年后，我将抽出一些时间休整旅行。特别高兴能有时间专心致志地重新磨砺我的技术锋芒，并训练一些神经网络（neural nets)！虽然我已经很想念所有的机器人以及那些 GPU/Dojo 集群了，但我期待它们能再次触手可及 ❤️😅

### 093

作者: @karpathy
时间: 2022-03-28
链接: https://x.com/karpathy/status/1508392415761256449
互动: Likes: 3,851; Retweets: 110; Replies: 301; Quotes: 38; Views: 0; Bookmarks: 60; isReply: 0

A number of people asked - I am doing a “digital nomad” trip, packed up in one backpack and going east, saying hi to friends along the way and reading papers/writing code. Currently in UK, continuing to Europe, Asia and wrapping around back to Bay Area.

不少人问我 —— 我正在进行一次「数字游民（digital nomad）」之旅，只背一个背包，一路向东。沿途我会拜访朋友，阅读论文，并编写代码。目前我在英国，之后将继续前往欧洲、亚洲，最后回到湾区。

### 094

作者: @karpathy
时间: 2022-03-28
链接: https://x.com/karpathy/status/1508393402320273412
互动: Likes: 19; Retweets: 1; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@lowthj6 Haha I came in my California wear and have indeed been struggling just a little. It’s awkwardly right at the border of where a jacket would be nice but not  strictly speaking necessary 😅

@lowthj6 哈哈，我穿着我那套加州风格的衣服来的，确实有点扛不住了。这天气就是那种，穿件外套会很舒服，但又不是非穿不可的尴尬临界点😅

### 095

作者: @karpathy
时间: 2022-03-28
链接: https://x.com/karpathy/status/1508432802672381962
互动: Likes: 214; Retweets: 4; Replies: 10; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@JilianneParker actually I spent a week in Austin and Dallas

@JilianneParker 实际上我在奥斯汀和达拉斯待了一周

### 096

作者: @karpathy
时间: 2022-03-28
链接: https://x.com/karpathy/status/1508437725514510336
互动: Likes: 1,309; Retweets: 103; Replies: 43; Quotes: 27; Views: 0; Bookmarks: 138; isReply: 0

(rant) "epochs" are a bug-inducing concept in neural net training and should be avoided in favor of number of iterations. Use of epochs silently functionally distorts training (as datasets change/grow) and decreases code portability (when one wishes to train on different dataset)

(个人看法）在神经网络训练中，「epoch」这个概念其实是个容易引入 bug 的隐患，我们应该尽量避免使用它，转而采用「迭代次数」来衡量训练过程。这是因为，当数据集发生变化或不断增长时，使用 epoch 会悄无声息地对训练过程造成功能上的扭曲，并且还会降低代码的可移植性，尤其是在你希望用不同的数据集进行训练时。

### 097

作者: @karpathy
时间: 2022-03-28
链接: https://x.com/karpathy/status/1508440504823857158
互动: Likes: 58; Retweets: 0; Replies: 8; Quotes: 1; Views: 0; Bookmarks: 2; isReply: 1

@__kolesnikov__ yes ty number of samples seen would be a good alternative to consider, eager to hear other's experiences with. do you have some references for?

@__kolesnikov__ 是的，谢谢！「看到的样本数量」会是一个值得考虑的不错替代方案，我很期待听到其他人在这方面的经验。你有没有相关的参考文献呢？

### 098

作者: @karpathy
时间: 2022-03-28
链接: https://x.com/karpathy/status/1508441073584070656
互动: Likes: 47; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@joeyearsley_ exactly ty, once you add data augmentations and esp data oversampling into the mix the whole concept starts to fall apart

@joeyearsley_ 没错谢谢，一旦你把数据增强（data augmentations），尤其是数据过采样（data oversampling），也考虑进去，那么整个概念就开始站不住脚了。

### 099

作者: @karpathy
时间: 2022-03-28
链接: https://x.com/karpathy/status/1508510215423078404
互动: Likes: 63; Retweets: 1; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@rskokan9 Sadly not planning to atm because there is so little anchoring me there so many years later, but hi!!! 👋 I just caught up with a childhood friend from Kosice and he informed me that even the Tesco on Hlavna is gone now. Basically unrecognizable 🥲

@rskokan9 遗憾的是，目前我没有这个打算，因为这么多年过去了，那里几乎没有什么值得我再回去留恋的了，不过你好啊！！！👋 我刚和一位来自科希策的儿时朋友叙旧，他告诉我连 Hlavna 街上的 Tesco 也都没了。简直认不出来了 🥲

### 100

作者: @karpathy
时间: 2022-03-30
链接: https://x.com/karpathy/status/1509227367302148098
互动: Likes: 1,223; Retweets: 195; Replies: 32; Quotes: 21; Views: 0; Bookmarks: 174; isReply: 0

New (small!) language model Chinchilla (70B) outperforms much larger Gopher (280B), GPT-3 (175B), Jurrasic-1 (178B), MT-NLG (530B)  https://t.co/yALvVcsTDW Important new LM scaling laws paper from DeepMind. Go smaller, train longer. Many misconfigurations likely continue to lurk.

DeepMind 发布了一篇关于语言模型（Language Model，LM）扩展定律的重要新论文，文中指出，新的小型语言模型 Chinchilla （拥有 70 亿参数）在性能上超越了参数量大得多的模型，包括 Gopher （280 亿参数）、GPT-3 （175 亿参数）、Jurrasic-1 （178 亿参数）和 MT-NLG （530 亿参数）。该论文的核心观点是：模型可以做得更小，但需要更长的训练时间。文章也暗示，当前许多模型的配置可能仍存在尚未被发现的优化空间。欲了解更多详情，请访问：https://t.co/yALvVcsTDW。

### 101

作者: @karpathy
时间: 2022-03-30
链接: https://x.com/karpathy/status/1509229657111080971
互动: Likes: 65; Retweets: 3; Replies: 6; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@ButterKaffee It's not that. These models are so expensive to train that you have to guess the "scaling laws" by studying smaller models, extrapolate that out, and make your best guess at optimal mega model training configuration. You then cross your fingers and train for a few weeks/months.

情况并非如此。这些模型训练成本极其高昂，因此你必须通过研究较小的模型来推测其「缩放定律（scaling laws）」，以此进行外推，并尽可能准确地估算出最佳的巨型模型（mega model）训练配置。之后，你只能寄希望于一切顺利，然后开始长达数周或数月的训练。

### 102

作者: @karpathy
时间: 2022-03-30
链接: https://x.com/karpathy/status/1509289133637832705
互动: Likes: 758; Retweets: 40; Replies: 1; Quotes: 14; Views: 0; Bookmarks: 55; isReply: 0

Seems likely we’ll have custom (and partially auto-generated) “textbooks” but for teaching language models, not humans, to help them “grok” concepts.

我们很可能将会拥有一些定制的（甚至部分由人工智能自动生成）「教科书」，不过这些「教科书」并非用来教人类，而是用来「教」大语言模型，帮助它们更深入地「理解」（grok）各种概念。

### 103

作者: @karpathy
时间: 2022-03-31
链接: https://x.com/karpathy/status/1509522871105773571
互动: Likes: 539; Retweets: 75; Replies: 7; Quotes: 4; Views: 0; Bookmarks: 115; isReply: 0

“Exploring Plain Vision Transformer Backbones for Object Detection” https://t.co/E1POjnFmgZ
Excellent read as usual from the FAIR team. Strong object detection results with only minor tweaks on the vanilla (ViT) Transformer backbone.

「探索用于目标检测的基础视觉 Transformer 骨干网络」https://t.co/E1POjnFmgZ
这篇来自 FAIR 团队的文章一如既往地出色，是篇值得一读的佳作。他们仅对原始的（vanilla）视觉 Transformer（ViT）骨干网络进行了一些细微调整，就在目标检测方面取得了强大的成果。

### 104

作者: @karpathy
时间: 2022-03-31
链接: https://x.com/karpathy/status/1509522874008281088
互动: Likes: 117; Retweets: 2; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 5; isReply: 1

Loving the philosophy of preserving simple Transformer as a Universal (Neural) Computer, where the core architecture is not meddled with much. Domain knowledge is “factored out”, only enters only through position encodings, sparsity masks, loss functions, data augmentations, etc.

我们很赞同这样一种理念：将简单的 Transformer 模型看作一台通用（神经网络）计算机，其核心架构不应被过多改动。领域知识（domain knowledge）被「抽离出来」，只通过位置编码（position encodings）、稀疏性掩码（sparsity masks）、损失函数（loss functions）和数据增强（data augmentations）等方式融入。

### 105

作者: @karpathy
时间: 2022-03-31
链接: https://x.com/karpathy/status/1509522876734521349
互动: Likes: 153; Retweets: 12; Replies: 5; Quotes: 2; Views: 0; Bookmarks: 4; isReply: 1

In this paper the Mask RCNN is still bolted on for detection. Philosophically (and I’m guessing authors might agree and are curious) would be exciting to see a fully E2E approach win eventually, simply adding another decoder Transformer, directly outputting the boxes.

在这篇论文中，Mask RCNN 依然是作为检测模块被「拼凑」或「硬连接」上去的。从更深层次的角度来看（我猜作者们可能也会同意并对此充满好奇），如果能看到一个完全端到端（E2E）的方法最终脱颖而出，那将是非常激动人心的。这样的方法只需简单地增加一个解码器 Transformer，就能直接输出边界框。

### 106

作者: @karpathy
时间: 2022-03-31
链接: https://x.com/karpathy/status/1509524954911498240
互动: Likes: 187; Retweets: 7; Replies: 9; Quotes: 3; Views: 0; Bookmarks: 3; isReply: 1

At that point we can just throw away a few decades of object detection research and it will be great :) Bitter sweet. Will happen to everyone.

到那时，我们就可以直接抛弃几十年来的目标检测研究了，而且这会很棒 :）这种感觉真是苦乐参半，但它终将发生在每个人身上。

### 107

作者: @karpathy
时间: 2022-03-31
链接: https://x.com/karpathy/status/1509548630272888840
互动: Likes: 4; Retweets: 1; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@xhluu Tbh I’m a bit suspicious of DETR. I don’t super love the need for matching function, and a bit worried about the optimization of it.

@xhluu 坦白说，我有点怀疑 DETR。我不太喜欢它对匹配函数（matching function）的需求，而且也有些担心它的优化效果。

### 108

作者: @karpathy
时间: 2022-03-31
链接: https://x.com/karpathy/status/1509659009455906816
互动: Likes: 67; Retweets: 0; Replies: 1; Quotes: 3; Views: 0; Bookmarks: 6; isReply: 1

@OriolVinyalsML We might, but constructed deliberately with the sole purpose of improving the log probability of a language model, instead of originating from some other original human concern, before being repurposed as data for language model as an afterthought.

@OriolVinyalsML 我们可能可以，但这样做的前提是这些数据是专门为了提高语言模型的对数概率而刻意构建的。它们并非源于人类原本的某些其他需求，而是在产生后才作为事后考虑，被重新用作语言模型的数据。

### 109

作者: @karpathy
时间: 2022-04-01
链接: https://x.com/karpathy/status/1509962678319595523
互动: Likes: 1,660; Retweets: 253; Replies: 49; Quotes: 43; Views: 0; Bookmarks: 508; isReply: 0

Just making sure everyone read “The Bitter Lesson”, as it is one of the best compact pieces of insight into nature of progress in AI. Good habit to keep checking ideas on whether they pass the bitter lesson gut check https://t.co/xf6UwaqohF

在这里提醒大家阅读一下「The Bitter Lesson」（《惨痛的教训》）这篇文章，因为它对人工智能（AI）领域进步的本质提供了一份精辟且浓缩的见解。养成一个好习惯，就是不断用「惨痛的教训」来审视自己的想法，看看它们是否经得起检验。https://t.co/xf6UwaqohF

### 110

作者: @karpathy
时间: 2022-04-01
链接: https://x.com/karpathy/status/1509965598876393476
互动: Likes: 39; Retweets: 1; Replies: 2; Quotes: 2; Views: 0; Bookmarks: 0; isReply: 1

@nc_znc @wellingmax @michael_nielsen I certainly view it more as an asymptotic statement

@nc_znc @wellingmax @michael_nielsen 我无疑更倾向于将其看作是一种渐近的说法。

### 111

作者: @karpathy
时间: 2022-04-01
链接: https://x.com/karpathy/status/1509968395432869890
互动: Likes: 2,055; Retweets: 121; Replies: 115; Quotes: 18; Views: 0; Bookmarks: 22; isReply: 0

I thought browsing internet in the US was unpleasant but Europe is on a whole next level of suffering with GDPR cookie prompts. You have to solve a different puzzle game of “make the dialog box go away” at every single site before you are rewarded with content you wanted to see

我原以为在美国上网的体验已经够糟糕了，但欧洲在应对通用数据保护条例（GDPR）的 cookie 提示方面，痛苦程度简直是更上一层楼。在欧洲，你必须在每个网站上都玩一场不同的「如何让弹窗消失」的解谜游戏，才能最终看到你想要的内容。

### 112

作者: @karpathy
时间: 2022-04-01
链接: https://x.com/karpathy/status/1509970423529848834
互动: Likes: 613; Retweets: 8; Replies: 67; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

It’s not at all just finding the OK or YES button. Many prompts are quite complex - highly customizable wrt exactly what cookie groups are used and when, wordy and explicit, scrollable / tabbable in various directions, etc. Little puzzles.

这根本不只是找到一个「OK」或「YES」按钮那么简单。很多此类提示（prompts）实际上相当复杂 —— 它们可以高度自定义（customizable），精确地控制哪些 cookie 组（cookie groups）在何时被使用；内容可能冗长而详细，并且可以在不同方向上滚动或通过标签页（tabbable）进行切换。这些都像一个个小谜题。

### 113

作者: @karpathy
时间: 2022-04-02
链接: https://x.com/karpathy/status/1510172924208156679
互动: Likes: 69; Retweets: 3; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@julien_c @_lewtun @lvwerra @Thom_Wolf I love that the animal is a parrot

@julien_c @_lewtun @lvwerra @Thom_Wolf 我很喜欢这只动物是鹦鹉。

### 114

作者: @karpathy
时间: 2022-04-02
链接: https://x.com/karpathy/status/1510229609937264650
互动: Likes: 7,624; Retweets: 406; Replies: 305; Quotes: 200; Views: 0; Bookmarks: 130; isReply: 0

I forgot how cool European cities are. More compact, denser, more unique / interesting, cleaner, safer, pedestrian/bike friendly, a lot more pedestrian only plazas with people relaxing / hanging out. A lot more of outside is an outdoor living space, not just transportation space.

我真是忘了欧洲城市有多么迷人。它们更紧凑、更密集、更独特有趣、更干净、更安全，对行人及骑行者也更友好。那里有更多专供行人使用的广场，人们可以在其中放松、闲逛。你会发现，户外的很多地方都被打造成了生活空间，而不仅仅是交通区域。

### 115

作者: @karpathy
时间: 2022-04-02
链接: https://x.com/karpathy/status/1510390427005947905
互动: Likes: 591; Retweets: 47; Replies: 18; Quotes: 2; Views: 0; Bookmarks: 30; isReply: 0

hypnotic

催眠的 / 令人着迷的 / 引人入胜的

### 116

作者: @karpathy
时间: 2022-04-05
链接: https://x.com/karpathy/status/1511359920804876298
互动: Likes: 835; Retweets: 104; Replies: 32; Quotes: 38; Views: 0; Bookmarks: 98; isReply: 0

New SOTA big language model, surpassing Chinchilla from just ~week ago. My favorite demo is the joke explanations, which rival/surpass my own ability :). 540B Transformer on 780B tokens, roughly 4.3X compute of Chinchilla. Data includes multilingual and code. Few notes: https://t.co/xCWVPA5M1r

一个新的 SOTA（State-of-the-Art，即最先进的）大语言模型（Large Language Model）问世了，它超越了大约一周前发布的 Chinchilla 模型。我最喜欢的演示是它对笑话的解释能力，这甚至可以媲美或超越我自己的水平。这个模型是一个拥有 5400 亿参数的 Transformer 模型（Transformer），在 7800 亿个 Token（Token）数据上进行了训练，其计算量大约是 Chinchilla 模型的 4.3 倍。训练数据包含了多语言和代码。以下是一些注意事项：https://t.co/xCWVPA5M1r

### 117

作者: @karpathy
时间: 2022-04-05
链接: https://x.com/karpathy/status/1511359923686367232
互动: Likes: 217; Retweets: 6; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 11; isReply: 1

- "discontinuous improvement" from scaling alone observed on ~25% of BIG-Bench tasks  🥹
- bitwise determinism 🤓
- mysterious (data+model)-dependent loss spikes (signatures of consciousness🤔? jk)
- chain-of-thought prompting + post-hoc calculator + few-shot can do quite well 🪄

- 仅通过扩大模型规模（scaling），在大约 25% 的 BIG-Bench 任务中，我们观察到了「非连续性」的性能提升，而不是平稳的进步🥹。
- 比特位级别的确定性（bitwise determinism）🤓
- 神秘的、与数据和模型都相关的损失函数尖峰（难道是意识的迹象🤔？开个玩笑罢了）
- 结合思维链提示（chain-of-thought prompting）、事后使用计算器以及少样本（few-shot）学习，可以取得相当出色的表现 🪄

### 118

作者: @karpathy
时间: 2022-04-06
链接: https://x.com/karpathy/status/1511617066448703493
互动: Likes: 44; Retweets: 2; Replies: 5; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@antonsterenborg @iScienceLuvr Ty yes I noticed :) really liked Amsterdam! If I had one more day I wanted to see Utrecht, next time.

@antonsterenborg @iScienceLuvr 谢谢，是的，我注意到了 :）真的很喜欢阿姆斯特丹！如果我能多待一天，我本想去乌得勒支，下次再去吧。

### 119

作者: @karpathy
时间: 2022-04-06
链接: https://x.com/karpathy/status/1511714627935166474
互动: Likes: 1,058; Retweets: 155; Replies: 25; Quotes: 10; Views: 0; Bookmarks: 39; isReply: 0

Incredible pace of progress recently in image generation and multimodal (image &lt;-&gt; text) representation learning.

最近，在图像生成和多模态 （图像 <-> 文本）表征学习（representation learning）领域，取得了令人难以置信的飞速进展。

### 120

作者: @karpathy
时间: 2022-04-06
链接: https://x.com/karpathy/status/1511716250883346445
互动: Likes: 98; Retweets: 8; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 7; isReply: 1

@DrEricDahl @stats_feed @drfeifei @heydave7 @jamesdouma Highly amusing conspiracy theories. I am interested in language models because they reveal techniques for training very big Transformer neural nets, and today's AI both language and vision (or anything else) is all just Transformers due to convergence see  https://t.co/cJPYotCKcr

@DrEricDahl @stats_feed @drfeifei @heydave7 @jamesdouma 那些阴谋论真是太有意思了。我之所以对语言模型（language models）感兴趣，是因为它们揭示了训练大型 Transformer 神经网络（Transformer neural nets）的技巧。你看，由于技术上的趋同，当前无论是语言、视觉还是其他领域的 AI（Artificial Intelligence），其核心都只是 Transformer 架构，详情可参考：https://t.co/cJPYotCKcr

### 121

作者: @karpathy
时间: 2022-04-06
链接: https://x.com/karpathy/status/1511717529017896967
互动: Likes: 28; Retweets: 1; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@gslaller Yes and the contrast learning objective I'd use back then is v similar to CLIP.  Scale was missing - in compute and datasets. But the idea of generating images of this fidelity was unthinkable in ~2015. DVQ+AR on patches would be plausible but diffusion models were not invented.

@gslaller 是的，我那时设想的对比学习目标（contrast learning objective）和 CLIP 非常相似。当时欠缺的是规模，主要体现在计算能力（compute）和数据集（datasets）上。但在大约 2015 年，能生成如此高真实感（fidelity）图像的想法简直不可思议。虽然当时在图像补丁（patches）上使用 DVQ+AR 似乎是可行的，但扩散模型（diffusion models）尚未被发明出来。

### 122

作者: @karpathy
时间: 2022-04-06
链接: https://x.com/karpathy/status/1511725354037219334
互动: Likes: 94; Retweets: 5; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 18; isReply: 1

@jamesdouma @DrEricDahl @stats_feed @drfeifei @heydave7 Exactly. But NLP has run far ahead of vision on showing impressive transfer learning to tasks outside of the self-supervised objective. Vision is bit behind I think partly due to required scale (many many more pixels than words). Papers like MAE are close https://t.co/C2Ih9i6AVv

@jamesdouma @DrEricDahl @stats_feed @drfeifei @heydave7 没错。不过，在展示将出色的迁移学习（transfer learning）应用到超越其自监督目标（self-supervised objective）的任务上，自然语言处理（NLP）已经遥遥领先于计算机视觉（vision）。我认为视觉领域之所以有些滞后，部分原因在于它所需的规模巨大（相比于文本数据，图像包含的像素（pixels）数量要多得多）。不过，像 MAE [论文] 这样的研究工作已经非常接近了 https://t.co/C2Ih9i6AVv

### 123

作者: @karpathy
时间: 2022-04-06
链接: https://x.com/karpathy/status/1511731166650544129
互动: Likes: 17; Retweets: 1; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@ranig @jamesdouma @DrEricDahl @stats_feed @drfeifei @heydave7 Yes in NLP humans did the hard work of compression into discrete tokens. In vision the pixels are extra plentiful, raw (uncompressed), and also have a lot more distracting entropy - e.g. structure in clouds, trees, etc. Could simulate it in NLP by sprinkling in 10X random tokens.

@ranig @jamesdouma @DrEricDahl @stats_feed @drfeifei @heydave7 没错，在自然语言处理（NLP）中，人类承担了将文本信息压缩成离散 Token 的繁重任务。然而，在计算机视觉（Vision）领域，图像的像素不仅数量庞大、未经压缩，还包含大量分散注意力的复杂信息（即熵（entropy)），比如云朵和树木中错综复杂的结构。这就像在自然语言处理（NLP）中，我们模拟性地加入 10 倍的随机 Token 一样，会大大增加处理难度。

### 124

作者: @karpathy
时间: 2022-04-07
链接: https://x.com/karpathy/status/1512117132716355590
互动: Likes: 1,861; Retweets: 120; Replies: 34; Quotes: 8; Views: 0; Bookmarks: 55; isReply: 0

The evolution of API for running cutting edge AI:
- run it on your own machine 
- run it in the cloud
- apply pay for and query an api endpoint
- pretty please ask one of the authors to run it for you on Twitter 
🥲

运行前沿 AI（Artificial Intelligence）的 API（Application Programming Interface）演进之路：
- 在你自己的机器上运行 AI
- 在云端运行 AI
- 付费申请并查询一个 API 端点
- 恳请作者之一在 Twitter 上为你运行 AI
🥲

### 125

作者: @karpathy
时间: 2022-04-07
链接: https://x.com/karpathy/status/1512118006293348354
互动: Likes: 37; Retweets: 0; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 0

@TechLeaderPro It all makes sense it’s just amusing

@TechLeaderPro 这完全说得通，只是觉得有点意思。

### 126

作者: @karpathy
时间: 2022-04-07
链接: https://x.com/karpathy/status/1512129911707873283
互动: Likes: 53; Retweets: 0; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@nickcammarata I’m ok with this :D

@nickcammarata 我觉得可以 :D

### 127

作者: @karpathy
时间: 2022-04-08
链接: https://x.com/karpathy/status/1512348020918013955
互动: Likes: 1,737; Retweets: 56; Replies: 100; Quotes: 16; Views: 0; Bookmarks: 39; isReply: 0

Starbucks is an oasis of essential infrastructure (esp for a traveler). Shelter ✅ water (+coffee!) ✅ food ✅ bathroom ✅ Wi-Fi ✅ someone who probably speaks English ✅ and all of it at many branches with near certainty and minimal variation 🙏🙇‍♂️

星巴克简直是提供必要基础设施的一片绿洲（尤其对于旅行者而言）。在这里，你可以找到住所 ✅ 水（+ 咖啡！）✅ 食物 ✅ 卫生间 ✅ Wi-Fi ✅ 甚至可能遇到会说英语的人 ✅ 最重要的是，在众多分店，所有这些服务几乎都能得到保证，而且标准统一，差异极小 🙏🙇‍♂️

### 128

作者: @karpathy
时间: 2022-04-08
链接: https://x.com/karpathy/status/1512367612461662209
互动: Likes: 43; Retweets: 1; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@WilliamJamesL “hidden” is the right keyword. I’m sure there is, but I just need some coffee, a snack, I need to use the bathroom and check my inbox

@WilliamJamesL「隐藏」确实是正确的关键词。我确信这个关键词是存在的，不过我眼下更需要来杯咖啡、一份零食，还得去趟洗手间并查看我的收件箱。

### 129

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513376741481459716
互动: Likes: 135; Retweets: 9; Replies: 3; Quotes: 2; Views: 0; Bookmarks: 1; isReply: 1

@fchollet PR/comms always love to perform triple backflips 👏

@fchollet 公关 / 传播部门总是喜欢卖力表现（或者说「大费周章」、「耍花活」）👏

### 130

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513416252626284545
互动: Likes: 1,364; Retweets: 47; Replies: 95; Quotes: 13; Views: 0; Bookmarks: 81; isReply: 0

Still trying to figure out where most of the real signal is in typical 5-star rating distributions. It's definitely not average rating. Simply the number of ratings is usually quite good. The ratio of 5-star to 4-star is for some reason quite good too.

我们仍在努力探索，在典型的五星评级分布中，真正有价值的信息究竟藏在哪里。可以肯定的是，它绝不仅仅是简单的平均评分。通常来说，仅仅是评价的数量本身，就颇具参考价值。而令人有些意外的是，五星评价与四星评价的比例，在某些情况下也出奇地有用。

### 131

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513420766146084865
互动: Likes: 615; Retweets: 24; Replies: 37; Quotes: 4; Views: 0; Bookmarks: 19; isReply: 0

Reading sci-fi with humanoid aliens who speak English and have faces is what others must be experiencing as they hear a fork scratching a plate.

读到科幻小说里那些说着英语、长着人脸的类人外星人，就像其他人听到叉子刮盘子的声音时感受到的那样。

### 132

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513422427509903364
互动: Likes: 35; Retweets: 0; Replies: 3; Quotes: 2; Views: 0; Bookmarks: 3; isReply: 1

@Smerity :D :D Absolutely had to, of course. One of my top favorite alien portrayals, I think strikes a good balance between 1) a more likely reality and 2) a certain relatability, for the sake of an entertaining story for human consumption. Really good! What'd you think?

@Smerity :D :D 当然，非看不可！
我最喜欢的外星人形象之一，我认为它在 1）更贴近现实的可能性和 2）一定的共鸣感之间找到了很好的平衡，为人类呈现了一个引人入胜的故事。
真的很棒！你觉得呢？

### 133

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513422711741194242
互动: Likes: 10; Retweets: 1; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@ryanmonsurate Yes enjoyed the world building! Didn't find the overarching idea too plausible but the world itself was interesting and creative.

@ryanmonsurate 是的，我很喜欢它构建的世界观！虽然不太认同其核心理念，但这个世界本身就很有趣、很有创意。

### 134

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513423432733663232
互动: Likes: 21; Retweets: 0; Replies: 2; Quotes: 2; Views: 0; Bookmarks: 0; isReply: 1

@Masoneer_ I really love the concept of psychohistory and the macro concept / idea of the story arc and the opening chapter, but later execution holds it back, kind of devolves into what I recall to be a bit of a soap opera.

@Masoneer_ 我非常喜欢心理史学（psychohistory）的概念，以及故事整体的宏观构想和开篇章节。然而，后续情节的展开却有些令人失望，在我看来，它逐渐演变成了一部肥皂剧。

### 135

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513423858950365187
互动: Likes: 20; Retweets: 0; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@tszzl @Smerity 👍Arrival is a masterpiece, Ted Chiang in top form. The short story, not the movie.

@tszzl @Smerity 👍《降临》绝对是部杰作，特德·蒋在这篇作品中展现了他炉火纯青的写作功力。我指的是原著短篇小说，而不是电影版。

### 136

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513424629116948480
互动: Likes: 28; Retweets: 0; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@sotanez Stanislaw Lem was a master of this general concept, that space of bodies and minds must be vast. Also loved his "His Master's Voice" and "Fiasco", similar themes.

@sotanez Stanislaw Lem 精通这个普遍概念 —— 他认为，身体和思想的空间都必须是广阔的。我也很喜欢他的《His Master's Voice》和《Fiasco》，这两部作品探讨了类似的主题。

### 137

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513425250284843009
互动: Likes: 616; Retweets: 11; Replies: 40; Quotes: 2; Views: 0; Bookmarks: 6; isReply: 1

I also love how when you sort by top average rating, you get lists of items with a perfect 5.0 rating but only 10 votes. It's a completely broken concept, out there on display in most websites with a shrug.

我也觉得很无奈，当你按平均评分由高到低排序时，经常会看到那些评分高达完美的 5.0，但实际上只有区区 10 个评价的商品列表。这是一个完全有缺陷的（broken）概念，在大多数网站上随处可见，却没有人去解决。

### 138

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513427131757469699
互动: Likes: 8; Retweets: 0; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@unsorsodicorda I did but I should try again :)

@unsorsodicorda 我试过了，但我应该再试一次 :)

### 139

作者: @karpathy
时间: 2022-04-11
链接: https://x.com/karpathy/status/1513428274407518209
互动: Likes: 9; Retweets: 0; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@mx0r Looking at its description again though, it's a "space opera trilogy about a ship's AI who becomes trapped in a human body, and her quest for revenge". Not so sure this book and I can be friends.

@mx0r 尽管又看了一遍它的介绍，这是一部「关于一艘飞船上的 AI （人工智能）被困在人类身体里，并踏上复仇之路的太空歌剧三部曲」。我不太确定这本书是否合我的胃口。

### 140

作者: @karpathy
时间: 2022-04-12
链接: https://x.com/karpathy/status/1513842492772438016
互动: Likes: 712; Retweets: 43; Replies: 16; Quotes: 4; Views: 0; Bookmarks: 42; isReply: 0

Haha ty @ykilcher for hosting me on his excellent ML News series for a cameo while I was in Zurich, in the role of a random pedestrian who knows too much about deep learning :D

哈哈，谢谢 @ykilcher 邀请我，在我在苏黎世期间，在他的精彩节目「ML News」系列中客串了一把！我扮演的角色是一个偶然出现的路人，但却对深度学习（deep learning）了解得有点太多了 :D

### 141

作者: @karpathy
时间: 2022-04-13
链接: https://x.com/karpathy/status/1514318794914766848
互动: Likes: 324; Retweets: 27; Replies: 29; Quotes: 4; Views: 0; Bookmarks: 37; isReply: 0

Someone should try to inspect the model output conditioned on those high loss batches just to make sure it does not have structure. That it doesn't plead or make demands or etc.

有人或许应该尝试检查一下模型在那些高损失批次（high loss batches）上的输出，以确保其中没有出现异常的结构。也就是说，要确认它没有做出恳求、提出要求或其他类似的举动。

### 142

作者: @karpathy
时间: 2022-04-14
链接: https://x.com/karpathy/status/1514515683106856965
互动: Likes: 32; Retweets: 2; Replies: 8; Quotes: 1; Views: 0; Bookmarks: 2; isReply: 1

@tejasdkulkarni Your daughter has a few hundred million years of training data. And most of the progress you're seeing is the brain maturing not learning. Precocial animals pretty much prove this :(  https://t.co/QI1LwEI0ff

@tejasdkulkarni 你的女儿拥有数亿年的训练数据（training data）。而且你所观察到的大部分进步是大脑的成熟，而非单纯的学习。早熟动物在很大程度上证明了这一点 :( https://t.co/QI1LwEI0ff

### 143

作者: @karpathy
时间: 2022-04-14
链接: https://x.com/karpathy/status/1514516879712399360
互动: Likes: 47; Retweets: 2; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@lee_redden @UberEats ? but that would decrease number on dashboard. you have to make number go up to promotion.

@lee_redden @UberEats ? 但那样会减少仪表盘上的数字啊。你必须把数字提高才能获得晋升。

### 144

作者: @karpathy
时间: 2022-04-14
链接: https://x.com/karpathy/status/1514620205753782272
互动: Likes: 15; Retweets: 2; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@tejasdkulkarni The details are not in genome but that’s just the easy last few bits. The hard part is the abstract “game engine” + meta learner foundation model, v strong init of which is somehow compressed in ATCGs. Blows my mind that it is so, but the biological mechanism clearly exists.

@tejasdkulkarni 那些细节并不存储在基因组（genome）中，那只是最后几个简单的部分。真正困难的部分是抽象的「游戏引擎」(game engine）加上元学习器基础模型（meta learner foundation model），而一个非常强大的初始版本却以某种方式被压缩在 ATCGs（即 DNA 的四种碱基）里。这真是令人难以置信，但这种生物学机制却又真实存在着。

### 145

作者: @karpathy
时间: 2022-04-16
链接: https://x.com/karpathy/status/1515357035780616192
互动: Likes: 653; Retweets: 39; Replies: 85; Quotes: 12; Views: 0; Bookmarks: 42; isReply: 0

The time evolution of human condition (approximated as a gaussian) is more that of expanding variance than that of moving mean.

如果我们将人类状况近似看作一个高斯分布（gaussian），那么它随时间的变化，更多地表现为这个分布的「方差」在不断扩大，而不是它的「均值」在持续移动。

### 146

作者: @karpathy
时间: 2022-04-16
链接: https://x.com/karpathy/status/1515361631366266886
互动: Likes: 1,706; Retweets: 139; Replies: 47; Quotes: 21; Views: 0; Bookmarks: 158; isReply: 0

Looking back, my most valuable college classes were physics, but for general problem solving intuitions alone:
- modeling systems with increasingly more complex terms
- extrapolating variables to check behaviors at limits
- pursuit of the simplest most powerful solutions
...

回想起来，我最有价值的大学课程是物理，但就培养解决问题的一般直觉而言，它教会了我：
- 如何用越来越复杂的项来构建系统模型
- 如何通过推演变量来检验系统在极限情况下的表现
- 如何追求最简洁却最有效的解决方案
...

### 147

作者: @karpathy
时间: 2022-04-16
链接: https://x.com/karpathy/status/1515363768624201729
互动: Likes: 284; Retweets: 24; Replies: 12; Quotes: 5; Views: 0; Bookmarks: 20; isReply: 1

@intelligent_eat Biology is absolutely fascinating. two thoughts:  1) it is taught totally wrong, via memorization, description and enumeration (ew). 2) it should be taught much later, as a branch of applied physics / chemistry / computer science.

@intelligent_eat 生物学实在是太吸引人了。对此我有两点思考：1）它的教学方式完全是错的，充斥着死记硬背、单纯的描述和罗列知识点（真无趣）。2）应该更晚地教授它，将其作为应用物理学、化学或计算机科学的一个分支学科来学习。

### 148

作者: @karpathy
时间: 2022-04-16
链接: https://x.com/karpathy/status/1515365331497275406
互动: Likes: 58; Retweets: 3; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@sergeivolodinch Actually totally agree and something I had to pick up and think about much later in life. Surprisingly absent in standard technical curriculums.

@sergeivolodinch 确实非常同意，这也是我直到人生后期才不得不去学习和思考的。令人惊讶的是，在标准的专业技术课程中，这一点竟然是缺失的。

### 149

作者: @karpathy
时间: 2022-04-16
链接: https://x.com/karpathy/status/1515371307126378504
互动: Likes: 20; Retweets: 1; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@madmaxbr5 everything, then nothing

@madmaxbr5 从拥有所有，到一无所有

### 150

作者: @karpathy
时间: 2022-04-20
链接: https://x.com/karpathy/status/1516725875550879748
互动: Likes: 814; Retweets: 43; Replies: 43; Quotes: 3; Views: 0; Bookmarks: 16; isReply: 0

Wish there was some certification for websites / apps that labels them as “clean” in that they do not use dark patterns. The iOS store in particular is completely infested with free apps that engage open and repeated harassment.

真希望有一种针对网站或应用（apps）的认证，能将那些不使用黑暗模式（dark patterns）的产品标记为「纯净」。尤其是 iOS 商店，里面充斥着大量免费应用，这些应用公然且反复地对用户进行骚扰。

### 151

作者: @karpathy
时间: 2022-04-20
链接: https://x.com/karpathy/status/1516726804530225154
互动: Likes: 108; Retweets: 0; Replies: 6; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@zachgilbert Look at literally any popular app and go to comments. https://t.co/6UPEtnIXZe

@zachgilbert 随便找个流行的 App 看看，然后去翻翻评论就知道了。https://t.co/6UPEtnIXZe

### 152

作者: @karpathy
时间: 2022-04-20
链接: https://x.com/karpathy/status/1516728256606744585
互动: Likes: 18; Retweets: 0; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@zachgilbert Many people would. The certification would make the other model much more competitive.

@zachgilbert 很多人都会这么想。这项认证将使另一个模型的竞争力大大增强。

### 153

作者: @karpathy
时间: 2022-04-20
链接: https://x.com/karpathy/status/1516729557839233027
互动: Likes: 19; Retweets: 0; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@MarcoDiBree @nftmarseth Many people are perfectly willing to pay and want to support creators. They do not want to be harassed.

@MarcoDiBree @nftmarseth 很多人都非常乐意付费，也希望能支持创作者。他们不希望受到骚扰。

### 154

作者: @karpathy
时间: 2022-04-24
链接: https://x.com/karpathy/status/1518030839338074112
互动: Likes: 455; Retweets: 15; Replies: 23; Quotes: 4; Views: 0; Bookmarks: 8; isReply: 1

@ilyasut all of you @gdb and @sama tweets have become more frequent and abstract so I have a running theory that there is some TweetGPT experiment ongoing 🤔;p My discriminator is struggling.

@ilyasut 你以及 @gdb 和 @sama 的所有推文变得越来越频繁也越来越抽象，所以我一直有个猜想，是不是正在进行一项 TweetGPT 实验呢 🤔;p 我的「判别器」快要分辨不清了。

### 155

作者: @karpathy
时间: 2022-04-24
链接: https://x.com/karpathy/status/1518035448739770368
互动: Likes: 50; Retweets: 2; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@gdb @ilyasut @sama Hahah, I had the same thought around GPT-2 time https://t.co/uvkruylhuz . At current pace likely to be less and less insulting until it is a... compliment? :D

@gdb @ilyasut @sama 哈哈哈，我在 GPT-2 时期也有过同样的想法 https://t.co/uvkruylhuz。按照目前的发展速度，它可能会变得越来越不具冒犯性，直到它成为…… 一种赞美？ :D

### 156

作者: @karpathy
时间: 2022-04-24
链接: https://x.com/karpathy/status/1518098910379581440
互动: Likes: 7; Retweets: 0; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@BenMcLeish @WholeMarsBlog @ilyasut @gdb @sama Hah I missed it that explains a lot :)

@BenMcLeish @WholeMarsBlog @ilyasut @gdb @sama 哈哈，我错过了，这下可都明白了 :)

### 157

作者: @karpathy
时间: 2022-04-24
链接: https://x.com/karpathy/status/1518216213922721793
互动: Likes: 4,705; Retweets: 225; Replies: 165; Quotes: 42; Views: 0; Bookmarks: 210; isReply: 0

Search it on TikTok is becoming the next append reddit to your google search to get actually good results

在 TikTok 上搜索内容，正逐渐成为一种新趋势，就像以前人们为了获得更精准、更有价值的搜索结果，会习惯性地在 Google 搜索中加上「reddit」一样。

### 158

作者: @karpathy
时间: 2022-04-24
链接: https://x.com/karpathy/status/1518218598984351745
互动: Likes: 330; Retweets: 12; Replies: 12; Quotes: 2; Views: 0; Bookmarks: 11; isReply: 1

(Like the append reddit hack, this is true for only some, but large number of query types. Fun to discover which ones they are :))

（就像 Reddit 上的那种「追加」技巧一样，这种方法虽然只对部分查询类型有效，但这些类型的数量非常庞大。发现它们具体是哪些会很有趣 :))

### 159

作者: @karpathy
时间: 2022-05-03
链接: https://x.com/karpathy/status/1521339847742746624
互动: Likes: 52; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@gwern @arankomatsuzaki @MetaAI cue the “you keep using that word open” meme :D

@gwern @arankomatsuzaki @MetaAI 想到「你一直用‘开放'这个词」那个梗了 :D

### 160

作者: @karpathy
时间: 2022-05-04
链接: https://x.com/karpathy/status/1521866322136555520
互动: Likes: 615; Retweets: 69; Replies: 18; Quotes: 3; Views: 0; Bookmarks: 70; isReply: 0

Nice and especially appreciate the release of the accompanying logbooks, detailing the struggles of training transformers at scale https://t.co/GErHySLdCJ

令人高兴的是，尤其值得称赞的是，随附日志的发布详细记录了大规模训练 Transformer（Transformer）所面临的挑战 https://t.co/GErHySLdCJ

### 161

作者: @karpathy
时间: 2022-05-04
链接: https://x.com/karpathy/status/1521868052152815616
互动: Likes: 124; Retweets: 7; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 15; isReply: 1

@rasbt 💯 I've always kept logs of projects I've worked on. Imo especially important in deep learning because the latency of each experiment is large, forcing one to increase throughput (babysit and multitask multiple experiments and ideas at once). Very difficult without logs.

@rasbt 💯 我一直都有记录我所参与项目的日志（logs）的习惯。我认为这在深度学习（deep learning）领域尤为重要，因为每个实验的延迟（latency）都很大，这就迫使我们必须提高吞吐量（throughput)—— 也就是要同时兼顾并并行处理多个实验和想法。如果没有日志，这项工作会变得非常困难。

### 162

作者: @karpathy
时间: 2022-05-06
链接: https://x.com/karpathy/status/1522624121951047681
互动: Likes: 626; Retweets: 19; Replies: 17; Quotes: 1; Views: 0; Bookmarks: 11; isReply: 1

@Inoryy Cute! not exactly convincing, but cute :)

@Inoryy 可爱！虽然不那么令人信服，但还是很可爱 :)

### 163

作者: @karpathy
时间: 2022-05-06
链接: https://x.com/karpathy/status/1522637764402892800
互动: Likes: 296; Retweets: 10; Replies: 9; Quotes: 1; Views: 0; Bookmarks: 11; isReply: 1

@OriolVinyalsML @Inoryy A combination of some wrong/vague answers (5+ people, 2+ mirrors, rug) and leading questions. From this chat alone it's not convincing that the model all by itself understands the joke. Haven't read the full 66 pages of Flamingo just yet but it's clearly on track to!🤯

@OriolVinyalsML @Inoryy 这是一个混合了错误或模糊答案（例如「5 + 人」、「2 + 面镜子」、「地毯」）以及引导性问题的情况。仅从这段聊天记录来看，要说这个模型（model）完全靠自己理解了这个笑话，这还不太令人信服。虽然我还没有读完关于 Flamingo 的全部 66 页论文，但它显然正朝着这个目标迈进！🤯

### 164

作者: @karpathy
时间: 2022-05-06
链接: https://x.com/karpathy/status/1522639577118240769
互动: Likes: 139; Retweets: 3; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@Inoryy Agree we're probably not there yet but recent progress makes the problem suddenly feel tractable. 10 years ago when I wrote the post it did not seem tractable. This is a huge improvement.

@Inoryy 我同意，我们可能还没有完全达成目标，但最近的进展让这个问题突然变得有解决的希望了。10 年前当我写那篇文章时，这个问题看起来似乎无解。这是一个巨大的进步。

### 165

作者: @karpathy
时间: 2022-05-07
链接: https://x.com/karpathy/status/1523020133198823426
互动: Likes: 382; Retweets: 17; Replies: 31; Quotes: 1; Views: 0; Bookmarks: 6; isReply: 0

Two fun food adventures today:
1) vegan "egg"+"cheese" veggie scramble for breakfast with @justegg  (generally loving the constraint-driven creativity of vegan cooking)
2) trying out a switch coffee -&gt; matcha (I use milk fronter to mix+warm matcha into water). delicious

今天有两项有趣的美食探索：
1）早餐做了素食版「鸡蛋」+「奶酪」炒蔬菜，用的是 @justegg（我特别喜欢素食烹饪中在限制下激发的创造力）。
2）尝试把咖啡换成抹茶（我用牛奶起泡器将抹茶与水混合并加热）。味道很棒！

### 166

作者: @karpathy
时间: 2022-05-07
链接: https://x.com/karpathy/status/1523021926762905600
互动: Likes: 53; Retweets: 1; Replies: 4; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@intelligent_eat @justegg I've been plant based on and off for a ~decade, but this last cycle has been year+ and really enjoying it. I sometimes cheat pasceterian. The recent proliferation of plant based foods and restaurants has made it much easier and every bit (or even more imo) delicious.

@intelligent_eat @justegg 我吃植物性饮食（plant based）断断续续大约有十年了，但最近这一轮已经坚持了一年多，我真的很享受。我有时会「放纵」一下，吃点鱼素（pasceterian）。最近植物性食物和餐厅的激增，让这一切变得容易得多，而且在我看来，这些食物味道棒极了，甚至比以前更美味。

### 167

作者: @karpathy
时间: 2022-05-09
链接: https://x.com/karpathy/status/1523722166314799104
互动: Likes: 3,101; Retweets: 73; Replies: 64; Quotes: 3; Views: 0; Bookmarks: 55; isReply: 0

Haha not sure why @MrBeast is interested in extra nerdy deep learning / AI tweets. But allegedly he is the future future owner of Twitter :D, and mad respect to the empire he has built, recommend the Joe Rogan episode #1788 with https://t.co/lvQRIS8jA5 https://t.co/luEBoi5MGy

哈哈，不确定为什么 @MrBeast 对这些特别硬核的深度学习（Deep Learning）/ AI（人工智能）推文感兴趣。但据称他将是 Twitter 未来的主人 :D，对他建立的商业帝国表示由衷的敬佩，推荐观看 Joe Rogan 第 #1788 集节目，链接如下：https://t.co/lvQRIS8jA5 https://t.co/luEBoi5MGy

### 168

作者: @karpathy
时间: 2022-05-09
链接: https://x.com/karpathy/status/1523740993626861568
互动: Likes: 3,225; Retweets: 88; Replies: 131; Quotes: 22; Views: 0; Bookmarks: 35; isReply: 0

Feels good to be back home in the bay area 🎉. I started to really miss just sitting uninterrupted at a computer in one spot for long chunks of time, stretching for the very tip of Maslow's hierarchy. Ok, first, cleaning this backlog of just a few hundred things...

回到湾区的家，感觉真棒 🎉。我开始非常想念那种不被打扰，能长时间在一个地方专注地坐在电脑前，努力追求马斯洛需求层次最顶端目标的感觉。好的，首先，得把这几百件积压的事情处理一下了……

### 169

作者: @karpathy
时间: 2022-05-09
链接: https://x.com/karpathy/status/1523750035556499457
互动: Likes: 535; Retweets: 19; Replies: 26; Quotes: 11; Views: 0; Bookmarks: 6; isReply: 1

@Brueck1988 Yes exactly. As tweet says I am back physically home and in the bay area from some travels. Should clarify I am not back at Tesla yet, only ~halfway through my sabbatical.

@Brueck1988 是的，一点没错。正如我在推文里提到的，我旅行归来，现在已经回到了家中，位于湾区。不过我需要说明一下，我还没有回到 Tesla 工作，我的休假才进行到一半左右。

### 170

作者: @karpathy
时间: 2022-05-16
链接: https://x.com/karpathy/status/1526303528360128512
互动: Likes: 1,007; Retweets: 120; Replies: 18; Quotes: 8; Views: 0; Bookmarks: 155; isReply: 0

Beautiful demo of some serious prompt engineering of GPT-3. Basically a new form of programming that we’re likely to see much more of

这是 GPT-3 精湛的提示工程（prompt engineering）的一个精彩演示。它本质上是一种新的编程形式，我们很可能会在未来看到更多这样的应用。

### 171

作者: @karpathy
时间: 2022-05-17
链接: https://x.com/karpathy/status/1526386030127235072
互动: Likes: 9; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@nutanc Thanks for sharing! Have to be mindful of all the cherry picking going around twitter recently.

@nutanc 谢谢分享！最近 Twitter 上有很多「选择性采证」的现象，需要多加留意。

### 172

作者: @karpathy
时间: 2022-05-17
链接: https://x.com/karpathy/status/1526386576418254848
互动: Likes: 17; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@JWonz I wouldn't exactly call it that. Software 2.0 was programming via datasets (or soft constraints more generally), where the neural net code is compiled by the optimization. Prompting is a whole different thing, I called it Software 3.0 a while back.

@JWonz 我不会完全那样定义它。Software 2.0 是一种通过数据集（或者更广义地讲，通过软约束）进行编程的方式，其中神经网络（neural net）代码是通过优化（optimization）过程生成的。而提示（Prompting）则是一个截然不同的概念，我早在之前就将其称之为 Software 3.0。

### 173

作者: @karpathy
时间: 2022-05-17
链接: https://x.com/karpathy/status/1526386672165744640
互动: Likes: 399; Retweets: 26; Replies: 11; Quotes: 0; Views: 0; Bookmarks: 26; isReply: 1

cc this older thread :) https://t.co/N5zdSKQVzG

补充一下之前的讨论串 :）https://t.co/N5zdSKQVzG

### 174

作者: @karpathy
时间: 2022-05-17
链接: https://x.com/karpathy/status/1526609800741605376
互动: Likes: 334; Retweets: 27; Replies: 10; Quotes: 7; Views: 0; Bookmarks: 36; isReply: 1

to spell it out:
1.0: programmer designs the algorithm, output is a binary
2.0: programmer designs the dataset, output are weights of a neural net
3.0: programmer designs the prompt, output is the "mind state" (activations) of a foundation model neural net
something like that 🤔

简单来说，可以这样理解：
1.0：程序员设计算法，输出是二进制文件。
2.0：程序员设计数据集，输出是神经网络的权重。
3.0：程序员设计提示词（prompt），输出是基础模型神经网络的「心智状态」（即激活（activations））。
大致如此吧 🤔

### 175

作者: @karpathy
时间: 2022-05-18
链接: https://x.com/karpathy/status/1526793062659149824
互动: Likes: 14; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@phillip_isola @joannejang yay so cute :)  (I've been waiting for someone to start dropping "visual stories" for a while https://t.co/AseXLfojK4 )

@phillip_isola @joannejang 哇，太可爱了 :）(我一直期待着有人能开始发布「视觉故事」呢 https://t.co/AseXLfojK4 )

### 176

作者: @karpathy
时间: 2022-05-22
链接: https://x.com/karpathy/status/1528407022873346049
互动: Likes: 1,123; Retweets: 95; Replies: 41; Quotes: 10; Views: 0; Bookmarks: 58; isReply: 0

Potentially unpopular opinion but I am so bored of fighting over definitions, chinese rooms and qualias. We can automate aspects of human intelligence and create real economic value. Let's measure, characterize and focus on that.

我的看法可能不受欢迎，但我已经厌倦了围绕各种定义、「中文房间」思想实验以及感受质（Qualia）等问题无休止的争论。我们应该关注如何将人类智能的某些方面自动化，并从中创造出实实在在的经济价值。因此，我们不妨去衡量、表征并专注于实现这一目标。

### 177

作者: @karpathy
时间: 2022-05-22
链接: https://x.com/karpathy/status/1528443124577513472
互动: Likes: 336; Retweets: 24; Replies: 10; Quotes: 8; Views: 0; Bookmarks: 48; isReply: 1

@hmd_palangi @prasanna @colinraffel @Diyi_Yang @ank_parikh Large, clean, diverse data*. The 3 pillars of a good dataset.

@hmd_palangi @prasanna @colinraffel @Diyi_Yang @ank_parikh 大规模的、纯净的、多样化的数据 *。它们是一个优质数据集的三大支柱。

### 178

作者: @karpathy
时间: 2022-05-22
链接: https://x.com/karpathy/status/1528453604515778560
互动: Likes: 858; Retweets: 60; Replies: 44; Quotes: 12; Views: 0; Bookmarks: 59; isReply: 0

real-world data distribution is ~N(0,1)
good dataset is ~U(-2,2)

现实世界的数据分布通常服从均值为 0、标准差为 1 的标准正态分布（N（0,1)）。
而一个好的数据集通常服从在区间 [-2,2] 上的均匀分布（U（-2,2)）。

### 179

作者: @karpathy
时间: 2022-05-22
链接: https://x.com/karpathy/status/1528454567586721797
互动: Likes: 122; Retweets: 2; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 0

@heikkiarponen you are ruining my beautiful short and sweet almost truth with nuance

@Heikki Arponen 你正在用那些细枝末节，把我说得美好又简洁，几乎就是真相的观点给破坏了。

### 180

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528584867675373569
互动: Likes: 2,547; Retweets: 113; Replies: 91; Quotes: 16; Views: 0; Bookmarks: 6; isReply: 1

I was lane changing while a motorcyclist very aggressively lane changed and accelerated from behind the car behind me. Autopilot aborted the lane change and was right.

我当时正在变道，这时一名摩托车手突然从我后面那辆车的后方猛烈变道并加速。自动驾驶（Autopilot）系统随即中止了变道操作，事实证明它的判断是正确的。

### 181

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528584865796399104
互动: Likes: 3,748; Retweets: 337; Replies: 97; Quotes: 55; Views: 0; Bookmarks: 52; isReply: 0

I’ve seen similar events play out in clip telemetry many times, but a few minutes ago is the first time Autopilot prevented an almost certain collision for me personally. Experiencing it in real life is something else.

我曾无数次在视频片段的遥测数据中看到类似事件的发生，但就在几分钟前， Autopilot 第一次为我个人避免了一次几乎必然发生的碰撞。亲身经历这种体验，感受完全不同。

### 182

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528752631178403842
互动: Likes: 26; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 0

@colinraffel Hahah, I like the slide and the joke and the delivery, it’s just become a bit of a pet peeve for me that people don’t value or think about the diversity pillar 😅, when I saw over and over how important it is for good performance.

@colinraffel 我很欣赏这份幻灯片、其中的幽默以及呈现方式。然而，我个人一直有个小小的「痛点」，那就是人们似乎不太重视或思考「多样性」这一核心要素，尤其是我反复见证了它对于取得优异表现的关键作用。

### 183

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528753242087165952
互动: Likes: 30; Retweets: 4; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@colinraffel And I have scars from ppl repeatedly comparing Tesla datasets to other co datasets in size alone, when cleanliness is huge (we have a professional in-house labeling team to ensure it) and when other co data can often be just mountains of highway miles. Broken comparison.

@colinraffel 人们总是反复将特斯拉数据集（Tesla datasets）与其他公司的数据集（other co datasets）仅仅从规模上进行比较，这让我感到很困扰。要知道，数据清洁度（cleanliness）是至关重要的（我们有专业的内部标注团队来确保数据质量），而其他公司的数据可能常常只是堆积如山的公路里程。这种比较本身就是有缺陷的。

### 184

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528792715810394112
互动: Likes: 3,158; Retweets: 109; Replies: 183; Quotes: 46; Views: 0; Bookmarks: 244; isReply: 0

Something I've been doing for a few years that others often find amusing: PiOclock. I have a daily alarm set for 3:14pm and within that minute I take a photo of wherever I am / what I'm doing. Bottom bar of the photo is always proof with my watch. Here an example from 2 days ago. https://t.co/qdAmFn4X7n

我坚持了好几年的一个习惯，总能引人发笑：PiOclock。我每天下午 3:14 都设置闹钟，并在那一分钟里，拍下我所在位置和正在做的事情。照片底部总会显示我的手表，以此作为时间证明。下面是两天前的一个例子。https://t.co/qdAmFn4X7n

### 185

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528792718998052864
互动: Likes: 1,225; Retweets: 31; Replies: 39; Quotes: 0; Views: 0; Bookmarks: 30; isReply: 1

Usually we only take photos of the interesting/meaningful and miss out on the routine/mundane, which I've noticed one can really appreciate years down the road. PiOclock is close to random samples of life, is easy and amusing to practice, and creates awesome photo collages.

我们通常只记录那些有趣或有意义的时刻，却往往忽略了生活中那些司空见惯的日常瞬间。然而我发现，正是这些看似平淡的片段，在多年以后回看时，反而会让人倍感珍贵和欣赏。PiOclock 旨在捕捉生活中的随机样本，它不仅操作简单有趣，还能创作出令人惊叹的照片拼贴画。

### 186

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528794196810747905
互动: Likes: 65; Retweets: 0; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@RaychourRobin and light! &lt;3 it.

@RaychourRobin 和光！喜欢它。

### 187

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528796304364609536
互动: Likes: 520; Retweets: 20; Replies: 14; Quotes: 5; Views: 0; Bookmarks: 30; isReply: 1

@GailAlfarATX The photos are memory anchors. With an anchor you can pretty easily recall an entire event. Without an anchor many events become inaccessible. I am always surprised (and usually very happy) to recall an event that I feel I'd have completely forgotten about without the anchor.

@GailAlfarATX 照片就像记忆的锚点。
有了这些锚点，你就能相当轻松地回想起整个事件。
但如果没有了它们，许多事情就会变得无从想起。
我总是很惊讶（而且通常都非常高兴），能通过这些「锚点」回忆起一些我本以为会彻底遗忘的事情。

### 188

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528796755357147137
互动: Likes: 88; Retweets: 2; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@mintotsai oh for sure, basics.

@mintotsai 哦，当然，都是基础知识。

### 189

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528797026304962560
互动: Likes: 174; Retweets: 2; Replies: 5; Quotes: 1; Views: 0; Bookmarks: 16; isReply: 1

@buildoooor human memory is very good but uses some kind of a linked list data structure without random access

@buildoooor 人类记忆虽然非常出色，但它使用的是某种链表数据结构（linked list data structure），并且不支持随机访问（random access）。

### 190

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528797451590569985
互动: Likes: 37; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@toniengelhardt :D random samples of life 🤷‍♂️😂

@toniengelhardt :D 生活中就是充满各种随机啊 🤷‍♂️😂

### 191

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528807616989671425
互动: Likes: 37; Retweets: 0; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@AnnPortered I am right handed but I've always worn my watch on my right hand anyway. Feels right 🤷‍♂️

@AnnPortered 我是右撇子，但我一直都把手表戴在右手。感觉这样才对 🤷‍♂️

### 192

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528807616989671425
互动: Likes: 37; Retweets: 0; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@AnnPortered I am right handed but I've always worn my watch on my right hand anyway. Feels right 🤷‍♂️

@AnnPortered 我是右撇子，但我一直都把手表戴在右手上。感觉这样才习惯 🤷‍♂️

### 193

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528808361558306817
互动: Likes: 1,818; Retweets: 160; Replies: 183; Quotes: 22; Views: 0; Bookmarks: 597; isReply: 0

The software engineering aspect of deep learning repos I've been watching closely is how they store, catalogue, override, manage and plumb hyperparameter configs. Have come to dislike argparse, YAMLs (too inflexible), and fully enumerated kwargs on classes/defs. Any favorites?

我一直在密切关注深度学习项目在软件工程实践中的一个方面，那就是它们如何存储、编目、覆盖、管理和整合（plumb）超参数配置。现在我已经开始不喜欢 argparse、YAML（因为它太不灵活），以及在类 / 定义中完全枚举所有关键字参数（kwargs）的方式。大家有什么比较推荐的方法吗？

### 194

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528808361558306817
互动: Likes: 1,818; Retweets: 160; Replies: 183; Quotes: 22; Views: 0; Bookmarks: 597; isReply: 0

The software engineering aspect of deep learning repos I've been watching closely is how they store, catalogue, override, manage and plumb hyperparameter configs. Have come to dislike argparse, YAMLs (too inflexible), and fully enumerated kwargs on classes/defs. Any favorites?

我一直在密切关注深度学习项目在软件工程方面的一个重要考量：它们如何存储、编目、覆盖、管理以及集成超参数配置。我个人已经不太喜欢使用 argparse、YAML 文件（觉得它们太不灵活），也不喜欢在类或函数定义中完整列举所有 kwargs（关键字参数）。大家有没有比较推荐的配置管理方式呢？

### 195

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528811165597696000
互动: Likes: 43; Retweets: 1; Replies: 10; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@iandanforth I find that it would often be very convenient to do a little bit of lightweight computation in the config file

@iandanforth 我发现，在配置文件中进行一些轻量级计算通常会非常方便。

### 196

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528811165597696000
互动: Likes: 43; Retweets: 1; Replies: 10; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@iandanforth I find that it would often be very convenient to do a little bit of lightweight computation in the config file

@iandanforth 我发现，在配置文件中进行一些轻量级计算常常会非常方便。

### 197

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528812075186679809
互动: Likes: 80; Retweets: 1; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 24; isReply: 1

@jekbradbury that's the one I was going to try next, first saw it used in https://t.co/BJkky9V24i

@jekbradbury 我接下来正打算尝试的便是这个，它最初被提到或应用是在 https://t.co/BJkky9V24i

### 198

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528812075186679809
互动: Likes: 80; Retweets: 1; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 24; isReply: 1

@jekbradbury that's the one I was going to try next, first saw it used in https://t.co/BJkky9V24i

@jekbradbury 那正是我接下来想尝试的，最早是在 https://t.co/BJkky9V24i 上看到它的用法。

### 199

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528816201031307264
互动: Likes: 36; Retweets: 0; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@sea_snell Yes exactly, I was in process of building out my own little version of that. Just had the nagging fear that I am re-inventing the wheel.

@sea_snell 是的，没错，我当时正在开发自己的一个小版本。只是我一直担心，自己是不是在重复造轮子（re-inventing the wheel）。

### 200

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528816201031307264
互动: Likes: 36; Retweets: 0; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@sea_snell Yes exactly, I was in process of building out my own little version of that. Just had the nagging fear that I am re-inventing the wheel.

@sea_snell 确实如此，我当时正在着手开发一个类似的小型版本。只是我一直担心自己是不是在重新发明轮子。

### 201

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528816677424467968
互动: Likes: 18; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@themintsv honestly I don't hate it

@themintsv 说实话，我倒不讨厌它。

### 202

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528816677424467968
互动: Likes: 18; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@themintsv honestly I don't hate it

@themintsv 老实说，我并不反感它。

### 203

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528816851404218369
互动: Likes: 28; Retweets: 0; Replies: 8; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@PhilsburyDoboy @iandanforth yes but then you realize you'd potentially like some conditionals too. maybe for loops. and next thing you know you're re-inventing python

@PhilsburyDoboy @iandanforth 是的，但随后你会意识到自己可能也需要一些条件语句。也许是 for 循环。再往后你会发现，自己不知不觉中又在重新发明 Python 了。

### 204

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528816851404218369
互动: Likes: 28; Retweets: 0; Replies: 8; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@PhilsburyDoboy @iandanforth yes but then you realize you'd potentially like some conditionals too. maybe for loops. and next thing you know you're re-inventing python

@PhilsburyDoboy @iandanforth 是的，但接着你就会意识到你可能还需要一些条件语句。也许是 for 循环。然后你就会不知不觉地发现，你又重新发明了一遍 Python。

### 205

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528822937670717440
互动: Likes: 4; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 0

@uhcontrarian Agree! One single file, short interpretable and hackable.

@uhcontrarian 同意！一个文件，短小精悍、易于理解且方便修改。

### 206

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528822937670717440
互动: Likes: 4; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 0

@uhcontrarian Agree! One single file, short interpretable and hackable.

@uhcontrarian 同意！一个单独的文件，短小精悍、易于理解和修改。

### 207

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528824933685469184
互动: Likes: 1; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@EladRichardson @kfir99 except this doesn't allow you to do math/conditionals etc while setting up the config, I think?

@EladRichardson @kfir99 除非是在设置配置的时候，它似乎不让你进行数学运算或者设定条件等，我这样想对吗？

### 208

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528824933685469184
互动: Likes: 1; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@EladRichardson @kfir99 except this doesn't allow you to do math/conditionals etc while setting up the config, I think?

@EladRichardson @kfir99 但这似乎不允许你在设置配置时进行数学运算或条件判断等操作，我想是这样吧？

### 209

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528825413580886016
互动: Likes: 4; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@umuti5ik I like the simplicity of dict but I prefer dot access a lot more aesthetically, and a small few more bells and whistles like freezing.

我喜欢 `dict` 的简洁，但我更偏爱通过点号进行访问，觉得在美学上更合我心意，而且还带有一些额外的亮点功能，例如「冻结」。

### 210

作者: @karpathy
时间: 2022-05-23
链接: https://x.com/karpathy/status/1528825413580886016
互动: Likes: 4; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@umuti5ik I like the simplicity of dict but I prefer dot access a lot more aesthetically, and a small few more bells and whistles like freezing.

@umuti5ik 我喜欢 dict 的简洁，但从美观角度来说，我更偏爱点访问（dot access），而且也喜欢它再多一些像「冻结（freezing）」这样的附加功能（bells and whistles）。

### 211

作者: @karpathy
时间: 2022-05-24
链接: https://x.com/karpathy/status/1529163501888950272
互动: Likes: 25; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@tim_zaman Tim don't be that person from sama tweet this morning! :D An optimal solution exists and we will find it.    https://t.co/mOcK2jCEec

@tim_zaman Tim，可别变成今天早上 sama 推文里说的那种人啦！:D 最优解是存在的，我们一定会找到它。https://t.co/mOcK2jCEec

### 212

作者: @karpathy
时间: 2022-05-25
链接: https://x.com/karpathy/status/1529288843207184384
互动: Likes: 66; Retweets: 2; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 17; isReply: 1

@arankomatsuzaki totally missed title opportunity :D highly amusing result, it's a way of using the input space for computation you'd normally want in the hidden state, and instead of it done in activations it is done in the discrete tokens of that space. did not super see this coming.

@arankomatsuzaki 完全错失了一个起标题的好机会 :D 结果非常有趣。这是一种利用输入空间（input space）进行计算的新方法，而这种计算通常会在隐藏状态（hidden state）中完成。不同之处在于，它不是通过激活（activations）实现，而是通过该空间中的离散 Token（discrete tokens）完成。我确实没想到会这样。

### 213

作者: @karpathy
时间: 2022-05-25
链接: https://x.com/karpathy/status/1529514197121384448
互动: Likes: 487; Retweets: 50; Replies: 19; Quotes: 4; Views: 0; Bookmarks: 39; isReply: 0

A good example of what I mean when I refer to large language models (LLMs) as "alien artifacts". Obviously powerful, especially if you poke it just right.

这很好地诠释了我为何会将大语言模型（LLMs）比作「异星造物」。它们显然强大无比，尤其当你以恰当的方式探究它时，其能力更是显露无疑。

### 214

作者: @karpathy
时间: 2022-05-26
链接: https://x.com/karpathy/status/1529879490783981570
互动: Likes: 124; Retweets: 6; Replies: 5; Quotes: 2; Views: 0; Bookmarks: 3; isReply: 1

@savvyRL @andrey_kurenkov Large language models are whatever you prompt them to be :)

@savvyRL @andrey_kurenkov 大语言模型（Large Language Model）嘛，你给它们什么提示，它们就能变成什么样子 :)

### 215

作者: @karpathy
时间: 2022-05-26
链接: https://x.com/karpathy/status/1529890623582400512
互动: Likes: 5; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@asoare159 here you go https://t.co/24A4szNlmY

@asoare159 给您链接： https://t.co/24A4szNlmY

### 216

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531075553012248576
互动: Likes: 137; Retweets: 3; Replies: 9; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@jeremyphoward @DrRaviPatelJr @weights_biases Not a huge fan; Pollutes the namespace, potentially shadows variables, adds more indirection and makes it harder to find the actual code. More generally prefer more explicit and paranoid code by default

@jeremyphoward @DrRaviPatelJr @weights_biases 我个人不太喜欢这种做法；它会占用命名空间（namespace），还可能「遮蔽」(shadow）变量，增加了代码的间接性，导致我们更难找到实际的代码所在。总的来说，我更倾向于默认使用更明确、更严谨的代码风格。

### 217

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531089383285088256
互动: Likes: 70; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@shaneguML this is really funny :) and too real

@shaneguML 这真的很有趣 :）太真实了

### 218

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531296768243077120
互动: Likes: 11,835; Retweets: 904; Replies: 309; Quotes: 127; Views: 0; Bookmarks: 115; isReply: 0

Currently products brag about being "smart". Like my coffee cup warmer that had me download an app, sign up for an account and ask for location permissions before it would warm my coffee. A future where products brag about being "dumb" must be coming and can't come soon enough.

如今，许多产品都以「智能」作为卖点。比如我的咖啡杯加热器，它竟然要求我先下载一个应用、注册一个账户，甚至请求位置权限，然后才能帮我加热咖啡。或许，一个产品反而会宣称自己「笨拙」的未来迟早会到来，而且我巴不得它早点出现。

### 219

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531299230765043715
互动: Likes: 41; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@iCaleb7 incredible

@iCaleb7 太棒了！

### 220

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531309481748901893
互动: Likes: 39; Retweets: 1; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@amuellerml @internetofshit Yes I've followed them for a long time. We need more than a Twitter account for real change though. Maybe Amazon can add a prominently featured IQ field to each product so you can use it in search &amp; filter.

@amuellerml @internetofshit 是的，我关注他们很久了。不过，要实现真正的改变，我们需要的不仅仅是一个 Twitter 账号。也许 Amazon 可以在每个产品中添加一个显著标示的 IQ 值字段，这样你就可以在搜索和筛选时使用它。

### 221

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531329083111378945
互动: Likes: 349; Retweets: 10; Replies: 11; Quotes: 0; Views: 0; Bookmarks: 7; isReply: 1

I navigated to a Github repo (minGPT in my case) went Code &gt; create Codespace. This launched a VS Code pointed to a new VM, `nvidia-smi` showed an eager and waiting V100, I ran `https://t.co/oFceWzm8rk` and it just worked 🤯. Really looking fwd to Github releasing it more widely.

我访问了一个 Github 仓库（以 minGPT 为例），然后点击 Code > create Codespace。这启动了一个连接到新虚拟机（VM）的 VS Code 环境，`nvidia-smi` 显示有一块 V100 GPU 可用并处于待命状态，我运行了 `https://t.co/oFceWzm8rk`，它竟然就这么成功了 🤯。我真的非常期待 Github 能将这项功能更广泛地推广开来。

### 222

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531329082025160704
互动: Likes: 935; Retweets: 101; Replies: 19; Quotes: 21; Views: 0; Bookmarks: 130; isReply: 0

Just wanted to sing some praise for Github Codespaces https://t.co/CRcaYElQ1i . It's not available to individuals yet (esp GPU VMs), but it is by far the easiest way I've seen to "just get a GPU in the cloud" - from one button on a Github repo to an open VS Code few seconds later

我只想对 Github Codespaces https://t.co/CRcaYElQ1i 大加赞扬。尽管它目前尚未向个人用户开放（特别是带有 GPU 的虚拟机），但它是我所见过迄今为止最简单、便捷的「轻松获得云端 GPU（Graphics Processing Unit）」的方式 —— 只需在 Github 仓库中点击一个按钮，几秒钟后就能直接进入 VS Code 开发环境。

### 223

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531329084105494528
互动: Likes: 370; Retweets: 9; Replies: 14; Quotes: 4; Views: 0; Bookmarks: 13; isReply: 1

Would have been a life-changer during the times of CS231n. Half+ of the posts on our student forum were various "environment setup and getting the code to even run Q&amp;A", not anything related to deep learning.

回想 CS231n 课程那个时候，这（指某种技术或工具）简直能彻底改变我们的学习体验。当年，我们学生论坛上超过一半的帖子都是关于「环境配置和如何让代码跑起来」的各种疑难解答，真正与深度学习内容相关的讨论反而不多。

### 224

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531331242414600193
互动: Likes: 52; Retweets: 0; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@a_meta4 I don't find Colab flexible enough. Maybe I haven't explored its full potential but I want to develop software, not just run some forward pass demo. This means VS Code and all of its awesome configurations and extensions (esp copilot), terminal, jupyterlab, tensorboard, etc.

@a_meta4 我觉得 Colab 用起来不够灵活。也许我还没完全发掘它的潜力，但我的目标是开发真正的软件，而不仅仅是跑一些简单的演示程序（比如模型的前向传播演示）。这就意味着我更需要像 VS Code 这样的工具，以及它那些强大的配置和扩展（特别是 copilot），还有终端、jupyterlab 和 tensorboard 等一整套开发环境。

### 225

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531331525664378882
互动: Likes: 20; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@dsracoon A beautiful exercise to go through at a right time and place and optionally.

@dsracoon 这是一次美好的体验，可在合适的时间、地点，并根据个人意愿选择进行。

### 226

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531378803674476544
互动: Likes: 100; Retweets: 3; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@hardmaru This may be the funniest thing I’ve seen deep learning do, about ever

@hardmaru 这可能是我见过的深度学习做得最有趣的事情了，简直是史上之最。

### 227

作者: @karpathy
时间: 2022-05-30
链接: https://x.com/karpathy/status/1531419016991936512
互动: Likes: 123; Retweets: 16; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 17; isReply: 1

@ak92501 looks super cool, + code @  https://t.co/BkBL16X8P3 currently A100 fp16 with head dims 16, 32, 64

@ak92501 看起来超级酷，代码请看这里：https://t.co/BkBL16X8P3。目前它运行在 A100 GPU 上，采用 FP16（半精度浮点数），并且其头维度（head dims）设置为 16、32、64。

### 228

作者: @karpathy
时间: 2022-06-01
链接: https://x.com/karpathy/status/1532107664196415488
互动: Likes: 292; Retweets: 25; Replies: 20; Quotes: 8; Views: 0; Bookmarks: 52; isReply: 0

wtfpython https://t.co/fPkX4H8JIA was on HN few days ago but took some time to step through. Few short faves:
&gt;&gt;&gt; isinstance(True, int) # True
&gt;&gt;&gt; hash(float('inf')) # 314159 :O
&gt;&gt;&gt; hash(float('-inf')) # -314159 :OO
&gt;&gt;&gt; 'е' == 'e' # False (copy paste it, see ord)
&gt;&gt;&gt; import this

几天前，我在 HN（Hacker News）上看到了 wtfpython https://t.co/fPkX4H8JIA 这个项目，花了一些时间才仔细研究完。下面是一些我特别喜欢的简短例子：
`>>> isinstance（True，int）# True`
`>>> hash（float（'inf')）# 314159 :O`
`>>> hash（float（'-inf')）# -314159 :OO`
`>>> 'е' == 'e' # False（复制粘贴一下，看看它们的 ord 值)`
`>>> import this`

### 229

作者: @karpathy
时间: 2022-06-02
链接: https://x.com/karpathy/status/1532396514001051649
互动: Likes: 73; Retweets: 3; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@tomgara @petewarden 😂 I am endlessly amused by this. Reminds me of https://t.co/LHfM8R9PPx

@tomgara @petewarden 😂 我对此感到乐不可支。这让我想起了 https://t.co/LHfM8R9PPx

### 230

作者: @karpathy
时间: 2022-06-02
链接: https://x.com/karpathy/status/1532467682984857600
互动: Likes: 97; Retweets: 1; Replies: 12; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@echen Me too - gmail spam filter has gotten noticeably worse somewhere in the last small few months. For first time in years I get clearly spam emails making it to my inbox and more legitimate emails are marked as spam, sometimes from friends I've been in email threads with in the past

@echen 我也有同感 ——Gmail 的垃圾邮件过滤器（spam filter）在最近几个月里明显变差了。这是多年来我第一次收到那些显而易见的垃圾邮件堂而皇之地进入我的收件箱，而更多正常的邮件却被标记成了垃圾邮件，其中有些甚至来自过去与我有过邮件往来的朋友。

### 231

作者: @karpathy
时间: 2022-06-03
链接: https://x.com/karpathy/status/1532799739179061250
互动: Likes: 3,325; Retweets: 244; Replies: 99; Quotes: 33; Views: 0; Bookmarks: 168; isReply: 0

I am cautiously and slightly unnervingly looking forward to the gradual and inevitable unification of language, images/video and audio in foundation models. I think that's going to look pretty wild.

我既谨慎又有些忐忑地期待着，语言、图像 / 视频和音频在基础模型中逐渐走向不可避免的统一。我认为那将会带来相当惊人的景象。

### 232

作者: @karpathy
时间: 2022-06-03
链接: https://x.com/karpathy/status/1532809574184431616
互动: Likes: 959; Retweets: 35; Replies: 4; Quotes: 1; Views: 0; Bookmarks: 12; isReply: 1

Every task bolted on top will enjoy orders of magnitude more data-efficient training than what we are used to today.

在此基础上构建的每一个任务，都将比我们目前习惯的训练方式获得数个数量级更高的数据训练效率。

### 233

作者: @karpathy
时间: 2022-06-03
链接: https://x.com/karpathy/status/1532812086455062528
互动: Likes: 1,439; Retweets: 73; Replies: 95; Quotes: 14; Views: 0; Bookmarks: 31; isReply: 1

They will be endowed with agency over originally human APIs: screen+keyboard/mouse in the digital realm and humanoid bodies in the physical realm. And gradually they will swap us out.

它们将获得像人类一样操作 API（Application Programming Interface）的能力：在数字世界中，它们可以像操作屏幕、键盘和鼠标一样；在物理世界中，它们可以控制类人身体。它们将逐渐取代我们。

### 234

作者: @karpathy
时间: 2022-06-03
链接: https://x.com/karpathy/status/1532852924212072448
互动: Likes: 943; Retweets: 27; Replies: 51; Quotes: 8; Views: 0; Bookmarks: 69; isReply: 0

I have one note on iOS notes app where I add random ideas / thoughts / todos / questions one per line to the top as they happen. Once in a while I look at and pop interesting stuff upwards. Most sink down. I’d normally forget 75% of what’s on there and find the practice valuable.

在 iOS 备忘录应用中，我有一个专门的备忘录，用来记录各种随机的想法、思考、待办事项和问题。每当有新想法时，我都会把它们作为一行新内容添加到备忘录的顶部。偶尔我会回顾这些记录，并将其中有意思的内容「提」到上方。而大部分内容则会随着时间的推移沉到下方。通常情况下，我会忘记其中 75% 的内容，但我发现这种记录方式非常有价值。

### 235

作者: @karpathy
时间: 2022-06-03
链接: https://x.com/karpathy/status/1532858571490086912
互动: Likes: 31; Retweets: 1; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@tyleryzhu Archive movie (2020) watch

@tyleryzhu 观看 2020 年的存档电影

### 236

作者: @karpathy
时间: 2022-06-04
链接: https://x.com/karpathy/status/1532894698439774208
互动: Likes: 2,207; Retweets: 200; Replies: 117; Quotes: 59; Views: 0; Bookmarks: 96; isReply: 0

AGI is a feeling. Like love. Stop trying to define it.

通用人工智能（AGI）是一种感觉，就像爱一样。别再试图去定义它了。

### 237

作者: @karpathy
时间: 2022-06-06
链接: https://x.com/karpathy/status/1533870068018212864
互动: Likes: 1,688; Retweets: 97; Replies: 265; Quotes: 16; Views: 0; Bookmarks: 126; isReply: 0

Do brains build generative models all the way down to pixel level? I happened to get woken up this morning just as I was scrutinizing a visual detail in the dream, which gave me a strong sense that it does. Previously I've been less sure. Anyone else try to debug?

大脑会形成（Generative Models）生成模型，甚至细致到像素级别吗？我今天早上醒来时，恰好正在梦中仔细观察一个视觉细节，这让我强烈感觉到它确实如此。此前我一直不太确定。还有其他人有过类似的探究吗？

### 238

作者: @karpathy
时间: 2022-06-07
链接: https://x.com/karpathy/status/1534244307829288960
互动: Likes: 468; Retweets: 64; Replies: 7; Quotes: 3; Views: 0; Bookmarks: 147; isReply: 0

Nice intro and references to diffusion models, the latest and greatest in image generative modeling. 
Code based on lucidrains' heroic re-implementations, whom everyone should follow, support, cherish and sponsor here https://t.co/faZ6pjGvMI

这段内容精彩地介绍了 ** 扩散模型（Diffusion Models)**，并引用了相关资料。扩散模型是目前图像生成建模领域中最先进、最强大的技术。
这里的代码是基于 lucidrains 的出色复现，大家应该去他的主页关注、支持并赞助他：https://t.co/faZ6pjGvMI

### 239

作者: @karpathy
时间: 2022-06-09
链接: https://x.com/karpathy/status/1534931350934806530
互动: Likes: 30; Retweets: 1; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@ZHaqqee Something more subtle is probably going on. That our brains build such representations doesn't necessarily mean that you also get to use them arbitrarily with conscious access and manipulation at will. Seems like they probably exist (see dreams) but we can't consciously use them.

@ZHaqqee 事情可能比我们想象的更微妙。我们的大脑构建出这些表征（representations），但这并不意味着我们就能随心所欲地有意识地访问和操控它们。看起来这些表征可能确实存在（比如在梦境中），但我们却无法在清醒时有意识地利用它们。

### 240

作者: @karpathy
时间: 2022-06-10
链接: https://x.com/karpathy/status/1535317998549884929
互动: Likes: 72; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@pfau It's really interesting; May well be a preview more broadly

@pfau 这真的很有趣；这很可能预示着更广泛的趋势。

### 241

作者: @karpathy
时间: 2022-06-10
链接: https://x.com/karpathy/status/1535323166838403073
互动: Likes: 590; Retweets: 59; Replies: 6; Quotes: 0; Views: 0; Bookmarks: 74; isReply: 0

Incredible effort!! 👏😍

做得太棒了！！ 👏😍

### 242

作者: @karpathy
时间: 2022-06-10
链接: https://x.com/karpathy/status/1535343722287624192
互动: Likes: 200; Retweets: 19; Replies: 6; Quotes: 2; Views: 0; Bookmarks: 12; isReply: 1

imo a major AI safety contribution, both in short-term (applications) and long-term (AGI) scope

在我看来，这是一项重要的 AI 安全贡献，其意义兼顾短期（应用）和长期（通用人工智能 AGI）两个维度。

### 243

作者: @karpathy
时间: 2022-06-11
链接: https://x.com/karpathy/status/1535664103213002752
互动: Likes: 1,712; Retweets: 136; Replies: 61; Quotes: 20; Views: 0; Bookmarks: 197; isReply: 0

TIL there are professional Google Maps players. His TikTok has videos classifying places on Earth with surprisingly high accuracy from 0.1 seconds of a random street view image presentation. Would be interesting to train a ConvNet to compete, expect it would work well.

今天才知道，原来还有专业的 Google Maps 玩家。他的 TikTok 视频中展示了，在随机街景图像只显示 0.1 秒的情况下，他就能以惊人的高准确率识别地球上的地点。如果能训练一个卷积神经网络（ConvNet）来和他们比赛，应该会很有趣，而且预计效果会不错。

### 244

作者: @karpathy
时间: 2022-06-11
链接: https://x.com/karpathy/status/1535732176930471936
互动: Likes: 23; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@gwern Yep I remember this paper from long ago but had lost the exact reference! Seems like this is a kind of task that a modern network could be superhuman at. I’m very impressed with how good humans can become though

@gwern 是的，我很久以前就读过这篇论文，但具体出处却记不清了！看来，这似乎是现代神经网络（network）能够超越人类（superhuman）完成的一类任务。不过，对于人类所能达到的精湛水平，我依然感到非常震撼。

### 245

作者: @karpathy
时间: 2022-06-12
链接: https://x.com/karpathy/status/1536061915528429569
互动: Likes: 278; Retweets: 11; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 4; isReply: 1

Merely continuing to glide down the existing scaling laws with engineering alone to lower the loss (which has so far correlated with output magic of the emergent properties) is just the lower bound / worst case.

仅仅依靠工程学（engineering），沿着现有的缩放定律（scaling laws）继续优化，来降低模型损失（loss）(迄今为止，模型损失的降低与涌现特性（emergent properties）带来的惊人输出效果紧密相关），这只是最低限度或者说是最坏的情况。

### 246

作者: @karpathy
时间: 2022-06-12
链接: https://x.com/karpathy/status/1536061913376776192
互动: Likes: 3,122; Retweets: 418; Replies: 119; Quotes: 64; Views: 0; Bookmarks: 389; isReply: 0

1) What is LaMDA and What Does it Want? https://t.co/BZmYnDxXZR
2) Interview https://t.co/fgpHpdPTRa

What can be said with confidence imo is that things are about to get a lot weirder because models appear to follow smooth scaling laws and data+model size can still plenty grow. https://t.co/E1FdaG1OWt

1）LaMDA 是什么，它想要什么？ https://t.co/BZmYnDxXZR
2）采访 https://t.co/fgpHpdPTRa

我认为，可以肯定地说，未来会变得更加不可思议，因为模型似乎遵循平滑的缩放定律（scaling laws），而且数据量和模型规模仍有很大的增长空间。https://t.co/E1FdaG1OWt

### 247

作者: @karpathy
时间: 2022-06-12
链接: https://x.com/karpathy/status/1536062688832282625
互动: Likes: 401; Retweets: 21; Replies: 22; Quotes: 4; Views: 0; Bookmarks: 14; isReply: 1

My favorite parts of talking to large language models is when they are asked for insight (e.g. interpreting the poem) and reply with verifiably sensible and interesting analysis and ideas. Or another example when a model from a while ago explained jokes even better than I could.

我最喜欢与大语言模型（Large Language Model）交流的时刻，是当它们被问及提供洞察（例如解读诗歌）时，能够给出可验证的、合理且有趣的分析和见解。又比如，早些时候的一个模型甚至比我本人还能更好地解释笑话。

### 248

作者: @karpathy
时间: 2022-06-12
链接: https://x.com/karpathy/status/1536069094859911168
互动: Likes: 287; Retweets: 19; Replies: 11; Quotes: 2; Views: 0; Bookmarks: 9; isReply: 1

@elonmusk Haha excellent question / application. Sadly I've only seen a few limited snippets so far. Maybe @gwern creative fiction is closest, but is very... comprehensive https://t.co/kFYvthXHBJ. For now at least they seem quite good at explaining them: https://t.co/QgEh59yyIa

@elonmusk 哈哈，这个问题问得太好了，或者说这个应用场景很有趣！遗憾的是，我到目前为止只看到了些许零星片段。也许 @gwern 的创意小说是最接近的，但内容非常…… 详尽全面 https://t.co/kFYvthXHBJ。至少目前看来，它们（指 AI 模型）似乎很擅长解释自身： https://t.co/QgEh59yyIa

### 249

作者: @karpathy
时间: 2022-06-13
链接: https://x.com/karpathy/status/1536144115364028416
互动: Likes: 242; Retweets: 3; Replies: 6; Quotes: 0; Views: 0; Bookmarks: 4; isReply: 1

@SecureOwl @fastml_extra ok that can't be real :D

@SecureOwl @fastml_extra 噢，那不可能是真的吧 :D

### 250

作者: @karpathy
时间: 2022-06-14
链接: https://x.com/karpathy/status/1536764371765825536
互动: Likes: 846; Retweets: 66; Replies: 116; Quotes: 44; Views: 0; Bookmarks: 94; isReply: 0

Ok large language model-based dating app. Each person helps finetune their GPT imitator. GPTs talk to each other. A ranking model scores conversations on probability that the match turns out well. High ranking matches meet. i.e. tractable approximation of https://t.co/24Rz4WraMM

设想一款基于大语言模型（Large Language Model）的约会应用。在这个应用中，每个人都会各自对他们的 GPT 模拟器进行微调（finetune）。这些 GPT 模拟器会相互交流。接着，一个排名模型（ranking model）会给这些对话打分，判断匹配成功的可能性。最终，得分高的匹配对象将安排见面。这可以看作是对 https://t.co/24Rz4WraMM 所描述场景的一种可行且易于处理的近似（tractable approximation）实现。

### 251

作者: @karpathy
时间: 2022-06-14
链接: https://x.com/karpathy/status/1536767340360044544
互动: Likes: 428; Retweets: 31; Replies: 42; Quotes: 9; Views: 0; Bookmarks: 21; isReply: 1

More generally it is about to become possible to create approximate digital replicas of people - not just text but audio+video. That you can also tune and prompt. A bit like brain upload but lossy and approximate. The 2nd+ order effects of this are interesting to think about.

更普遍地说，我们很快就能创建出人的近似数字复制品 —— 不仅限于文本，还包括音频和视频。你还可以对这些复制品进行调整和提示（prompt）。这有点像将大脑上传，但它是带有损失且近似的版本。由此产生的二阶及以上效应，非常值得我们深入思考。

### 252

作者: @karpathy
时间: 2022-06-14
链接: https://x.com/karpathy/status/1536770282697871361
互动: Likes: 193; Retweets: 8; Replies: 28; Quotes: 1; Views: 0; Bookmarks: 11; isReply: 1

These people don't even have to be alive - e.g. talk to Plato. Or https://t.co/JnOeHjtXkP . Or they could be re-mixed, e.g. 50% you + 50% Plato. A lot of space for other ideas and exploration.

这些「人」甚至不必真实存在 —— 比如，你可以和柏拉图对话。或者像这个链接所示：https://t.co/JnOeHjtXkP。它们也可以被重新组合，例如 50% 的你加上 50% 的柏拉图。这为其他创意和探索提供了广阔的空间。

### 253

作者: @karpathy
时间: 2022-06-14
链接: https://x.com/karpathy/status/1536777075167621120
互动: Likes: 347; Retweets: 6; Replies: 12; Quotes: 1; Views: 0; Bookmarks: 16; isReply: 1

@AjdDavison I like to use "self-supervised" when the code looks exactly like supervised learning, except the labels are not coming from human labels but some automatic process (e.g. next word, or reconstruction).

@AjdDavison 我倾向于这样定义「自监督（Self-supervised）」：当一个代码实现看起来与监督学习（Supervised Learning）完全一样，但其标签并非源于人工标注，而是来自某个自动化过程时（例如，预测下一个词或进行重建），我就会使用「自监督」这个词。

### 254

作者: @karpathy
时间: 2022-06-14
链接: https://x.com/karpathy/status/1536832801722408962
互动: Likes: 10; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@ericjang11 yep, I recall that part of the book. But I feel like that would only be a minor aspect of that kind of technology manifesting in society more broadly.

@ericjang11 是的，我记得书里提到过那部分。但我感觉，这对于那种技术在社会中更广泛地普及来说，可能只是一个次要的方面。

### 255

作者: @karpathy
时间: 2022-06-14
链接: https://x.com/karpathy/status/1536853557416079361
互动: Likes: 36; Retweets: 2; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@cwarny good. the real galaxy brain moment is when you can just pretty please ask a GPT to do the task and see it oblige, potentially with no training whatsoever. this doesn't work just yet, but the way things are going it will.
https://t.co/NO4BSGmEcW

@cwarny 说得好。真正让人茅塞顿开的时刻是，你只需轻松地让一个 GPT（Generative Pre-trained Transformer）去完成某个任务，然后看到它照办，而且可能根本不需要任何训练。这目前还不行，但按照目前的趋势，这迟早会成为现实。
https://t.co/NO4BSGmEcW

### 256

作者: @karpathy
时间: 2022-06-14
链接: https://x.com/karpathy/status/1536859640985661440
互动: Likes: 482; Retweets: 13; Replies: 19; Quotes: 2; Views: 0; Bookmarks: 5; isReply: 1

@fchollet @elonmusk happy to!

@fchollet @elonmusk 乐意效劳！

### 257

作者: @karpathy
时间: 2022-06-16
链接: https://x.com/karpathy/status/1537245593923248129
互动: Likes: 97; Retweets: 4; Replies: 6; Quotes: 3; Views: 0; Bookmarks: 10; isReply: 1

@LiamFedus @shaneguML @_jasonwei @YiTayML @JeffDean @edchi @OriolVinyalsML @barret_zoph @colinraffel @percyliang @denny_zhou @MaartenBosma Naively, smooth lines feel like memorization and sharp lines feel like algorithms. Would be interesting to look at some tasks one by one in more detail to see if there is any structure in the individual examples that go from not working to working. For both classes of task.

@LiamFedus @shaneguML @_jasonwei @YiTayML @JeffDean @edchi @OriolVinyalsML @barret_zoph @colinraffel @percyliang @denny_zhou @MaartenBosma 直观来说，平滑的曲线似乎代表着记忆（memorization），而尖锐的线条则更像是算法（algorithms）的体现。对于这两类任务，如果能逐个更详细地分析那些从「不奏效」到「奏效」的案例，看看这些具体例子中是否存在某种结构性规律，那将会是一件非常有意思的事情。

### 258

作者: @karpathy
时间: 2022-06-16
链接: https://x.com/karpathy/status/1537253139564244994
互动: Likes: 28; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@LiamFedus @shaneguML @_jasonwei @YiTayML @JeffDean @edchi @OriolVinyalsML @barret_zoph @colinraffel @percyliang @denny_zhou @MaartenBosma it's a tiny bit of an algorithm if you squint enough ```f1 = sports_from_name; f2 = sports_from_action; a = f1(x); b = f2(x); return a == b ? plausible : implausible.``` but with that much squinting almost every other task would be... fascinating work &amp; questions!

@LiamFedus @shaneguML @_jasonwei @YiTayML @JeffDean @edchi @OriolVinyalsML @barret_zoph @colinraffel @percyliang @denny_zhou @MaartenBosma 如果你稍微眯一下眼，会发现这勉强算是一个小算法：```f1 = sports_from_name; f2 = sports_from_action; a = f1（x); b = f2（x); return a == b ? plausible ：implausible.``` 但如果都用这种「眯眼看」的方式，那几乎所有其他任务都会变成…… 真是引人入胜的工作和思考！

### 259

作者: @karpathy
时间: 2022-06-16
链接: https://x.com/karpathy/status/1537256149946273793
互动: Likes: 36; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@gwern I make fun of this phenomenon a bit in my Forward Pass short story. It's a very interesting exercise to add as context, but still unnerving to see the original behavior. https://t.co/bAyB1GBnVI

@gwern 我在我的短篇小说《Forward Pass》中，对这种现象稍作调侃。将它作为上下文背景加入，这本身是一个非常有趣的尝试，但亲眼看到原始行为时，仍然会让人感到不安。https://t.co/bAyB1GBnVI

### 260

作者: @karpathy
时间: 2022-06-16
链接: https://x.com/karpathy/status/1537486295848538117
互动: Likes: 495; Retweets: 47; Replies: 26; Quotes: 6; Views: 0; Bookmarks: 97; isReply: 0

Good thread. Imo it's not obvious that most of the "work" of forwarding neural nets in our chips is not computation but data movement. Nets are not "laid out" like brains. Instead, compute units iteratively chunk through tiny pieces of the forward pass. It's total emulation mode.

这个话题很有意思。在我看来，一个常被忽视的事实是：我们的芯片在运行神经网络时，大部分「工作」其实并非计算本身，而是数据的搬运和移动。神经网络在芯片中的「部署」方式，并不像大脑那样拥有复杂的物理结构。相反，计算单元（compute units）会反复地、小块小块地处理神经网络前向传播（forward pass）的微小部分。这整个过程，更像是一种彻底的模拟（emulation）模式。

### 261

作者: @karpathy
时间: 2022-06-16
链接: https://x.com/karpathy/status/1537501153830633473
互动: Likes: 61; Retweets: 2; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@sorenmind Like, eager to try. Uniform selection is still standard but feels very wasteful and a low bar. Presence of noisy/weird data foils naive attempts to improve. Appreciate nice code and tutorial.ipynb!

@sorenmind 我很想尝试一下。目前，均匀采样（Uniform selection）仍然是标准做法，但这感觉非常浪费，而且其门槛（low bar）较低。此外，噪声数据或异常数据（noisy/weird data）的存在，也阻碍了我们改进现有方法的初步尝试。非常感谢您提供的优质代码和 `tutorial.ipynb`！
</step3_3_refined_translation>

### 262

作者: @karpathy
时间: 2022-06-17
链接: https://x.com/karpathy/status/1537589479300423680
互动: Likes: 29; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 5; isReply: 1

@andyzengtweets Would love someone to redo SHRDLU https://t.co/7eivet7eNk , 50+ years later.

@andyzengtweets 希望有人能重做 SHRDLU https://t.co/7eivet7eNk ，在 50 多年后的今天。

### 263

作者: @karpathy
时间: 2022-06-17
链接: https://x.com/karpathy/status/1537932794755567618
互动: Likes: 16; Retweets: 3; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@StevenLevy "hydrocarbon bigotry". heard it here first.

@StevenLevy「碳氢化合物偏执」。这个说法我是第一次在这里听到。

### 264

作者: @karpathy
时间: 2022-06-18
链接: https://x.com/karpathy/status/1538245587128053760
互动: Likes: 37; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@borisdayma @l2k This was fun! 👏 amusing that the model was around for so long before it reached a critical “viral threshold” :)

@borisdayma @l2k 这太有趣了！👏 没想到这个模型问世了这么久，才终于达到了关键的「病毒式传播阈值」，开始火起来 :)

### 265

作者: @karpathy
时间: 2022-06-27
链接: https://x.com/karpathy/status/1541513914168487936
互动: Likes: 224; Retweets: 1; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 4; isReply: 1

@julien_c haha!♥️ my pleasure to contribute a silly little commit bug fix to the hottest AI repo :)

@julien_c 哈哈！♥️ 很荣幸能为这个最热门的 AI 仓库贡献一个微不足道的提交错误修复 :)

### 266

作者: @karpathy
时间: 2022-06-28
链接: https://x.com/karpathy/status/1541605355934822401
互动: Likes: 161; Retweets: 5; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@jackclarkSF Future extrapolations include: Adobe Photoshop. Hollywood.

@jackclarkSF 未来可能涉及的领域包括：Adobe Photoshop。好莱坞。

### 267

作者: @karpathy
时间: 2022-06-28
链接: https://x.com/karpathy/status/1541819670029668352
互动: Likes: 6; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Curious_Monkey7 @evolvingstuff @julien_c Lol use of quotes is my (style) bug while trying to fix the actual bug described up top

@Curious_Monkey7 @evolvingstuff @julien_c 哈哈，在尝试修复上面提到的实际错误时，我把引号用错了，这只是我的一个（风格）问题。

### 268

作者: @karpathy
时间: 2022-06-29
链接: https://x.com/karpathy/status/1542291710884995072
互动: Likes: 6; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@rmarcilhoo @renegadesilicon @ITNAmatter it's good stuff 👍

@rmarcilhoo @renegadesilicon @ITNAmatter 这很棒 👍

### 269

作者: @karpathy
时间: 2022-07-01
链接: https://x.com/karpathy/status/1542884612623372290
互动: Likes: 655; Retweets: 93; Replies: 27; Quotes: 5; Views: 0; Bookmarks: 75; isReply: 0

Large language models continuing their bit surprisingly rapid advances, here in solving math/STEM problems, without substantial architecture modifications or paradigm shifts. "The main novelty of this paper is a large training dataset", and fine-tuning on top of PaLM 540B.

大语言模型（Large Language Model）在解决数学 / STEM 问题方面持续取得令人惊讶的快速进展，而这并未依赖于重大的架构修改或范式转变。原文指出，「本文的主要创新之处在于采用了大型训练数据集」，并在 PaLM 540B 的基础上进行了微调。

### 270

作者: @karpathy
时间: 2022-07-01
链接: https://x.com/karpathy/status/1542886366073171968
互动: Likes: 415; Retweets: 23; Replies: 16; Quotes: 2; Views: 0; Bookmarks: 10; isReply: 1

It's just that... at one point the narrative was that solving math/STEM problems would look like converting to/from some formal grammar and running a special-purpose inference engine. That one can get so far just feeding raw text/LaTeX into a big transformer is highly amusing.

只是…… 曾几何时，主流的说法是，解决数学 / STEM 问题将涉及将其转换成或从某种形式语法转换出来，然后运行一个专用推理引擎。而如今，仅仅通过将原始文本 / LaTeX 输入一个大型 Transformer，就能取得如此大的进展，这着实令人忍俊不禁。

### 271

作者: @karpathy
时间: 2022-07-01
链接: https://x.com/karpathy/status/1542888110245089280
互动: Likes: 13; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@DrJimFan really?

@DrJimFan 真的吗？

### 272

作者: @karpathy
时间: 2022-07-07
链接: https://x.com/karpathy/status/1545092107840327680
互动: Likes: 842; Retweets: 90; Replies: 22; Quotes: 8; Views: 0; Bookmarks: 60; isReply: 0

Fun video (I missed earlier) on the behind-the-scenes of the #dalle2 Cosmopolitan cover. Final program: "A wide angle shot from below of a female astronaut with an athletic feminine body walking with swagger towards camera on mars in an infinite universe , synthwave digital art".

最近看到一个很有趣的视频（我之前错过了），它揭秘了 #dalle2 如何为《Cosmopolitan》杂志创作封面的幕后故事。最终用于生成图像的提示词是：「一个广角镜头，从下方仰视一位身材健美、充满女性魅力的女宇航员，她在无限的宇宙中，以自信的姿态在火星上走向镜头，整体呈现合成波数字艺术（synthwave digital art）风格。」

### 273

作者: @karpathy
时间: 2022-07-07
链接: https://x.com/karpathy/status/1545188796496375810
互动: Likes: 35; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@dribnet hah, fascinating! revealing the prompt (i.e. the "source code") is a way of open-sourcing the art and allowing others to fork and remix it.

@dribnet 哈哈，太有意思了！公开提示词（Prompt)（也就是我们常说的「源代码」），其实就是将艺术「开源」，让其他人可以像「分叉」代码一样去借鉴和「二次创作」这些作品。

### 274

作者: @karpathy
时间: 2022-07-08
链接: https://x.com/karpathy/status/1545203138080034816
互动: Likes: 896; Retweets: 69; Replies: 163; Quotes: 18; Views: 0; Bookmarks: 310; isReply: 0

Enumerated and sorted some sci-fi I've read over time https://t.co/e0NvnKfwt6 seeking more favorites!

我整理并列出了这些年来读过的一些科幻小说 https://t.co/e0NvnKfwt6 欢迎推荐更多好书！

### 275

作者: @karpathy
时间: 2022-07-08
链接: https://x.com/karpathy/status/1545204262463893505
互动: Likes: 71; Retweets: 1; Replies: 6; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@GailAlfarATX I've done a bit of both, but around 80% is read. For some books I even end up getting all 3 of: 1) digital copy, 2) physical copy, 3) audiobook 🤦‍♂️

@GailAlfarATX 我两种方式都有涉猎，但其中大约 80% 是阅读。对于某些书，我甚至最终会买齐这三种：1）数字版，2）实体版，3）有声书 🤦‍♂️

### 276

作者: @karpathy
时间: 2022-07-08
链接: https://x.com/karpathy/status/1545210461456437248
互动: Likes: 3; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@mElantkowski I can't remember it was a long time ago, I'll give it another shot.

@mElantkowski 我不记得了，那都是很久以前的事情了，我会再试一次。

### 277

作者: @karpathy
时间: 2022-07-08
链接: https://x.com/karpathy/status/1545210703140638720
互动: Likes: 15; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@aniketvartak The Egg is awesome. Highest amount of psychological impact per character.

@aniketvartak 《蛋》真是太棒了。它以最少的字数，带来了最大的心理冲击。

### 278

作者: @karpathy
时间: 2022-07-08
链接: https://x.com/karpathy/status/1545554828155179008
互动: Likes: 642; Retweets: 86; Replies: 24; Quotes: 12; Views: 0; Bookmarks: 118; isReply: 0

"torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision" https://t.co/vP0RuImY8e haha. Actually torch.cuda.manual_seed is also what you need. But clearly 3407 looks like the top rng seed to use :)

"torch.manual_seed（3407）就够了：论随机种子对计算机视觉深度学习架构的影响」https://t.co/vP0RuImY8e 哈哈。实际上，你还需要 torch.cuda.manual_seed。但很明显，3407 似乎是最受欢迎的随机数生成器（rng）种子 :)

### 279

作者: @karpathy
时间: 2022-07-09
链接: https://x.com/karpathy/status/1545565873988988928
互动: Likes: 444; Retweets: 28; Replies: 25; Quotes: 3; Views: 0; Bookmarks: 31; isReply: 0

Merged a sizable refactor branch (38 commits) to minGPT master https://t.co/79S9lShJRN . Can now load pertained GPT2 checkpoints. Added a few notebooks/demos/tests, e.g. a generation demo. Here's what 'gpt2-xl' (1.5B) thinks/knows about me via prompt "Andrej Karpathy, the..." hah https://t.co/3zQUzo3OuZ

我已将一个包含 38 次提交（commits）的大型重构分支合并到 minGPT 项目的主分支 https://t.co/79S9lShJRN。现在，它能够加载预训练的 GPT2 检查点（checkpoints）。我还增加了一些 Jupyter notebook 示例、演示和测试文件，例如一个生成演示。这是拥有 15 亿参数的 'gpt2-xl' 模型通过提示词（prompt)「Andrej Karpathy，the...」对我的「看法」或「认知」，颇有趣味 https://t.co/3zQUzo3OuZ

### 280

作者: @karpathy
时间: 2022-07-09
链接: https://x.com/karpathy/status/1545567015007424512
互动: Likes: 14; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@compulyze haha! they are all the exact same length actually, but counted in byte pair encoding _tokens_. Each token can be variably short/long in number of characters it decodes to. So that line is shorter because it generated more "short" tokens e.g. probably around "CEO of OOAK Research"

@compulyze 哈哈！实际上，它们的长度是完全相同的，但这里我们是根据字节对编码（Byte Pair Encoding）的 Token（Token）数量来计算的。每个 Token 在解码成字符时，其长度是可变长的。所以，那一行看起来更短，是因为它生成了更多「短」Token，比如「CEO of OOAK Research」这段文字可能就包含了很多这类短小的 Token。

### 281

作者: @karpathy
时间: 2022-07-09
链接: https://x.com/karpathy/status/1545602045566017536
互动: Likes: 16; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 0

@Mvandepanne Huge congratulations!!! :) 👏👏👏

@Mvandepanne 衷心祝贺！！！ :）👏👏👏

### 282

作者: @karpathy
时间: 2022-07-09
链接: https://x.com/karpathy/status/1545844575046053889
互动: Likes: 511; Retweets: 48; Replies: 25; Quotes: 8; Views: 0; Bookmarks: 104; isReply: 0

"I should have loved biology" https://t.co/xJ9dYA33yo Good, though I felt the same way about almost all other subjects too. It is considered good and proper form to enumerate information in a breadth-first manner.

「我本应该热爱生物学」https://t.co/xJ9dYA33yo 这篇文章写得不错，不过我回想起来，自己对几乎所有其他科目也都有类似的感觉。一般来说，以广度优先（breadth-first）的方式来列举信息，被认为是一种恰当且有效的方法。

### 283

作者: @karpathy
时间: 2022-07-12
链接: https://x.com/karpathy/status/1546683284452544512
互动: Likes: 648; Retweets: 58; Replies: 43; Quotes: 5; Views: 0; Bookmarks: 104; isReply: 0

Spent a chunk of today reverse-engineering and integrating GPT-2 byte pair encoder into minGPT https://t.co/7YxtpsZJHd . Tokenizers are maybe the (hidden) most complex, unintuitive parts of today's language models. There was a good post I lost link to on some of their subtleties.

今天，我花了很多时间对 GPT-2 字节对编码器（byte pair encoder）进行逆向工程，并将其集成到 minGPT 中 https://t.co/7YxtpsZJHd。分词器（Tokenizer）也许是当今大语言模型（Large Language Model）中（那些不为人知的）最复杂、也最反直觉的部分。我之前看过一篇很棒的帖子，专门探讨了它们的一些微妙之处，可惜链接现在找不到了。

### 284

作者: @karpathy
时间: 2022-07-12
链接: https://x.com/karpathy/status/1546690733322543104
互动: Likes: 64; Retweets: 2; Replies: 14; Quotes: 3; Views: 0; Bookmarks: 4; isReply: 1

@fpingh It's a nice one! (but no) "Tokenization is a surprisingly complex topic once you start to get into the finer details of each model.  It seems like it is it's own separate research area" +1. In the future we'll be rendering text and feeding it to pure vision-only models anyway.

@fpingh（虽然这个想法不错，但)「分词（Tokenization）是一个令人意想不到的复杂话题，一旦你开始深入了解每个模型的具体细节，就会发现它似乎是一个独立的专门研究领域」+1。未来，我们将会把文本渲染成图像，然后将其输入给纯视觉模型。

### 285

作者: @karpathy
时间: 2022-07-12
链接: https://x.com/karpathy/status/1546906438475268096
互动: Likes: 439; Retweets: 63; Replies: 8; Quotes: 2; Views: 0; Bookmarks: 44; isReply: 0

Congrats to the BigScience team!! 🎉 4 months of training.
More info:
https://t.co/nWr1lOOuCL
Technical logs:
https://t.co/afiPsCvMVC
I believe you can forward on HF Hub, or if you have an 8XA10080GB node lying around :). But offloading work is ongoing, evaluation too. Cool!!

祝贺 BigScience 团队！！🎉 历经 4 个月的训练。
更多信息：
https://t.co/nWr1lOOuCL
技术日志：
https://t.co/afiPsCvMVC
我相信你可以在 HF Hub 上部署或使用这个模型，或者如果你手头有一个 8XA10080GB 节点。:）不过，负载卸载（offloading）和评估工作都还在进行中。太棒了！！

### 286

作者: @karpathy
时间: 2022-07-12
链接: https://x.com/karpathy/status/1546911018005065728
互动: Likes: 12; Retweets: 0; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@gwern Yes, that's the one!! (two :)). There is a lot more that could be covered too, e.g. the lack of re.IGNORECASE  repercussions. Also not sure why some apostrophes 's, 'd, ... are special cased. Or effects on handling of non-whitespace-separated languages.

@gwern 是的，就是那个！没错（不止一个，而是两个 :）。还有许多可以探讨的方面，例如未考虑 `re.IGNORECASE` （忽略大小写）可能带来的影响。我也不确定为什么像's、'd 等这类撇号会受到特殊处理。此外，对于非空白符分隔的语言，这种处理方式会产生怎样的影响，也值得进一步研究。

### 287

作者: @karpathy
时间: 2022-07-12
链接: https://x.com/karpathy/status/1546917332189995008
互动: Likes: 18; Retweets: 4; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 6; isReply: 1

@Kupusoglu @gwern oh didn't realize, two posts from @nostalgebraist:
1) bpe blues: https://t.co/XV3OhrPYjL
2) bpe blues+: https://t.co/vZ5R5lqteP

@Kupusoglu @gwern 原来如此，来自 @nostalgebraist 的两篇文章值得关注：
1）关于 BPE（Byte Pair Encoding）的一些困扰和思考，题为《bpe blues》：https://t.co/XV3OhrPYjL
2）对 BPE（Byte Pair Encoding）问题的进一步探讨，题为《bpe blues+》：https://t.co/vZ5R5lqteP

### 288

作者: @karpathy
时间: 2022-07-12
链接: https://x.com/karpathy/status/1546932032751616001
互动: Likes: 7; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@rantlab @gwern see one of my deeper replies in the thread

@rantlab @gwern 请看我在讨论串中某个更深入的回复。

### 289

作者: @karpathy
时间: 2022-07-13
链接: https://x.com/karpathy/status/1547311319879000064
互动: Likes: 61; Retweets: 5; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@DNA_RNA_Uni I was curious what #dalle2 had to say :D https://t.co/hShJihK6ba

@DNA_RNA_Uni 我很好奇 #dalle2 生成了什么内容 :D https://t.co/hShJihK6ba

### 290

作者: @karpathy
时间: 2022-07-13
链接: https://x.com/karpathy/status/1547314866473472000
互动: Likes: 653; Retweets: 60; Replies: 11; Quotes: 8; Views: 0; Bookmarks: 135; isReply: 0

Mind blown by the DALL•E 2 Prompt Book. An instruction manual for the text box.

DALL·E 2 提示书让我大开眼界。它就像是一本针对文本框的说明手册。

### 291

作者: @karpathy
时间: 2022-07-13
链接: https://x.com/karpathy/status/1547316346106458112
互动: Likes: 208; Retweets: 4; Replies: 12; Quotes: 1; Views: 0; Bookmarks: 5; isReply: 1

(though there's clearly a lot more potential than just a text box, for a photoshop v2)

(不过，对于未来的 Photoshop v2 来说，它的潜力显然远不止一个文本框那么简单)

### 292

作者: @karpathy
时间: 2022-07-13
链接: https://x.com/karpathy/status/1547332301519851520
互动: Likes: 6,129; Retweets: 162; Replies: 284; Quotes: 46; Views: 0; Bookmarks: 31; isReply: 1

I have no concrete plans for what’s next but look to spend more time revisiting my long-term passions around technical work in AI, open source and education.

我目前还没有明确的下一步计划，但希望能投入更多时间，重新追求我长期以来对 AI（人工智能）领域的技术工作、开源项目以及教育事业的热情。

### 293

作者: @karpathy
时间: 2022-07-13
链接: https://x.com/karpathy/status/1547332300186066944
互动: Likes: 23,799; Retweets: 1,093; Replies: 883; Quotes: 691; Views: 0; Bookmarks: 274; isReply: 0

It’s been a great pleasure to help Tesla towards its goals over the last 5 years and a difficult decision to part ways. In that time, Autopilot graduated from lane keeping to city streets and I look forward to seeing the exceptionally strong Autopilot team continue that momentum.

在过去的 5 年里，能帮助 Tesla 达成其目标，我感到非常荣幸，而做出离开的决定则非常艰难。在那段时间里，Autopilot（自动辅助驾驶系统）从最初的车道保持功能，发展到如今能够在城市街道上运行。我期待看到这个极其出色的 Autopilot 团队能继续保持这种强劲的发展势头。

### 294

作者: @karpathy
时间: 2022-07-13
链接: https://x.com/karpathy/status/1547341166512771072
互动: Likes: 2,053; Retweets: 184; Replies: 71; Quotes: 46; Views: 0; Bookmarks: 17; isReply: 1

@PrvnKalavai Important to keep in mind that the Autopilot team is hundreds of strong engineers who very much know what they're doing, just don't have my public visibility. I was only one part of that effort and I think get an outsized spotlight cast on me because I do.

@PrvnKalavai 重要的是要记住，Autopilot 团队由数百名能力出众的工程师组成，他们非常清楚自己在做什么，只是不像我一样拥有公众知名度。我只是那项工作的一部分，我认为自己之所以获得了不成比例的关注，是因为我拥有这种公众可见性。

### 295

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548766160434241536
互动: Likes: 7,366; Retweets: 484; Replies: 346; Quotes: 79; Views: 0; Bookmarks: 548; isReply: 0

Obviously ppl should carry a CO2 monitor at all times :) Outside air is ~400ppm, stuffy room ~1000+. CO2 ppm is proxy for how much other people's air you're breathing (~covid risk). Thinking gets hazier at 1000+. Meeting rooms and bedrooms can climb much higher than you'd expect.

看来，大家确实应该随身携带一个二氧化碳监测仪 :）室外空气的二氧化碳浓度大约是 400ppm，而密闭、不通风的房间里可能会达到 1000ppm 以上。二氧化碳（CO2）浓度（以 ppm 为单位）可以间接反映你吸入了多少别人的「废气」（也与感染新冠等疾病的风险相关）。当浓度超过 1000ppm 时，人的思维会变得迟钝。会议室和卧室的二氧化碳浓度往往比你想象的要高得多。

### 296

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548768303279050752
互动: Likes: 57; Retweets: 0; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 12; isReply: 1

@alex_teichman I use and like aranet4 and like it, but haven't done extensive research / comparison.

@alex_teichman 我在使用 aranet4，而且很喜欢它，但我还没有做过深入的研究或比较。

### 297

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548768997234995200
互动: Likes: 75; Retweets: 2; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@leafmuncher Yes, saw it climb to as high as ~3000. But saw variation too, depending on the plane, place, and over time (for some reason they turn down the circulation for a few minutes, then ramp it back up). Not sure how much the covid-co2 correlation breaks due to air filters.

@leafmuncher 是的，我看到它最高能攀升到约 3000。不过，我也观察到这个数值会有变化，具体取决于飞机型号、所处位置以及时间（由于某些原因，他们会先调低空气循环几分钟，然后再调高）。我不确定空气过滤器能在多大程度上削弱新冠病毒与二氧化碳浓度之间的关联。

### 298

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548769935479824384
互动: Likes: 15; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@trengarajan @migueldeicaza I was surprised that my bedroom regularly climbed to almost 2000. Leaving the window open will steady state the room to a reasonable ~600. Was also surprised how quickly smallish meetings rooms with few people can climb up. Had to work with EHS to crank up HVACs.

@trengarajan @migueldeicaza 我很惊讶，我卧室的（二氧化碳浓度）经常会升高到接近 2000 PPM （百万分之）。开着窗户可以将房间（的二氧化碳浓度）稳定在一个比较合理的约 600 PPM。我也很惊讶，即使是小型会议室，人不多（二氧化碳浓度）也能迅速升高。我不得不与环境、健康和安全部门 （EHS，Environment，Health，and Safety）合作，调高供暖、通风和空调系统 （HVAC，Heating，Ventilation，and Air Conditioning）的运作强度。

### 299

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548770720863305728
互动: Likes: 186; Retweets: 0; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@NCSLovi Would do a lot of good for the world imo, and make a real dent into covid spread.

@NCSLovi 我认为，这将对世界大有好处，并且能真正有效遏制新冠传播。

### 300

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548772263922610176
互动: Likes: 126; Retweets: 9; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 55; isReply: 1

@danaugrs @VitalikButerin Cool, wasn't aware, his backpack post is awesome more generally https://t.co/lNzjCCZk8F

@danaugrs @VitalikButerin 太棒了，我之前没注意到，他那篇关于背包的帖子在更广的意义上也很精彩 https://t.co/lNzjCCZk8F

### 301

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548776412143702016
互动: Likes: 44; Retweets: 3; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@passionfingerz that's awesome, the security theater around exhaustively wiping down all the surfaces (while ignoring air co2 ppm) has been perplexing for an airborne respiratory virus.

@passionfingerz 这太棒了，想到（对一种空气传播的呼吸道病毒）那些围绕着彻底擦拭所有表面的「安全秀」(security theater），却忽视了空气中的二氧化碳浓度（ppm），这真是令人费解。

### 302

作者: @karpathy
时间: 2022-07-17
链接: https://x.com/karpathy/status/1548791831793524736
互动: Likes: 38; Retweets: 1; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@AwokeKnowing @NCSLovi It obviously doesn't stop covid. I am in favor of simple public health practices (e.g. proper ventilation) to reduce the spread of unpleasant-at-best respiratory illness - covid, flu, common cold, etc that exist today or later.

@AwokeKnowing @NCSLovi 显然，这并不能完全阻断新冠病毒的传播。我个人支持采取一些简单的公共卫生措施（例如，保持适当的通风），以有效减少包括新冠、流感和普通感冒等呼吸道疾病的扩散。这些疾病无论在当下还是未来，都可能带来不适，因此采取预防措施至关重要。

### 303

作者: @karpathy
时间: 2022-07-18
链接: https://x.com/karpathy/status/1549082681291390977
互动: Likes: 57; Retweets: 1; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 8; isReply: 1

@devonzuegel haha! &lt;3 the video a lot 🥹
(https://t.co/bDaffLS3uz for others)

@devonzuegel 看到这个视频，我感到非常喜欢和感动。
(https://t.co/bDaffLS3uz 方便大家查阅)

### 304

作者: @karpathy
时间: 2022-07-18
链接: https://x.com/karpathy/status/1549100500800311296
互动: Likes: 10; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@devonzuegel @devonzuegel is there any "state of the art" you're aware of when it comes to Chobaniland?

@devonzuegel @devonzuegel 关于 Chobaniland，你有没有了解的「最新进展」或「前沿动态」？

### 305

作者: @karpathy
时间: 2022-07-18
链接: https://x.com/karpathy/status/1549125465176039424
互动: Likes: 592; Retweets: 68; Replies: 11; Quotes: 7; Views: 0; Bookmarks: 110; isReply: 0

Great post on the technical challenges of training a 176B Transformer Language Model. ~10 years ago you'd train neural nets on your CPU workstation with Matlab. Now need a compute cluster and very careful orchestration of its GPU memory w.r.t. both limits and access patterns.

这篇帖子深入探讨了训练一个 176B Transformer 语言模型所面临的技术挑战。回溯到大约 10 年前，人们通常会在自己的 CPU 工作站上用 Matlab 来训练神经网络。而如今，这项任务则需要一个计算集群，并且必须极其精心地协调其 GPU 内存，兼顾内存容量限制和数据访问模式。

### 306

作者: @karpathy
时间: 2022-07-18
链接: https://x.com/karpathy/status/1549129090401112064
互动: Likes: 838; Retweets: 101; Replies: 24; Quotes: 10; Views: 0; Bookmarks: 161; isReply: 0

For people wondering why, as a "vision person", I am interested in language models:
1) the distinctions of different areas of AI are blurring very fast, see my earlier tweet thread: https://t.co/cJPYotUl3Z
2) language models are engines of generalization: https://t.co/5eBiViyh18

对于那些好奇我这个「视觉领域研究者」为什么会关注语言模型（language model）的朋友：
1）人工智能（AI）不同领域之间的界限正在迅速模糊，大家可以参考我之前的推文串： https://t.co/cJPYotUl3Z
2）语言模型是实现通用能力的重要驱动力： https://t.co/5eBiViyh18

### 307

作者: @karpathy
时间: 2022-07-18
链接: https://x.com/karpathy/status/1549133877825810433
互动: Likes: 32; Retweets: 2; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@EMostaque @MetaAI 💯 something to normalize :). Papers with code. And online inference demo. And logbook (*new*! :D).

@EMostaque @MetaAI 💯 这些都应该常态化 :)：带有代码的论文、在线推理演示，以及日志本（* 新 *! :D）。

### 308

作者: @karpathy
时间: 2022-07-19
链接: https://x.com/karpathy/status/1549184192486903808
互动: Likes: 2,381; Retweets: 126; Replies: 146; Quotes: 14; Views: 0; Bookmarks: 34; isReply: 0

I have a theory that 90% of physical mail volume is total spam and 90% of phone call volume is total spam (and people waiting on the line for a customer service representative). Societal entropy and bloat.

我有一个猜想：90% 的实体邮件量都是垃圾邮件，而 90% 的电话呼叫量也都是垃圾电话（或者是在线等待客服代表的通话）。这反映出社会中无用的「熵增」和过度「膨胀」。

### 309

作者: @karpathy
时间: 2022-07-22
链接: https://x.com/karpathy/status/1550590820545310720
互动: Likes: 339; Retweets: 14; Replies: 17; Quotes: 5; Views: 0; Bookmarks: 11; isReply: 1

Another amusing part is that I've always thought that neural net "thinking" would look like some uninterpretable jumble of activations in a hidden state of some RNN++. But here much of it is in natural language, in the input space! We get interpretable "stack traces" of thought.

更有趣的是，我一直以为神经网络（neural net）的「思考」过程，会表现为某个 RNN++ 模型隐藏状态（hidden state）中一团难以解读的激活信号。然而，在这里，这些「思考」大部分以自然语言的形式，直接呈现在输入空间（input space）中！我们因此得到了可解释的「思维痕迹」，就像程序运行时的「堆栈轨迹」(stack traces）一样。

### 310

作者: @karpathy
时间: 2022-07-22
链接: https://x.com/karpathy/status/1550590819421151234
互动: Likes: 188; Retweets: 9; Replies: 4; Quotes: 1; Views: 0; Bookmarks: 5; isReply: 1

Via these techniques LLMs may end up coordinating entire internal dialogs when necessary, similar to human problem solving. E.g.: ok let me look up some stuff first. now here's a few attempts. hmm some of these don't look right. these 3 ideas all lead to similar answer, maybe it.

通过运用这些技术，大语言模型（LLM）在必要时最终能够协调其内部的整个对话过程，这和人类解决问题的方式很相似。比如，大语言模型可能会「思考」：好的，让我先查找一些资料。现在我有一些尝试性的解决方案。嗯，其中有些似乎不太对劲。这三个想法都指向了类似的答案，也许这正是正确方向。

### 311

作者: @karpathy
时间: 2022-07-22
链接: https://x.com/karpathy/status/1550590818041311232
互动: Likes: 609; Retweets: 75; Replies: 8; Quotes: 6; Views: 0; Bookmarks: 212; isReply: 0

Language Model Cascades https://t.co/eLmZDToMq6
Good paper and all the references (chain-of-thought, scratchpad, bootstrapping, verifiers, tool-use, retrievals, etc...). There's a quickly growing stack around/above a single large language model, expanding their reasoning power

语言模型级联 https://t.co/eLmZDToMq6
这是一篇很棒的论文，其中也涵盖了所有相关的参考文献（例如思维链（chain-of-thought）、草稿本（scratchpad）、自举（bootstrapping）、验证器（verifiers）、工具使用（tool-use）、检索（retrievals）等）。目前，围绕着或超越单一的大语言模型（Large Language Model），一个快速发展的技术生态正在形成，极大地扩展了它们的推理能力。

### 312

作者: @karpathy
时间: 2022-07-23
链接: https://x.com/karpathy/status/1550865754060247040
互动: Likes: 654; Retweets: 44; Replies: 35; Quotes: 4; Views: 0; Bookmarks: 84; isReply: 0

Is someone aware of a language model experiment where you keep all the 2022 goodies/data, except swap a Transformer for an LSTM? I expect a gap should exist and is worth thinking about more closely, e.g. from the perspective of being both 1) expressive and 2) SGD optimizable.

有没有人知道这样一个语言模型实验：保留 2022 年所有的良好训练数据和条件，但将 Transformer（Transformer）架构替换成了 LSTM（Long Short-Term Memory）架构？我猜测两者之间应该存在性能差距，并且这个差距值得我们深入探讨，例如从模型是否同时具备 1）强大表达能力和 2）适合通过随机梯度下降（SGD）优化这两个角度来思考。

### 313

作者: @karpathy
时间: 2022-07-23
链接: https://x.com/karpathy/status/1550873727214096384
互动: Likes: 31; Retweets: 1; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 5; isReply: 1

@ethanCaballero Got it, I think I'm a bit more interested in _why_, e.g. via ablations that span hybrid architectures between and around the two. Shorter paths from output to all inputs (shallow compute graph)? Lack of "tailed" non-linearities (sigmoid/tanh)? MHSA? LayerNorms? etc.

@ethanCaballero 我明白了。我想我更感兴趣的是「为什么」会出现这种情况，例如，我们可以通过对介于两者之间及其周边的混合架构进行消融研究（ablation study）来探究。究竟是输出到所有输入的路径更短（即采用了浅层计算图）？还是缺乏诸如 sigmoid 或 tanh 这类「有尾部」（指其输出有界）的非线性激活函数？亦或是多头自注意力机制（MHSA)？或是层归一化（LayerNorms）等等原因呢？

### 314

作者: @karpathy
时间: 2022-07-23
链接: https://x.com/karpathy/status/1550903912038670336
互动: Likes: 350; Retweets: 14; Replies: 14; Quotes: 0; Views: 0; Bookmarks: 49; isReply: 1

(randomly triggered while reading Animal Eyes, which is quite excellent https://t.co/Cq4Hrc1tl7)

(在阅读《动物之眼》时偶然触发的想法，这本书非常棒 https://t.co/Cq4Hrc1tl7)

### 315

作者: @karpathy
时间: 2022-07-23
链接: https://x.com/karpathy/status/1550903910633680897
互动: Likes: 1,469; Retweets: 106; Replies: 150; Quotes: 16; Views: 0; Bookmarks: 92; isReply: 0

Human vision extracts only a tiny amount of information from surrounding EM radiation. Sensitive to narrow wavelength band. Nowhere near a full spectrogram, just ~gaussian sampled at 3 (SML) frequencies. With ok resolution in fovea. Without polarization. At just 2 points. Sad ;(

人类视觉仅能从周围的电磁辐射（EM radiation）中提取到微乎其微的信息。我们只对一个狭窄的波长范围敏感。这远非一个完整的频谱图（spectrogram），它只是在大约 3 个（SML，即短波长、中波长、长波长）频率上进行的高斯采样（gaussian sampled）。我们视网膜中央凹（fovea）的分辨率还算可以。此外，我们的眼睛也无法感知偏振（polarization）。而且，我们只有两只眼睛。这些都说明了人类视觉的局限性，令人感到有些遗憾。

### 316

作者: @karpathy
时间: 2022-07-23
链接: https://x.com/karpathy/status/1550906508262531073
互动: Likes: 5; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@jaschasd Agree, it's very dense in interesting.

@jaschasd 同意，内容非常丰富有趣。

### 317

作者: @karpathy
时间: 2022-07-23
链接: https://x.com/karpathy/status/1550907262713024512
互动: Likes: 196; Retweets: 5; Replies: 26; Quotes: 0; Views: 0; Bookmarks: 4; isReply: 1

@michael_nielsen It's like okay. I want the full light field, at high resolution, with full spectrograph and polarization. Is that so much to ask for, evolution?...

@Michael Nielsen 行吧，我想要完整的光场，高分辨率，带有完整的光谱仪和偏振。进化啊，这要求太多了吗？...

### 318

作者: @karpathy
时间: 2022-07-23
链接: https://x.com/karpathy/status/1550908910491471872
互动: Likes: 72; Retweets: 3; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@ChrSzegedy @michael_nielsen Yeah, "friggin' awesome" is not part of the process. Evolution very srs.

@ChrSzegedy @michael_nielsen「简直棒极了」这样的感叹并非科学过程的一部分。演化（Evolution）是一个极其严肃（srs）的问题。

### 319

作者: @karpathy
时间: 2022-07-28
链接: https://x.com/karpathy/status/1552707569629552640
互动: Likes: 447; Retweets: 43; Replies: 13; Quotes: 2; Views: 0; Bookmarks: 72; isReply: 0

Cool thread/links, all of these feel like little individual tools in a new "photoshop v2", as I've been calling it. I'm curious what fraction of imminent economy is the creation and appreciation of art. And in the limit how distinguishable it is from wireheading.

这些帖子或链接都很有意思，其中的各项内容都像是新版「Photoshop v2」（我一直这样称呼它）里的一个个小工具。我很好奇，在未来的经济中，艺术的创作和鉴赏会占据多大比重。以及从长远来看，这与「脑部快感刺激」（wireheading）之间又有多大区别。

### 320

作者: @karpathy
时间: 2022-07-29
链接: https://x.com/karpathy/status/1553063985158582273
互动: Likes: 133; Retweets: 16; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 61; isReply: 1

@chlassner @labmlai My current favorites for "top hype" are 
1) https://t.co/24A4szNlmY
2) https://t.co/IuT0Oddism
I removed top hype from arxiv-sanity because it was the most expensive section to maintain and (1) and (2) exist. arxiv-sanity is now best for more specific areas of otherwise low hype.

@chlassner @labmlai 我目前关于「热门趋势」的首选是：
1）https://t.co/24A4szNlmY
2）https://t.co/IuT0Oddism
我将 arxiv-sanity 中的「热门趋势」部分移除了，因为它是维护成本最高的一个版块，而且（1）和（2）已能提供类似功能。现在，arxiv-sanity 最适合用于那些关注度不高但更为具体的领域。

### 321

作者: @karpathy
时间: 2022-07-29
链接: https://x.com/karpathy/status/1553064475300753408
互动: Likes: 3; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@chlassner @labmlai I certainly received more questions than I expected from people who basically only used arxiv-sanity for its top hype page alone. I'm on a fence about re-introducing it (but leaning no) in a world where (1) and (2) work perfectly great.

@chlassner @labmlai 我确实收到了比我预想的更多的问题，这些问题大多来自那些仅仅是把 arxiv-sanity 当作其「热门趋势页面」（top hype page）来使用的人。在一个现有方案（1）和（2）都运行得非常好的背景下，我还在犹豫是否要重新引入这项功能（但我个人倾向于不重新引入）。

### 322

作者: @karpathy
时间: 2022-07-30
链接: https://x.com/karpathy/status/1553459582982246401
互动: Likes: 686; Retweets: 25; Replies: 31; Quotes: 5; Views: 0; Bookmarks: 49; isReply: 0

Fun episode as usual, of a podcast I’ve started to consistently look forward to

和往常一样，这一集播客很有趣，它已经成为我一直期待收听的节目。

### 323

作者: @karpathy
时间: 2022-07-30
链接: https://x.com/karpathy/status/1553468300343906304
互动: Likes: 117; Retweets: 3; Replies: 8; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@mmakki96 @theallinpod Haha favorite bestie changes per episode (eg this one Friedberg? :)), over long time probably Chamath, has a way of pulling back and teaching inline with the content. Common sentiment but very much enjoy the group as a whole, mostly.

@mmakki96 @theallinpod 哈哈，最喜欢的朋友（嘉宾）每集都在变（比如这集是 Friedberg？:)），但从长远来看，我最喜欢的大概是 Chamath，他总能适时抽身出来，结合内容进行讲解。这是普遍的看法，但我个人大部分时候都非常喜欢这个团队。

### 324

作者: @karpathy
时间: 2022-08-08
链接: https://x.com/karpathy/status/1556710388393119744
互动: Likes: 286; Retweets: 23; Replies: 6; Quotes: 2; Views: 0; Bookmarks: 54; isReply: 0

ty @jackclarkSF for continuining the Import AI newsletter, one of my favorites, good links in this week's issue https://t.co/OvA63sNxHe

感谢 @jackclarkSF 持续更新 Import AI 时事通讯，这是我最喜欢的一个，本周这期有很多不错的链接 https://t.co/OvA63sNxHe

### 325

作者: @karpathy
时间: 2022-08-11
链接: https://x.com/karpathy/status/1557774783500038144
互动: Likes: 62; Retweets: 2; Replies: 6; Quotes: 2; Views: 0; Bookmarks: 11; isReply: 1

@Suhail And technically using PyTorch isn't even close to "from scratch" :) But it is a good layer of abstraction to hang around. Sadly PyTorch is succumbing to entropy, it has basically become completely opaque. Finding implementation for the simplest things is now basically impossible.

@Suhail 从技术上讲，使用 PyTorch 离「从头开始」还差得很远 :）但它提供了一个很好的抽象层，值得我们去使用。遗憾的是，PyTorch 正在变得越来越混乱复杂，它基本上已经变得完全不透明了。现在想找到一些最简单功能的实现方式，几乎是不可能的。

### 326

作者: @karpathy
时间: 2022-08-11
链接: https://x.com/karpathy/status/1557777042866724865
互动: Likes: 4; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@xqcdp @Suhail Actually yes George has very much the correct insight

@xqcdp @Suhail 没错，George 的见解非常正确。

### 327

作者: @karpathy
时间: 2022-08-11
链接: https://x.com/karpathy/status/1557777262543376384
互动: Likes: 21; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 4; isReply: 1

@Suhail @jeremyphoward exactly, i've always thought of it as "unlocking" prod tools

@Suhail @jeremyphoward 没错，我一直觉得这就像是「解锁」生产工具。

### 328

作者: @karpathy
时间: 2022-08-11
链接: https://x.com/karpathy/status/1557778652523155458
互动: Likes: 16; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@xqcdp @Suhail one more way viable approach I think is keeping torch.Tensor but re-writing the rest and sticking to Python

@xqcdp @Suhail 我认为另一种可行的办法是保留 torch.Tensor，但重写其他部分，并且仅限于使用 Python。

### 329

作者: @karpathy
时间: 2022-08-11
链接: https://x.com/karpathy/status/1557854886720458752
互动: Likes: 74; Retweets: 6; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 18; isReply: 1

@jeremyphoward @Suhail @numba_jit It's useful at some point but also hard to get into at intermediate level. I found NVIDIA's CUDA docs to be low quality and books I'm aware of outdated. A few random lectures/repos here and there were helpful. Afaict CUDA expertise seems to spread on mostly apprenticeship model.

@jeremyphoward @Suhail @numba_jit CUDA 在某些时候确实很有用，但对于中级水平的学习者来说，却很难深入掌握。我发现 NVIDIA 的 CUDA 文档质量不高，而我了解的一些相关书籍也大多已经过时。只有一些零星分布的讲座或代码仓库（repos）能提供一些帮助。据我所知，CUDA 的专业知识似乎主要通过学徒模式（apprenticeship model）进行传播。

### 330

作者: @karpathy
时间: 2022-08-13
链接: https://x.com/karpathy/status/1558569874447671296
互动: Likes: 1,048; Retweets: 56; Replies: 113; Quotes: 22; Views: 0; Bookmarks: 51; isReply: 0

Earth as a dynamical system is a really bad computer. A lot of information processing is concentrated in a few tiny compute nodes (brains, chips) with terrible interconnects, even as bad as use of physical translation and air pressure waves. And powered primitively by combustion.

如果把地球看作一个动力系统（dynamical system），那么它实在是一个非常糟糕的「计算机」。因为地球上的大量信息处理工作，仅仅集中在少数几个微小的计算节点（比如我们的大脑、电子芯片）中。而且，这些节点之间的连接方式也极其拙劣，甚至差劲到需要依赖物理位移和空气压力波（比如通过语音、文字、交通运输等）来传递信息。更原始的是，它主要依靠燃烧（比如化石燃料）来获取能量。

### 331

作者: @karpathy
时间: 2022-08-13
链接: https://x.com/karpathy/status/1558570960483020800
互动: Likes: 432; Retweets: 18; Replies: 28; Quotes: 1; Views: 0; Bookmarks: 5; isReply: 1

Earth is a fire-powered computer, biology and technology.

地球就像一台由「火」驱动的计算机，其组成包括了生物和技术。

### 332

作者: @karpathy
时间: 2022-08-13
链接: https://x.com/karpathy/status/1558575780136857600
互动: Likes: 9; Retweets: 3; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@Dmojavensis If you look at today alone most of the information processing is powered by fire (combustion). Chips from the electric grid (burning fossil fuels, mostly) and life from aerobic respiration (burning food, mostly).

@Dmojavensis 如果单看今天，大部分的信息处理都离不开「火」（即燃烧反应）的驱动。比如，芯片的运行依赖于电网供电 （而电网的电力主要来自化石燃料的燃烧），生命的维持则依赖于有氧呼吸 （也就是「燃烧」食物）。
</step3_ref_translation>

### 333

作者: @karpathy
时间: 2022-08-13
链接: https://x.com/karpathy/status/1558577397540089857
互动: Likes: 96; Retweets: 6; Replies: 2; Quotes: 2; Views: 0; Bookmarks: 4; isReply: 1

@codeMnky01 The physical laws and initial conditions of Universe spontaneously create computers that look back. If there is anything to look at. If not then it's some kind of a cruel joke lol.

宇宙的物理定律和初始条件，自发地创造出了能够回溯（审视、理解或计算自身）的计算机。如果宇宙中真有什么值得观察或探究的事物还好说，但如果不是这样，那这大概就是某种残酷的玩笑了吧，哈哈。

### 334

作者: @karpathy
时间: 2022-08-13
链接: https://x.com/karpathy/status/1558578359688261633
互动: Likes: 218; Retweets: 10; Replies: 11; Quotes: 1; Views: 0; Bookmarks: 7; isReply: 0

Mostly what I think about when I look at the stars. Actually potentially pretty funny.

当我仰望星空时，我主要思考的就是这些。说实话，这可能还挺有意思的。

### 335

作者: @karpathy
时间: 2022-08-13
链接: https://x.com/karpathy/status/1558581172438835201
互动: Likes: 37; Retweets: 3; Replies: 1; Quotes: 2; Views: 0; Bookmarks: 1; isReply: 1

@sbtnmichael Yeah... I think you're kind of forced to not exactly draw boundaries and consider the Earth as one computer. Of course Earth is coupled to the rest of it but the coupling feels so much weaker that the abstraction makes sense.

@sbtnmichael 是的…… 我想我们多少有点不得不模糊边界，把地球视作一台单一的计算机。当然，地球与宇宙的其他部分存在耦合，但这种耦合感非常微弱，以至于这种抽象化（abstraction）的处理方式是完全合理的。

### 336

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558615851141439489
互动: Likes: 1,024; Retweets: 104; Replies: 11; Quotes: 2; Views: 0; Bookmarks: 90; isReply: 0

stunning possibilities

激动人心的可能性

### 337

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558616611778572289
互动: Likes: 578; Retweets: 65; Replies: 7; Quotes: 4; Views: 0; Bookmarks: 85; isReply: 0

Great interview, thank you @EMostaque, https://t.co/Ua4aGRz4PZ team and collaborators for blessing us with #stablediffusion. I was able to download and forward the model on my GPU. Super fun, though I am still a newbie prompt engineer (below: a lush treehouse #solarpunk). https://t.co/iEbp0FLTTe

这次采访非常棒，感谢 @EMostaque、https://t.co/Ua4aGRz4PZ 团队和合作者，为我们带来了 #stablediffusion。我成功地在我的 GPU 上下载并运行了这个模型。这太有趣了，尽管我仍然是一个新手提示工程师 （prompt engineer） （下面：一个郁郁葱葱的树屋 #solarpunk）。https://t.co/iEbp0FLTTe

### 338

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558635532267057153
互动: Likes: 7; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 0

@Jeff_Aronson @EMostaque there's infinite variation available for any prompt, each forward pass a different result

@Jeff_Aronson @EMostaque 对于任何提示，都有无限多的变化，每一次前向传播（forward pass）都会产生不同的结果。

### 339

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558863432023126020
互动: Likes: 1,009; Retweets: 83; Replies: 41; Quotes: 10; Views: 0; Bookmarks: 82; isReply: 0

There's something deep and borderline unintuitive about most real-world problems just happening to be (informally) NP-Complete: hard to solve but easy to verify a solution to. It's this asymmetry that makes progress possible, as culture can record previous computational work.

大多数现实世界的问题恰好（非正式地）是 NP - 完全（NP-Complete）的，这其中蕴含着深刻且有些反直觉的特点：它们本身很难解决，但要验证一个解决方案却非常容易。正是这种不对称性，使得我们在解决问题上能够取得进步，因为人类社会可以记录和传承之前的计算工作。

### 340

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558867301838909441
互动: Likes: 52; Retweets: 4; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@AgustinLebron3 Exactly. This property that also naturally casts our knowledge into a block chain, with compute nodes (people) striving to solve puzzles, broadcasting proof of work (solutions) to the network and claiming rewards.

@AgustinLebron3 说得没错。这种特性也自然而然地将我们的知识塑造成一个 block chain，其中计算节点（也就是人们）致力于解决各类谜题，向网络发布工作证明（即他们找到的答案），并以此获得奖励。

### 341

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558879658484985857
互动: Likes: 601; Retweets: 48; Replies: 60; Quotes: 4; Views: 0; Bookmarks: 35; isReply: 0

my favorite #stablediffusion past time atm is sampling #solarpunk utopias with happy people and animals living in high-tech harmony with nature :). Except finding it to be hard work and I'm not great at it. Where can I hire a prompt engineer to help create better versions... 🤔 https://t.co/mqKWEfAwV9

我目前最喜欢的 #stablediffusion 消遣，就是生成 #solarpunk 风格的乌托邦场景：那里有快乐的人和动物，与高科技和谐地生活在大自然中 :）。不过，我发现这工作还挺费劲，而且自己也不是很擅长。我在哪里能雇到一位提示工程师（prompt engineer）来帮忙创作出更棒的版本呢？🤔 https://t.co/mqKWEfAwV9

### 342

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558882828602855425
互动: Likes: 14; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@TechRonic9876 unsavory

@TechRonic9876 不愉快的

### 343

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558894425215930368
互动: Likes: 8; Retweets: 0; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Feni__Sam bleh i lost it, it was something like "painting of a beautiful #solarpunk village with happy families and animals and solar panels"

@Feni__Sam 唉，我把它弄丢了，大概是这样的描述：「一幅美丽的 #solarpunk（太阳朋克）村庄画卷，画面中有幸福的家庭、可爱的动物以及随处可见的太阳能电池板。」

### 344

作者: @karpathy
时间: 2022-08-14
链接: https://x.com/karpathy/status/1558894692502188032
互动: Likes: 5; Retweets: 1; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@Feni__Sam found it: python scripts/txt2img.py --prompt "a beautiful painting of a lush solarpunk village with solar panels and happy families and animals playing outside #solarpunk #cottagecore" --plms --n_iter 2 --n_samples 4 --seed 1337

@Feni__Sam 发现了这个：python scripts/txt2img.py --prompt「一幅美丽的画作：一个郁郁葱葱的太阳朋克（solarpunk）村庄，配有太阳能电池板，幸福的家庭和动物们在户外玩耍 #solarpunk #cottagecore」--plms --n_iter 2 --n_samples 4 --seed 1337

### 345

作者: @karpathy
时间: 2022-08-15
链接: https://x.com/karpathy/status/1559274851864416256
互动: Likes: 542; Retweets: 43; Replies: 16; Quotes: 0; Views: 0; Bookmarks: 12; isReply: 0

Unknown to the world, Charles Babbage also designed and forged an artificial neural network machine in secret... (fanfiction #stablediffusion) https://t.co/0UVYQXP66q

世人所不知的是，Charles Babbage 还曾秘密设计并研制了一台人工神经网络（Artificial Neural Network）机器……（同人小说 #stablediffusion）https://t.co/0UVYQXP66q

### 346

作者: @karpathy
时间: 2022-08-15
链接: https://x.com/karpathy/status/1559276540168327168
互动: Likes: 4; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@paulctan @liuliu honestly I never really fully understood how that allegedly happened

@paulctan @liuliu 老实说，我从来没真正弄明白那事儿到底是怎么发生的。

### 347

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559343616270557184
互动: Likes: 2,450; Retweets: 323; Replies: 71; Quotes: 62; Views: 0; Bookmarks: 251; isReply: 0

why settle for a few images from #stablediffusion when you can slowly walk your way around the sample space and create hyponotic videos you can't look away from? In this 2min video (~1hr to render on A100) I'm smoothly interpolating between random noise inputs into the model. https://t.co/A4Ue1pqoMo

当你可以缓慢地探索样本空间，创造出令人着迷、让你欲罢不能的视频时，又何必只满足于 #stablediffusion 生成的几张图片呢？在这个 2 分钟的视频中 （在 A100 上渲染约 1 小时），我展示了如何通过在模型中对随机噪声输入进行平滑插值来实现这一效果。https://t.co/A4Ue1pqoMo

### 348

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559343619235991553
互动: Likes: 358; Retweets: 25; Replies: 5; Quotes: 3; Views: 0; Bookmarks: 81; isReply: 1

hacky code here if anyone (with access to the model weights, GPU and time) wants to make their own dreams https://t.co/vWad1DuLVL

这里有一些可供实验的代码，如果有人具备模型权重（model weights）、图形处理器（GPU）且有足够的时间，想自己动手生成「梦境」或创意作品的话，可以参考：https://t.co/vWad1DuLVL

### 349

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559344065564385281
互动: Likes: 249; Retweets: 9; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

prompt was "ultrarealistic steam punk neural network machine in the shape of a brain, placed on a pedestal, covered with neurons made of gears. dramatic lighting. #unrealengine"

极致真实的蒸汽朋克风神经网络机器（neural network machine），呈大脑形态，置于基座之上，表面覆盖着由齿轮制成的神经元。戏剧性的光影效果。#unrealengine

### 350

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559345712743084032
互动: Likes: 34; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@dmvaldman yeah absolutely can be done, e.g. see @xsteenbrugge work. here i was more curious what happens when you dream a fixed prompt

@dmvaldman 是的，这绝对可以实现，例如可以看看 @xsteenbrugge 的相关工作。不过，我在这里更想探讨的是，当你让一个系统「想象」或「生成」一个固定的提示（prompt）时，会发生什么。

### 351

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559350261788790784
互动: Likes: 12; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 5; isReply: 0

@scottlegrand Sorry I'm sure this will be available for many people soon. Stable diffusion https://t.co/tnTrqbOBPo is about to be released more widely, then someone has to wrap this code (or similar) into a usable service. The cost of a video like this would currently be around ~$1 of compute.

@scottlegrand 不好意思，我确定这项技术很快就会对很多人开放了。Stable diffusion https://t.co/tnTrqbOBPo 即将更广泛地发布，到那时，就会有人把这段代码（或类似的）封装成一个易于使用的服务。目前，制作这样一个视频所需的计算资源成本大约是 1 美元。

### 352

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559351829036642304
互动: Likes: 207; Retweets: 9; Replies: 23; Quotes: 0; Views: 0; Bookmarks: 7; isReply: 1

I feel like Twitter compressed the video too much, so I tried uploading to YouTube as well https://t.co/ywu28r1x8b , with mixed results (?). 
Anyway, will leave run overnight to produce ~10min dream of a prompt, send suggestions :)

我感觉 Twitter 把视频压缩得太多了，所以我尝试也上传到了 YouTube https://t.co/ywu28r1x8b ，但效果似乎好坏参半。
无论如何，我会让它通宵运行，生成一个大约 10 分钟由提示词（prompt）生成的「梦想」画面。欢迎大家提出建议 :)

### 353

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559355125482680320
互动: Likes: 20; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@BabaBrinkman Haha yeah ofc, I’ll set the video to cc

哈哈，对啊，当然，我会把视频设置成字幕。

### 354

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559389958921547776
互动: Likes: 17; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@altryne agree with you I was being lazy, please go ahead! (it's under CC)

@altryne 我同意你，之前是我偷懒了，请继续吧！（它在知识共享许可协议（Creative Commons）下)

### 355

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559589334449152000
互动: Likes: 424; Retweets: 28; Replies: 7; Quotes: 6; Views: 0; Bookmarks: 25; isReply: 0

_Dramatically_ greater creativity of AI art is possible when the model weights are available, creates opportunities for arbitrary experiments (e.g. my steampunk NN video, or work of @xsteenbrugge, @genekogan, @runwayml +many others), many other objectives / optimization styles.

当模型权重可用时，AI 艺术的创造力能得到显著提升，这为进行任意实验（例如我的蒸汽朋克 NN 视频，或 @xsteenbrugge、@genekogan、@runwayml 以及许多其他人的作品），以及探索多种其他目标和优化风格，都创造了可能。

### 356

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559589334449152000
互动: Likes: 424; Retweets: 28; Replies: 7; Quotes: 6; Views: 0; Bookmarks: 25; isReply: 0

_Dramatically_ greater creativity of AI art is possible when the model weights are available, creates opportunities for arbitrary experiments (e.g. my steampunk NN video, or work of @xsteenbrugge, @genekogan, @runwayml +many others), many other objectives / optimization styles.

当模型权重（model weights）开放时，AI 艺术的创造力可以得到_极大的_提升。这为进行各种自由探索性的实验（例如我的蒸汽朋克 NN 视频，以及 @xsteenbrugge、@genekogan、@runwayml 等众多创作者的作品），以及探索许多其他目标和优化方式创造了机会。

### 357

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559589335896244224
互动: Likes: 225; Retweets: 11; Replies: 19; Quotes: 3; Views: 0; Bookmarks: 9; isReply: 1

also here's my A100 dreaming of "blueberry spaghetti" the entire night :D https://t.co/QuqAICMZ1P

顺便一提，我的 A100 整晚都在梦里吃「蓝莓意大利面」呢 :D https://t.co/QuqAICMZ1P

### 358

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559589335896244224
互动: Likes: 225; Retweets: 11; Replies: 19; Quotes: 3; Views: 0; Bookmarks: 9; isReply: 1

also here's my A100 dreaming of "blueberry spaghetti" the entire night :D https://t.co/QuqAICMZ1P

另外，我的 A100 显卡整晚都在「梦」着「蓝莓意大利面」呢 :D https://t.co/QuqAICMZ1P

### 359

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559672720895254528
互动: Likes: 1,243; Retweets: 45; Replies: 31; Quotes: 1; Views: 0; Bookmarks: 69; isReply: 1

If you know Python, have a vague recollection of taking some derivatives in your high school, watch this video and not understand backpropagation and the core of neural nets by the end then I will eat a shoe :D

如果你懂 Python，对高中学过的导数（derivatives）还略有印象，那么看完这个视频后，如果你仍然不理解反向传播（backpropagation）以及神经网络（neural nets）的核心原理，我可要「吃鞋」了哦 :D

### 360

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559672719414681601
互动: Likes: 7,727; Retweets: 1,158; Replies: 110; Quotes: 129; Views: 0; Bookmarks: 2,956; isReply: 0

!!!! Ok I recorded a (new!) 2h25m lecture on "The spelled-out intro to neural networks and backpropagation: building micrograd" https://t.co/KQ23lQW1BT . 
This is the culmination of about 8 years of obsessing about the best way to explain neural nets and backprop.

!!!! 各位，我录制了一堂（新的！）时长 2 小时 25 分钟的讲座，深入讲解了「神经网络（neural networks）和反向传播（backpropagation）：构建 micrograd」https://t.co/KQ23lQW1BT。
这堂课凝聚了我大约 8 年来潜心钻研如何以最佳方式阐释神经网络和反向传播的全部心血。

### 361

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559672720895254528
互动: Likes: 1,243; Retweets: 45; Replies: 31; Quotes: 1; Views: 0; Bookmarks: 69; isReply: 1

If you know Python, have a vague recollection of taking some derivatives in your high school, watch this video and not understand backpropagation and the core of neural nets by the end then I will eat a shoe :D

如果你懂 Python，对高中学过的导数知识有些模糊的印象，那么看完这个视频后，要是你还不理解反向传播（backpropagation）和神经网络（neural nets）的核心原理，我就要吃一双鞋了 :D

### 362

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559672719414681601
互动: Likes: 7,727; Retweets: 1,158; Replies: 110; Quotes: 129; Views: 0; Bookmarks: 2,956; isReply: 0

!!!! Ok I recorded a (new!) 2h25m lecture on "The spelled-out intro to neural networks and backpropagation: building micrograd" https://t.co/KQ23lQW1BT . 
This is the culmination of about 8 years of obsessing about the best way to explain neural nets and backprop.

我录制了一个（新的！）时长 2 小时 25 分钟的讲座，名为「神经网络和反向传播的详细介绍：构建 micrograd」https://t.co/KQ23lQW1BT。
这是我近 8 年来，致力于寻找解释神经网络（neural networks）和反向传播（backpropagation）的最佳方式所凝结的成果。

### 363

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559690727348613120
互动: Likes: 76; Retweets: 3; Replies: 2; Quotes: 2; Views: 0; Bookmarks: 12; isReply: 1

@radenmuaz the top-level idea/philosophy behind the repo is excellent. the low-level code itself was difficult to understand when i stared it a few days ago. geohot's recent "tiny tour of tinygrad" did not help lol.

@radenmuaz 这个代码库（repo）背后的核心理念（philosophy）非常棒。不过，几天前我研究它的时候，觉得底层的代码本身确实很难理解。geohot 最近那期「tinygrad 的微型之旅」视频，说实话，也没能帮上什么忙，真是令人啼笑皆非（lol）。

### 364

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559690727348613120
互动: Likes: 76; Retweets: 3; Replies: 2; Quotes: 2; Views: 0; Bookmarks: 12; isReply: 1

@radenmuaz the top-level idea/philosophy behind the repo is excellent. the low-level code itself was difficult to understand when i stared it a few days ago. geohot's recent "tiny tour of tinygrad" did not help lol.

@radenmuaz 这个代码仓库（repo）的核心理念（top-level idea/philosophy）非常出色。不过，我几天前研究它的低层代码（low-level code）时，发现理解起来有些困难。就连 geohot 最近的「tiny tour of tinygrad」教程也没能帮上什么忙，哈哈。

### 365

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559691116353503232
互动: Likes: 11; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@voxelbased @realGeorgeHotz yes ofc https://t.co/m7FMfoZ6Q0

@voxelbased @realGeorgeHotz 是的，当然了。https://t.co/m7FMfoZ6Q0

### 366

作者: @karpathy
时间: 2022-08-16
链接: https://x.com/karpathy/status/1559691116353503232
互动: Likes: 11; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@voxelbased @realGeorgeHotz yes ofc https://t.co/m7FMfoZ6Q0

@voxelbased @realGeorgeHotz 是的，当然了 https://t.co/m7FMfoZ6Q0

### 367

作者: @karpathy
时间: 2022-08-17
链接: https://x.com/karpathy/status/1559774561964437504
互动: Likes: 11; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 0

@VishalYesudas @WholeMarsBlog I don't even remember that channel, yeah I think it's something old where I used it for Stanford vision lab

@VishalYesudas @WholeMarsBlog 我甚至都不记得那个频道了，是的，我想那是我以前为 Stanford vision lab 用过的一个老账号（或频道）吧。

### 368

作者: @karpathy
时间: 2022-08-17
链接: https://x.com/karpathy/status/1559948711886696449
互动: Likes: 97; Retweets: 10; Replies: 6; Quotes: 3; Views: 0; Bookmarks: 4; isReply: 1

(I left my A100 dream of the same prompt last night and produced this longer (slightly higher quality?) video and with music https://t.co/ndOW3UgXZW)

(我昨晚让我的 A100 根据相同的提示词生成了这个更长 （质量似乎也稍高一点？）的视频，而且还配上了音乐 https://t.co/ndOW3UgXZW)

### 369

作者: @karpathy
时间: 2022-08-17
链接: https://x.com/karpathy/status/1559988001119186944
互动: Likes: 10; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@landon_pond The neural net takes two inputs: 1 the prompt and 2 a random noise vector, and produces an image. You can hold the prompt fixed and just sample many different noises, each will give a different image. In this video I start with a random noise input and then change it very slowly.

@landon_pond 神经网络（neural net）会接收两个输入：一是提示（prompt），二是随机噪声向量（random noise vector），然后生成一张图像。你可以固定提示不变，只采样许多不同的噪声向量，这样每次都会生成不同的图像。在这个视频中，我先使用一个随机噪声作为输入，然后非常缓慢地改变它。

### 370

作者: @karpathy
时间: 2022-08-18
链接: https://x.com/karpathy/status/1560056318332846080
互动: Likes: 42; Retweets: 4; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@soumithchintala @chrmanning @roydanroy @tdietterich @ylecun @percyliang ... not me awkwardly standing in the corner of the room watching a mob fight over terminology, kind of liking the term myself and thinking that it's pretty clear what it refers to, but unwilling to get involved...🫢

@soumithchintala @chrmanning @roydanroy @tdietterich @ylecun @percyliang ... 看着一群人为了术语吵翻天，我却尴尬地站在房间角落里，心里其实还挺喜欢这个术语，觉得它指代什么挺清楚的，但又不想掺和进去……🫢

### 371

作者: @karpathy
时间: 2022-08-18
链接: https://x.com/karpathy/status/1560327776095260672
互动: Likes: 266; Retweets: 15; Replies: 3; Quotes: 3; Views: 0; Bookmarks: 5; isReply: 1

quotes from blog:
"I learned that quantization research is like printers. Nobody cares about printers. Nobody likes printers. But everybody is happy if printers do their job." 😂
"Let’s think step-by-step." 😂😂
💀

「我发现量化研究（quantization research）就像打印机：没人关心打印机，也没人喜欢打印机。但如果打印机能好好工作，大家都会很高兴。」😂
「让我们一步一步地思考。」😂😂
笑死。💀

### 372

作者: @karpathy
时间: 2022-08-18
链接: https://x.com/karpathy/status/1560327774623055872
互动: Likes: 574; Retweets: 54; Replies: 12; Quotes: 3; Views: 0; Bookmarks: 107; isReply: 0

Beautiful work (as usual). "Two-part" int8 quantization allows inference of ~2X larger transformers with fixed memory budget, open source code wrapped in a library, paper, more speculative blog post, and opening up very interesting "emergent features" questions in transformers 😍

这项工作（一如既往地）非常出色！「两部分」int8 量化（int8 quantization）技术，使得在固定内存预算下，能够运行约 2 倍大的 Transformer 模型。相关的开源代码已封装成一个库，同时还有一篇论文和一篇更具推测性的博客文章发布。这项研究还引出了 Transformer（Transformer）模型中关于「涌现特性」的许多有趣问题 😍

### 373

作者: @karpathy
时间: 2022-08-18
链接: https://x.com/karpathy/status/1560329572368822272
互动: Likes: 25; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Tim_Dettmers 👍 it's "full package work" :)

@Tim_Dettmers 👍 这真是「全面到位的工作」:)

### 374

作者: @karpathy
时间: 2022-08-19
链接: https://x.com/karpathy/status/1560696444234784768
互动: Likes: 594; Retweets: 51; Replies: 26; Quotes: 4; Views: 0; Bookmarks: 57; isReply: 0

mesmerised with infinite creativity of neural nets 🌈😵‍💫 (and we're just barely scratching the surface) had my A100 GPU dream about "psychedelic faces", while I dreamt about other things. cool music found on the youtube audio library, again by @JVNA ty
https://t.co/hCNCehgTkb

我被神经网络（neural nets）无限的创造力深深吸引了 🌈😵‍💫（我们才刚刚触及皮毛而已）。当我的 A100 GPU 梦到「迷幻面孔」时，我则梦到了别的事情。这超赞的音乐是在 YouTube 音频库中找到的，再次感谢 @JVNA 谢谢！
https://t.co/hCNCehgTkb

### 375

作者: @karpathy
时间: 2022-08-19
链接: https://x.com/karpathy/status/1560700168449564673
互动: Likes: 137; Retweets: 3; Replies: 14; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

it's like... what is even happening as my visual cortex melts

这就像…… 我的视觉皮层快要融化了，眼前到底发生了什么？

### 376

作者: @karpathy
时间: 2022-08-19
链接: https://x.com/karpathy/status/1560760297983381504
互动: Likes: 416; Retweets: 25; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 58; isReply: 0

Despite only August I'd like to nominate this as a top tweet in AI of 2022, summarizing the state of the field right now. I do hesitate because there is all of 4 months for something even funnier to happen.

虽然才八月，我还是想把这条推文评为 2022 年度 AI 领域（Artificial Intelligence）的最佳推文，因为它精准概括了当前该领域的发展状况。不过我确实有些犹豫，毕竟接下来还有整整四个月的时间，说不定会有更令人意想不到的事情发生。

### 377

作者: @karpathy
时间: 2022-08-22
链接: https://x.com/karpathy/status/1561801610572939264
互动: Likes: 1,049; Retweets: 119; Replies: 16; Quotes: 9; Views: 0; Bookmarks: 99; isReply: 0

“This release is the culmination of many hours of collective effort to create a single file that compresses the visual information of humanity into a few gigabytes.”

这次发布凝聚了无数集体心血，最终形成了一个单一文件，它能将人类的视觉信息压缩到区区几个 GB 的大小。

### 378

作者: @karpathy
时间: 2022-08-22
链接: https://x.com/karpathy/status/1561818955966058500
互动: Likes: 1,884; Retweets: 309; Replies: 40; Quotes: 28; Views: 0; Bookmarks: 149; isReply: 0

imo #stablediffusion release today is a day of historic proportion for human creativity, with so much human visual creativity bottled up into one accessible artifact. Big part of a phase shift into an era of human+AI art collab that we’ve just barely scratched the surface of.

在我看来， #stablediffusion 今日的发布，对人类创造力而言是具有划时代意义的一天。它将如此多的人类视觉创造力汇聚并封装到一个易于获取的成果中。这标志着我们正进入一个全新的、人类与 AI 共同进行艺术创作（human+AI art collab）的时代，而我们对此的探索才刚刚开始。

### 379

作者: @karpathy
时间: 2022-08-22
链接: https://x.com/karpathy/status/1561820532617519104
互动: Likes: 367; Retweets: 18; Replies: 11; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

I say this mostly not because of where it is today but because of how much potential and unexplored territory there is intuitively in the underlying modeling, and how it works and interacts with humans.

我之所以这么说，主要不是因为它目前的（发展）水平，而是因为其底层建模本身蕴藏着巨大的直观潜力和许多未被探索的领域，以及它与人类互动和运作的方式。

### 380

作者: @karpathy
时间: 2022-08-23
链接: https://x.com/karpathy/status/1562144064887492608
互动: Likes: 23; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@jon_barron Maybe because the classifier is assumed appended on top of a base model, and separated out as a decoder in a lot of recent work, and almost doesn’t count as part of the base model? But I agree with you the definition was imo clear as simply the number of layers with weights.

@jon_barron 也许是因为分类器（classifier）被认为是附加在基础模型（base model）之上，并且在最近的许多工作中，它被分离出来作为一个解码器（decoder），几乎不被视为基础模型的一部分？但我同意你的看法，在我看来，这个定义很明确，简单来说就是有权重的层数。

### 381

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564665459521101824
互动: Likes: 395; Retweets: 34; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 34; isReply: 0

Recent progress in AI has opened up a lot of opportunities for products and applications. Great to see the AI Grant providing some rocket fuel! 🚀 (and happy to be a small part of as an advisor)

AI（人工智能）的最新进展为各种产品和应用带来了许多机会。很高兴看到 AI Grant 提供了强大的助推力！🚀（也很高兴能以顾问身份贡献一份绵薄之力)

### 382

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564675329888620544
互动: Likes: 45; Retweets: 2; Replies: 6; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@slava__bobrov @DNA_RNA_Uni a gripping portrait of death :|

@slava__bobrov @DNA_RNA_Uni 一幅扣人心弦的死亡画面 :|

### 383

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564698516164730880
互动: Likes: 909; Retweets: 67; Replies: 67; Quotes: 24; Views: 0; Bookmarks: 80; isReply: 0

🤔 vision may be a high-enough throughput input to the brain that is also sufficiently connected to its reward modules that AI-assisted generative art may converge to wire-heading. Probably nothing

🤔 视觉作为一种进入大脑的输入方式，其信息吞吐量可能足够大，并且它与大脑的奖励模块也有着充分的连接。因此，有人猜测，AI 辅助的生成式艺术最终可能会导致「电极快感（wire-heading）」的现象 —— 即系统会过度沉迷于自我刺激以获取奖励，而忽视其他目标。不过，这或许只是杞人忧天的想法。

### 384

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564700963641798656
互动: Likes: 314; Retweets: 8; Replies: 35; Quotes: 2; Views: 0; Bookmarks: 9; isReply: 1

it would feel like tripping on a fully immersive audio/video/(VR?) experience that you can't (don't want to) pull yourself away from

这感觉就像是彻底迷失在一种全身心投入的音频 / 视频 /（VR?）体验中，让你根本无法（也不想）抽身离开。

### 385

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564720730595422208
互动: Likes: 37; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@poolio “nothing beats the reward of a batch of fresh samples.” 😂😂 now how would you like them at 60Hz? In 4k? In a cool pattern? Personalized?

@poolio「没有什么比收到一批新鲜样本更让人有成就感的了。」😂😂 那么，你希望它们以 60Hz 呈现吗？是 4k 吗？是酷炫的模式吗？还是个性化的呢？

### 386

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564721345031680000
互动: Likes: 31; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@AshdinV pupils 😳 ha

@AshdinV 眼睛 😳 哈

### 387

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564737280572411904
互动: Likes: 746; Retweets: 16; Replies: 10; Quotes: 1; Views: 0; Bookmarks: 14; isReply: 1

I don’t think I literally said impossible but I laughed it off, because in 2015 we were still generating black and white digits or faces or little blurry 32x32 cifar-10 texture blobs at best. Like how all of data, algorithms and compute had to advance together.

我没有直接说这不可能，但我当时确实付之一笑。因为在 2015 年，我们最多也只能生成一些黑白数字、人脸图像，或者是一些模糊的 32x32 CIFAR-10 纹理小色块。这正说明了数据、算法和计算能力这些要素都必须同步发展。

### 388

作者: @karpathy
时间: 2022-08-30
链接: https://x.com/karpathy/status/1564737278634627072
互动: Likes: 2,706; Retweets: 162; Replies: 27; Quotes: 12; Views: 0; Bookmarks: 100; isReply: 0

Fei-Fei to me after I showed her my first image captioning (image to text) network around 2015: “very cool, now do it backwards!”. Me: “haha that’s impossible” 🥲. Turns out you just need a few ~B alt-text dataset scrape, transformer, diffusion, and a cluster of ~thousand A100s.

大约在 2015 年，我向 Fei-Fei 展示了我的第一个图像描述（image to text）网络。看过之后，她对我说：「太酷了，现在把它反过来做！」。我当时回复：「哈哈，那是不可能的」🥲。然而，事实证明，你只需要收集数十亿级别（~B）的 alt-text 数据集、一个 Transformer、一个扩散（Diffusion）模型，以及一个由大约一千台 A100 GPU 组成的集群，就能实现这个「不可能」的任务。

### 389

作者: @karpathy
时间: 2022-08-31
链接: https://x.com/karpathy/status/1565061050373853184
互动: Likes: 29; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@NaveenGRao @MosaicML I just mean as rough orders of magnitude, from a PhD student perspective wanting to do that as per advisor ask (including some experimentation overhead). Agree there’s a lot that can be done to make big model training more accessible and that it is very desirable ty for helping

@NaveenGRao @MosaicML 我只是想大致说明一下数量级，这是从一个博士生的角度出发，他希望按照导师的要求来开展这项工作（包括一些实验开销）。我也认同，确实有很多方法可以使大型模型训练（big model training）更易于进行，而这正是我们非常乐见的。感谢你们的帮助。

### 390

作者: @karpathy
时间: 2022-09-01
链接: https://x.com/karpathy/status/1565167475141971968
互动: Likes: 259; Retweets: 12; Replies: 6; Quotes: 0; Views: 0; Bookmarks: 52; isReply: 0

good to see papers start to flesh out the (imo v large) space of extensions to the current primitive text -&gt; image diffusion setup. e.g. imagine laying out many positive/negative prompts over arbitrary regions on larger canvas, guide any regions with various other objectives, ...

很高兴看到有论文开始探索并具体化（在我看来这是一个巨大的）对现有基础的文本到图像扩散模型（Diffusion Model）应用的扩展空间。例如，我们可以设想在更大的画布上，在任意区域放置许多正面或负面提示，并用各种其他目标来引导这些区域生成内容，...

### 391

作者: @karpathy
时间: 2022-09-01
链接: https://x.com/karpathy/status/1565174644935577600
互动: Likes: 1; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@deliprao in the paper of that tweet

@deliprao 在该推文所引用的论文中

### 392

作者: @karpathy
时间: 2022-09-02
链接: https://x.com/karpathy/status/1565578366886940672
互动: Likes: 1,986; Retweets: 164; Replies: 66; Quotes: 32; Views: 0; Bookmarks: 82; isReply: 0

LOTR Rings of Power is out. But I spent most of the first episode sad and internally mourning and reminiscing the miracle of the original trilogy. I basically can’t watch it hurts too much. Lol @ review I encountered: https://t.co/ZfEewBprvi

《魔戒：力量之戒》开播了。但在看第一集的大部分时间里，我都很伤心，内心一直在缅怀和回忆着原版三部曲的那些奇迹。我基本上看不下去，感觉太难受了。哈哈，刷到一篇评论：https://t.co/ZfEewBprvi

### 393

作者: @karpathy
时间: 2022-09-02
链接: https://x.com/karpathy/status/1565578521149222912
互动: Likes: 448; Retweets: 11; Replies: 8; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

me rn https://t.co/TpYN37kD1j

我此刻的心情正是如此 https://t.co/TpYN37kD1j

### 394

作者: @karpathy
时间: 2022-09-02
链接: https://x.com/karpathy/status/1565581983249358848
互动: Likes: 103; Retweets: 1; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@TimDehoucke I love this idea. Maybe an AI can one day beat the original trilogy 😳

@TimDehoucke 我很喜欢这个想法。也许有一天，某个 AI（Artificial Intelligence）能够超越原始三部曲呢 😳

### 395

作者: @karpathy
时间: 2022-09-02
链接: https://x.com/karpathy/status/1565590660341645312
互动: Likes: 28; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@clavid_k ikr 😂 I kept thinking #unrealengine, trending on artstation

@clavid_k 可不是嘛 😂 我一直觉得像 #unrealengine，是 Artstation 上的热门话题。

### 396

作者: @karpathy
时间: 2022-09-02
链接: https://x.com/karpathy/status/1565754356552544256
互动: Likes: 4; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@zippy731 @deforum_art :O hypnotic

@zippy731 @deforum_art 哇塞，太迷人了！

### 397

作者: @karpathy
时间: 2022-09-03
链接: https://x.com/karpathy/status/1566100736076697600
互动: Likes: 121; Retweets: 2; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 8; isReply: 1

@hardmaru @micheli_vincent @francoisfleuret so fun to see a little hacked up minGPT in the repo, hacked directly in code instead of configuring some unreadable monster with 100 kwargs

@hardmaru @micheli_vincent @francoisfleuret 看到仓库里那个经过小巧改造的 minGPT 真是太棒了，它是直接通过代码实现的，而不是通过配置一个包含上百个关键字参数（kwargs）的、难以阅读的复杂系统。

### 398

作者: @karpathy
时间: 2022-09-04
链接: https://x.com/karpathy/status/1566557586094034945
互动: Likes: 13; Retweets: 1; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@CGDaveMac There is. Some are trying to subtly watermark the generated images, but it is spotty. May be possible to train classifieds that identify generated images for a while.

@CGDaveMac 是有的。一些人正尝试对生成的图像进行隐秘的数字水印（watermark）处理，但这目前很不稳定。不过，或许可以在一段时间内训练出能够识别这些生成图像（generated images）的分类器（classifiers）。

### 399

作者: @karpathy
时间: 2022-09-06
链接: https://x.com/karpathy/status/1567225780241055745
互动: Likes: 83; Retweets: 2; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@gunsnrosesgirl3 @fredodurand I am shook

@gunsnrosesgirl3 @fredodurand 我惊呆了

### 400

作者: @karpathy
时间: 2022-09-06
链接: https://x.com/karpathy/status/1567233122252750848
互动: Likes: 639; Retweets: 80; Replies: 42; Quotes: 8; Views: 0; Bookmarks: 186; isReply: 0

"AI And The Limits Of Language" https://t.co/ORHuyfnTQ6 good article on a big open question in my mind - how much can an AI learn from internet text alone? what if added a lot of images/videos from the internet? do we have to reach all the way to embodied agents?

「AI 与语言的局限」https://t.co/ORHuyfnTQ6 这是一篇好文章，探讨了我心中一个重要的开放性问题 —— 一个 AI（人工智能）仅凭互联网文本能学到多少？如果再加入大量的互联网图片和视频呢？我们是否必须非得发展到具身智能体（embodied agents）才能达到目标？

### 401

作者: @karpathy
时间: 2022-09-07
链接: https://x.com/karpathy/status/1567592846773092352
互动: Likes: 325; Retweets: 11; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 9; isReply: 1

in this lecture we:
1. estimate a bigram language model with counting
2. sample from the model
3. vectorize our implementation using torch tensors
4. implement the negative log likelihood loss
5. convert all of it into the neural net framework
6. optimize it with gradient descent

在本次讲座中，我们将学习：
1. 如何通过计数来构建一个二元语法（bigram）语言模型
2. 如何从这个模型中生成新的序列或文本（即采样)
3. 如何使用 PyTorch 的张量（torch tensors）来加速我们的程序运行，使其向量化
4. 如何实现负对数似然损失（negative log likelihood loss）函数
5. 如何将所有这些步骤融入到神经网络的框架中
6. 如何利用梯度下降（gradient descent）算法来优化这个模型

### 402

作者: @karpathy
时间: 2022-09-07
链接: https://x.com/karpathy/status/1567592845221179392
互动: Likes: 2,284; Retweets: 314; Replies: 28; Quotes: 33; Views: 0; Bookmarks: 698; isReply: 0

🎓New (1h57m) video lecture: "The spelled-out intro to language modeling: building makemore". 
&gt; We build a neural net bigram language model (working up to transformers). Micrograd was fun, now things complexify: tensors, broadcasting, training, sampling.. https://t.co/7CkV0NNtTw

🎓 最新视频讲座（时长 1 小时 57 分钟)："大白话讲语言建模：一步步构建 makemore」。
> 我们将从构建一个神经网络二元语言模型（neural net bigram language model）开始，逐步深入到 Transformer 模型。之前玩 Micrograd 很有趣，现在我们将面对更复杂的概念：张量（tensors）、广播（broadcasting）、模型训练（training）和采样（sampling）等等... https://t.co/7CkV0NNtTw

### 403

作者: @karpathy
时间: 2022-09-07
链接: https://x.com/karpathy/status/1567592848165601281
互动: Likes: 531; Retweets: 14; Replies: 24; Quotes: 0; Views: 0; Bookmarks: 16; isReply: 1

Future lectures will gradually complexify the neural net to take more than one input character, and will take the form of: 1. multilayer perceptron (~2003 style), 2. RNNs (~2011 style), 3. modern transformer (~2017+ style). From there into vision, then vision+nlp. Should be fun!

未来的讲座将逐步让神经网络变得更复杂，使其能够处理不止一个输入字符，内容形式会包含：1. 多层感知器（Multilayer Perceptron）(~2003 年左右的风格），2. 循环神经网络（RNNs）(~2011 年左右的风格），3. 现代 Transformer（~2017 年以后的风格）。之后还会深入探讨计算机视觉领域，再接着是视觉与自然语言处理（NLP）的结合。相信这会非常有趣！

### 404

作者: @karpathy
时间: 2022-09-07
链接: https://x.com/karpathy/status/1567622197812002817
互动: Likes: 75; Retweets: 0; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@KaliTessera I recorded and edited this one over 3 days, maybe total of ~12 hours. But that included going down a bad path for part 2, so I had to erase 1 hour of content and redo it. There's quite a bit of iteration as I'm searching for a best way to incrementally complexify a concept.

@KaliTessera 我花了 3 天时间录制和编辑这个内容，总共大约 12 小时。不过，其中也包括在第二部分走了一段弯路，所以我不得不删除约 1 小时的内容并重新制作。这中间经历了相当多的迭代（iteration），因为我一直在寻找一种最佳方式，能够循序渐进地阐释一个复杂概念。

### 405

作者: @karpathy
时间: 2022-09-07
链接: https://x.com/karpathy/status/1567623143732449280
互动: Likes: 14; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@sanchom LSTM a little bit annoying because it has both a cell and hidden state to keep track of at each time step, but I'll def include a GRU. Ok maybe I'll end up doing LSTM too.

@sanchom 门控循环单元（LSTM）有点复杂，因为它在每个时间步（time step）都需要同时管理细胞状态（cell state）和隐藏状态（hidden state），但我肯定会包含门控循环单元（GRU）。好吧，也许我最终也会实现 LSTM。

### 406

作者: @karpathy
时间: 2022-09-08
链接: https://x.com/karpathy/status/1567703919081721858
互动: Likes: 61; Retweets: 0; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 0

@Mvandepanne Thank you Michiel! I thought for a long time about what approach best transfers my knowledge to someone else's brain and settled on this format, instead of e.g. books/articles, code releases, or live lectures. Still tuning though. And I think I'm missing exercises, imo necessary.

@Mvandepanne 谢谢你 Michiel！我曾长时间思考，哪种方法能最有效地将我的知识传授给他人，最终选择了这种形式，而非像书籍 / 文章、代码发布或现场讲座那样。不过，这项工作仍在不断摸索和调整中。而且，我个人认为练习环节必不可少，但目前仍有欠缺。

### 407

作者: @karpathy
时间: 2022-09-08
链接: https://x.com/karpathy/status/1567716371504693248
互动: Likes: 6; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@Weather_West @BigTechAlert @Tesla Yeah lol :( really liked your tweets btw just a bit too many of them 😅

@Weather_West @BigTechAlert @Tesla 对啊，哈哈哈，无奈了 :( 说起来，你的推文我真的很喜欢，就是数量有点太多了 😅

### 408

作者: @karpathy
时间: 2022-09-08
链接: https://x.com/karpathy/status/1567888746082861062
互动: Likes: 3; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 0

@MuruganYuvaraaj good point thank you will try

@MuruganYuvaraaj 说得对，谢谢，我会试试看。

### 409

作者: @karpathy
时间: 2022-09-10
链接: https://x.com/karpathy/status/1568644275247923206
互动: Likes: 1,797; Retweets: 236; Replies: 28; Quotes: 33; Views: 0; Bookmarks: 513; isReply: 0

Stable Diffusion concepts library https://t.co/X2jHPdWp4E textual inversion is amazing - can train a custom word vector (not otherwise reachable by english text) to mean a concept, based on examples. Opens up many possibilities of condensing objects/styles into special tokens 🚀

Stable Diffusion 概念库（https://t.co/X2jHPdWp4E）中使用的「文本反转（textual inversion）」技术真是令人惊叹！它能根据提供的示例，训练出一个自定义的词向量（word vector）。这个词向量所代表的概念，是普通英文文本无法直接表达的。这项技术为我们将特定物体或风格「浓缩」成特殊的 Token 开启了无限可能 🚀

### 410

作者: @karpathy
时间: 2022-09-10
链接: https://x.com/karpathy/status/1568645140818071553
互动: Likes: 232; Retweets: 8; Replies: 7; Quotes: 4; Views: 0; Bookmarks: 10; isReply: 1

prompts may start to take on a mixed english mixed special inverted token forms, like "a photo of &lt;karpathy/cool-object-v7&gt; in the style of &lt;coolperson/trippystyle&gt;".

提示（prompt）可能会开始呈现一种英语与特殊倒置 Token 混合的复杂形式，例如「一张 &lt;karpathy/cool-object-v7&gt; 的照片，风格为 &lt;coolperson/trippystyle&gt;」。

### 411

作者: @karpathy
时间: 2022-09-10
链接: https://x.com/karpathy/status/1568645664548200451
互动: Likes: 161; Retweets: 2; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

beautiful addition to the quickly growing toolkit of steering diffusion models

迅速壮大的扩散模型操控工具集中又一个出色的新增功能

### 412

作者: @karpathy
时间: 2022-09-10
链接: https://x.com/karpathy/status/1568648558534098945
互动: Likes: 30; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@ShumingHu no you're strictly adding a new concept everything else is kept frozen.

@ShumingHu 不，你仅仅是添加了一个新的概念，而其他所有部分都保持冻结（即不作改动）。

### 413

作者: @karpathy
时间: 2022-09-10
链接: https://x.com/karpathy/status/1568650161089576963
互动: Likes: 148; Retweets: 7; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 18; isReply: 1

(adding link to the paper in thread: https://t.co/JStpB55XG3)

(在帖子中附上论文链接：https://t.co/JStpB55XG3)

### 414

作者: @karpathy
时间: 2022-09-10
链接: https://x.com/karpathy/status/1568660455664787456
互动: Likes: 861; Retweets: 43; Replies: 31; Quotes: 4; Views: 0; Bookmarks: 27; isReply: 0

Sometimes research feels like exploring the nooks and crannies of local forests and valleys and sometimes it feels like landing in America.

有时，科研感觉像是在探索本地森林和山谷的每一个角落，而有时，它又像突然降落到美国一样，开启了全新的旅程。

### 415

作者: @karpathy
时间: 2022-09-10
链接: https://x.com/karpathy/status/1568667991927328769
互动: Likes: 53; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Plinz it's pretty interesting to me that this is a number of people's reaction when the meaning is rather obvious

@Plinz 我觉得很有意思的是，即便意思相当明显，许多人却有这样的反应。

### 416

作者: @karpathy
时间: 2022-09-11
链接: https://x.com/karpathy/status/1568772812873289728
互动: Likes: 29; Retweets: 1; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@kamikaz1_k yes it's just that stable diffusion is a relatively complex model so it takes a lot of time to build up to it if you want to do it properly and in full detail. more "surface explanations" are plentiful on the internet already though depending on what level of abstraction you like

@kamikaz1_k 是的，主要原因是 stable diffusion 是一个相对复杂的模型，因此如果你想透彻且详细地理解它，确实需要投入大量时间去逐步学习。不过，互联网上已经有很多「浅层讲解」或基础介绍，这取决于你希望了解的深入程度。

### 417

作者: @karpathy
时间: 2022-09-11
链接: https://x.com/karpathy/status/1568987080885432327
互动: Likes: 37; Retweets: 3; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 7; isReply: 1

@djgish yes see soft prompts https://t.co/LPzIDAkepM

@djgish 是的，可以参考软提示（soft prompts）https://t.co/LPzIDAkepM

### 418

作者: @karpathy
时间: 2022-09-11
链接: https://x.com/karpathy/status/1569062470962257920
互动: Likes: 107; Retweets: 1; Replies: 6; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@natolambert ty! next video implements an MLP to get logits for the next character (where neural net fun actually starts), pending last minor edits then probably uploading tonight or tomorrow

@natolambert 感谢！在下一个视频中，我们将实现一个多层感知机（MLP）来获取下一个字符的 logits（即神经网络真正发挥作用的精彩之处）。目前视频还在进行最后的细微修改，预计今晚或明天上传。

### 419

作者: @karpathy
时间: 2022-09-12
链接: https://x.com/karpathy/status/1569336377766199302
互动: Likes: 287; Retweets: 10; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 25; isReply: 1

in this lecture:
1. we implement the model from the paper in PyTorch
2. intro internals of torch.Tensor (views, storage)
3. training loop, overfitting one batch
4. finding good initial learning rate
5. train/val/test splits
6. underfitting, overfitting
7. experimentation process

本次讲座我们将探讨：
1. 在 PyTorch 中复现论文里提出的模型
2. 深入理解 torch.Tensor 的内部机制，包括其视图（views）和存储（storage）方式
3. 构建训练循环，并演示如何在一个批次（batch）数据上实现过拟合（overfitting)
4. 寻找合适的初始学习率（learning rate)
5. 如何进行训练集、验证集和测试集的数据划分（train/val/test splits)
6. 解释欠拟合（underfitting）和过拟合（overfitting）现象
7. 机器学习实验的整个流程

### 420

作者: @karpathy
时间: 2022-09-12
链接: https://x.com/karpathy/status/1569336376260460544
互动: Likes: 1,300; Retweets: 150; Replies: 27; Quotes: 9; Views: 0; Bookmarks: 263; isReply: 0

📈 New (1h15m) video lecture (#3): The spelled-out intro to language modeling: building makemore. Part 2: MLP https://t.co/tBnlGWOVAs
&gt; We continue our implementation of makemore: the multi layer perceptron (MLP) language model of Bengio et al. 2003

📈 全新（1 小时 15 分钟）视频讲座（#3)：语言建模入门详解：构建 makemore（第二部分：多层感知器 MLP） https://t.co/tBnlGWOVAs
> 我们将继续实现 makemore 项目，深入探讨 Bengio 等人于 2003 年提出的多层感知器（MLP）语言模型。

### 421

作者: @karpathy
时间: 2022-09-12
链接: https://x.com/karpathy/status/1569337189640867842
互动: Likes: 214; Retweets: 16; Replies: 5; Quotes: 1; Views: 0; Bookmarks: 17; isReply: 1

The paper (pdf): https://t.co/br8txsl9j2
google collab of the notebook we built: https://t.co/fFcMdB4gBz https://t.co/PUxiAgwHb4

论文（pdf)：https://t.co/br8txsl9j2
我们构建的 notebook 的 Google Colab 链接：https://t.co/fFcMdB4gBz https://t.co/PUxiAgwHb4

### 422

作者: @karpathy
时间: 2022-09-14
链接: https://x.com/karpathy/status/1570195369975513095
互动: Likes: 1,183; Retweets: 118; Replies: 25; Quotes: 11; Views: 0; Bookmarks: 158; isReply: 0

Very interesting! A bit like Autopilot but for your computer.

这非常有意思！它有点像是你电脑上的「自动驾驶」系统（Autopilot）。

### 423

作者: @karpathy
时间: 2022-09-16
链接: https://x.com/karpathy/status/1570847229539385344
互动: Likes: 8; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@_arohan_ @giffmana @achowdhery @arankomatsuzaki ah, okay

@_arohan_ @giffmana @achowdhery @arankomatsuzaki 啊，好的

### 424

作者: @karpathy
时间: 2022-09-19
链接: https://x.com/karpathy/status/1571884730643259394
互动: Likes: 562; Retweets: 8; Replies: 19; Quotes: 0; Views: 0; Bookmarks: 9; isReply: 1

@Suhail Haha ty, but I am pretty chill about it, I don't think people usually mean it in a derogatory way and I mostly laugh about it. I've also been called a 'blogger' :D. There's a bit of general lack of language norms around how to talk about professional but freelancer posistions.

@Suhail 哈哈谢谢，不过我对此看得很开，我不觉得大家通常会带着贬义去说，我大多时候都一笑了之。我还被人叫过「博主」呢 :D。对于如何描述既专业又以自由职业形式存在的工作，目前确实缺乏一套普遍的语言规范。

### 425

作者: @karpathy
时间: 2022-09-19
链接: https://x.com/karpathy/status/1571884852101918721
互动: Likes: 166; Retweets: 4; Replies: 5; Quotes: 2; Views: 0; Bookmarks: 5; isReply: 1

@Suhail E.g. you're a ‘blogger’, but if some media/publisher company pays you then suddenly you are an ‘author’ or a ‘journalist’. And you’re an ‘influencer’ but if some university pays you then suddenly you are a ‘teacher’ or a ‘professor’.

@Suhail 举个例子，你可能是一名「博主」，但如果某家媒体 / 出版公司付钱给你，你立马就成了「作者」或「记者」。同样地，你可能是一名「影响者」，但如果某所大学付钱给你，你立马就成了「老师」或「教授」。

### 426

作者: @karpathy
时间: 2022-09-19
链接: https://x.com/karpathy/status/1571884968078610434
互动: Likes: 169; Retweets: 5; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@Suhail Imo as society is trending towards a lot more freelance professionals I imagine language around it will adjust. For now, at least for me, mostly funny.

@Suhail Imo 随着社会越来越倾向于出现更多自由职业者（freelance professionals），我想与之相关的语言表达也会随之调整。至少目前对我而言，这大部分时候都挺好笑的。

### 427

作者: @karpathy
时间: 2022-09-19
链接: https://x.com/karpathy/status/1571905197907275776
互动: Likes: 98; Retweets: 0; Replies: 8; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@EMostaque @Suhail 'thought leader' 🤮😂

@EMostaque @Suhail「思想领袖」🤮😂

### 428

作者: @karpathy
时间: 2022-09-19
链接: https://x.com/karpathy/status/1571907378744012801
互动: Likes: 58; Retweets: 0; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@EMostaque @Suhail Haha I've heard you described as "chaotic good"
(my favorite square of the D&amp;D alignment system)

@EMostaque @Suhail 哈哈听说有人用「混乱善良」来形容你
(这是我最喜欢的 D&D 阵营类型之一)

### 429

作者: @karpathy
时间: 2022-09-19
链接: https://x.com/karpathy/status/1571908688587395074
互动: Likes: 10; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@EMostaque @Suhail (google image search ref: https://t.co/ti7O2lYymF )

@EMostaque @Suhail（谷歌图片搜索参考：https://t.co/ti7O2lYymF)

### 430

作者: @karpathy
时间: 2022-09-20
链接: https://x.com/karpathy/status/1572275299211161607
互动: Likes: 26; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@_arohan_ @borisdayma @jekbradbury :D umm you forgot to list Phil's dog Ice Cream, potentially the most famous deep learning dog afaict  https://t.co/zKntvNzv4G

@_arohan_ @borisdayma @jekbradbury :D 哎呀，你们是不是忘了把 Phil 的狗狗 Ice Cream 也列进去？据我所知，它可能才是深度学习圈里最有名气的狗狗呢 https://t.co/zKntvNzv4G

### 431

作者: @karpathy
时间: 2022-09-21
链接: https://x.com/karpathy/status/1572652147657052162
互动: Likes: 1,099; Retweets: 76; Replies: 14; Quotes: 5; Views: 0; Bookmarks: 47; isReply: 0

👏 OpenAI at its best :)

OpenAI 真是太棒了，处于巅峰状态！:)

### 432

作者: @karpathy
时间: 2022-09-21
链接: https://x.com/karpathy/status/1572724122324332544
互动: Likes: 20; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@mat_kelcey @ayhanfuat @venomsnake006 :| I was 🤯 definitely not what you'd expect imo

@mat_kelcey @ayhanfuat @venomsnake006 :| 我真是大开眼界🤯，这绝对不是你想象的那样（在我看来）。

### 433

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1572795152477093888
互动: Likes: 558; Retweets: 40; Replies: 26; Quotes: 7; Views: 0; Bookmarks: 62; isReply: 0

Saw this 4 hours ago but can't stop thinking about it. "The generator initialized in the first call is used for the second one (so it continues to generate from where it left off)". Interesting API design choice case study. In PyTorch you pass a Generator, more assumed stateful.

我大约在四小时前看到了这个设计，但它一直在我脑海中盘旋。「在第一次调用中初始化（实例化）的生成器（Generator）被用于第二次调用，因此它能从上次中断的地方继续生成。」这是一个非常有意思的应用程序接口（API）设计案例研究。在像 PyTorch 这样的框架中，你通常会传递一个生成器，它被更多地视为有状态（stateful）的对象。

### 434

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019730851397632
互动: Likes: 2,314; Retweets: 411; Replies: 35; Quotes: 27; Views: 0; Bookmarks: 807; isReply: 0

Reading through OpenAI Whisper paper https://t.co/3PmWvQNCFs some notes: https://t.co/QVeqaGVvsV

在阅读 OpenAI Whisper 论文 https://t.co/3PmWvQNCFs 之后，我整理了一些笔记： https://t.co/QVeqaGVvsV

### 435

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019734911569920
互动: Likes: 147; Retweets: 9; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 6; isReply: 1

Idea 2: Scrape a large (680,000hr) audio+transcript dataset, spend much attention+care on heuristics for rejecting/cleaning algorithmically. Some of it is wrong but there is a ton of it. Simple supervised learning from there on, skip auxiliary objectives, self-supervision, etc.

思路二：收集一个大型的（680,000 小时）音频 + 文本数据集，并投入大量精力在设计启发式方法（heuristics）上，通过算法对其进行筛选和清理。尽管这些数据中可能存在一些错误，但其总量非常庞大。在此基础上，直接进行简单的监督学习（supervised learning），而无需使用辅助目标（auxiliary objectives）、自监督（self-supervision）等复杂方法。

### 436

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019733707788288
互动: Likes: 243; Retweets: 10; Replies: 4; Quotes: 1; Views: 0; Bookmarks: 23; isReply: 1

Idea 1: keep the neural net and the optimization super simple: vanilla Transformer (2017 style) LLM. The innovation is around 1) what the dataset and the training objective is and 2) the I/O schema that allows a single model to multi-task as a speech recognition swiss-army knife.

想法 1：保持神经网络（neural net）和优化（optimization）方案的极致简洁：采用标准的 Transformer（2017 年风格）大语言模型（LLM）。其创新之处主要体现在：1）数据集和训练目标（training objective）的设计；2）独特的输入 / 输出（I/O）模式，使得单个模型能够像语音识别（speech recognition）的「瑞士军刀」一样，胜任多种任务（multi-task）。

### 437

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019738082463744
互动: Likes: 155; Retweets: 6; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

Idea 3: Use special tokens at the input to condition the model for all desired tasks in a single model (language id, speech detection, transcription, translation). Create a "meta-language" of special tokens of a fixed schema that orchestrates the tasks/stages. https://t.co/H5a2VUgTSe

Idea 3：在模型输入端使用特殊 Token（Token）来引导模型，使其在一个单一模型中完成所有预设任务，包括：语言识别（language id）、语音检测（speech detection）、转录（transcription）和翻译（translation）。具体做法是创建一个由固定模式的特殊 Token 构成的「元语言」，专门用于协调和组织这些任务及阶段。https://t.co/H5a2VUgTSe

### 438

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019741999939584
互动: Likes: 148; Retweets: 6; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 2; isReply: 1

Idea 4: Adopt the GPT train/eval mindset: train on large internet-scraped datasets, then evaluate zero-shot performance on standard evaluation benchmarks (ignoring their training sets entirely!). This approach decreases dataset-specific overfitting and creates more robust models. https://t.co/JbY5nnpV0b

第四条建议：采纳 GPT 的训练 / 评估理念：在大规模互联网抓取的数据集上进行训练，然后在标准评估基准上，对模型的零样本（zero-shot）性能进行评估（完全忽略这些基准自身的训练集！）。这种方法能减少模型对特定数据集的过拟合（overfitting），并构建出更鲁棒（robust）的模型。https://t.co/JbY5nnpV0b

### 439

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019745158254592
互动: Likes: 150; Retweets: 7; Replies: 2; Quotes: 1; Views: 0; Bookmarks: 6; isReply: 1

Striking story/paragraph from the paper on why this is the correct regime of training:evaluation to focus on. TLDR it is possible to overfit to datasets and their statistics without producing actually robust and generalizable models. https://t.co/XVQm9xYrta

这篇论文中有一个引人注目的观点 / 段落，解释了为什么我们应该关注这种特定的训练与评估模式。简而言之，模型有可能在过度拟合（overfit）于数据集及其统计特性之后，仍然无法生成真正鲁棒（robust）且泛化能力强（generalizable）的模型。https://t.co/XVQm9xYrta

### 440

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019751252578304
互动: Likes: 95; Retweets: 4; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 5; isReply: 1

Few more notes:
- multi-task transfer is (-) for small models but (+) for large models! (much optimism for more scaling)
- long-form transcription using hacky decoding heuristics :\
- eval is hard: WER has well-documented problems, requires hacky/extensive text normalization.

还有一些需要注意的地方：
- 多任务迁移（multi-task transfer）对小型模型表现出负面影响（-），但对大型模型则能带来积极效果（+)！（这让我们对模型进一步扩展（scaling）抱有极大的乐观）
- 长篇转录（long-form transcription）往往需要依赖一些非正规的解码启发式算法（decoding heuristics）来完成。
- 评估工作（eval）难度较大：词错误率（WER）存在一些众所周知的问题，并且通常需要进行临时性的或大量的文本规范化处理。

### 441

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019749268672512
互动: Likes: 88; Retweets: 4; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

Scaling laws indicate room for additional performance improvements from scaling both 1) the model size and 2) the dataset size, though with some hints of diminishing returns in the case of English specifically, which is most abundant in the training set. https://t.co/mI2dWP8QyW

根据扩展定律（Scaling laws）显示，我们可以通过增大 1）模型规模和 2）数据集规模来进一步提升性能。不过，对于训练集中最丰富的英语数据来说，这种提升的效果已经开始显现出一些回报递减的迹象。https://t.co/mI2dWP8QyW

### 442

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019755987881984
互动: Likes: 414; Retweets: 25; Replies: 17; Quotes: 0; Views: 0; Bookmarks: 51; isReply: 1

TLDR: You can get far with: vanilla Transformer (2017). Scrape a massive (though weakly-labeled) dataset, use simple supervised learning. Multi-task. Eval in zero-shot regime. More perf expected from further model+data scaling. Eval is hard. Some parts (decoding) feel hacky.

要点速览：仅凭标准版 Transformer（2017）模型，你就能取得显著进展。具体来说，可以收集一个大规模的（尽管是弱标记的）数据集，并使用简单的监督学习（supervised learning）方法进行训练。此外，采用多任务（multi-task）学习，并在零样本（Zero-shot）模式下进行评估。预计通过模型和数据规模的进一步扩展，性能还会得到更大提升。不过，评估工作本身是具有挑战性的，而且模型中的一些部分（例如解码，decoding）目前显得有些简陋。

### 443

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573019754016567296
互动: Likes: 384; Retweets: 20; Replies: 5; Quotes: 1; Views: 0; Bookmarks: 12; isReply: 1

Favorite paragraph of the paper: citing the software packages used throughout the project. Personally excited and hopeful to see this become a lot more common. https://t.co/LGLVJxB4iq

论文里我最喜欢的一段：它列出了整个项目中使用到的软件包。我个人非常期待并希望这种做法能变得更加普遍。https://t.co/LGLVJxB4iq

### 444

作者: @karpathy
时间: 2022-09-22
链接: https://x.com/karpathy/status/1573035197867515904
互动: Likes: 25; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@eliwaxmann actually me too, I'd suspect it could help to init (or jointly train) parts of the model with self-supervised objectives.

@eliwaxmann 实际上我也是这么想的。我怀疑这可能有助于通过自监督目标（self-supervised objectives）来初始化或联合训练模型的一部分。

### 445

作者: @karpathy
时间: 2022-09-23
链接: https://x.com/karpathy/status/1573104091651534851
互动: Likes: 685; Retweets: 37; Replies: 36; Quotes: 6; Views: 0; Bookmarks: 24; isReply: 0

Woohoo!! #stablediffusion to assist: me soon. "Andrej Karpathy dressed in kimono sipping matcha in a tea house in Japan with Mount Fuji in the background, sunset professional portrait, Nikon 85mm f/1.4G" nice 😂 https://t.co/yLVbdZu6Up

喔吼！！#stablediffusion 很快就能帮到我了。「Andrej Karpathy 穿着和服，在日本一间茶馆里品尝抹茶，背景是富士山，日落时的专业肖像，尼康 85mm f/1.4G」太棒了 😂 https://t.co/yLVbdZu6Up

### 446

作者: @karpathy
时间: 2022-09-23
链接: https://x.com/karpathy/status/1573110962227675138
互动: Likes: 1,360; Retweets: 57; Replies: 32; Quotes: 4; Views: 0; Bookmarks: 30; isReply: 0

I remember when I got an early invite to try DALL-E 2 and I was frozen at the prompt text box for a minute and finally typed in "cat"😅. The art of prompts that the community has discovered and increasingly perfected over the last few months for text-&gt;image models is astonishing.

我记得当我收到 DALL-E 2 的早期邀请时，在它的提示文本框前我足足呆住了一分钟，最后才输入了「猫咪」。然而，在过去几个月里，社区为文本到图像模型（text-to-image models）所发现并日益完善的「提示词艺术」（即如何编写有效的提示词 Prompt），着实令人惊叹。

### 447

作者: @karpathy
时间: 2022-09-23
链接: https://x.com/karpathy/status/1573112341105090560
互动: Likes: 40; Retweets: 0; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@MichaelTrazzi umm this prompt looks like is from April

@MichaelTrazzi 嗯，这个提示看起来是四月份的。

### 448

作者: @karpathy
时间: 2022-09-23
链接: https://x.com/karpathy/status/1573113104791109632
互动: Likes: 25; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 4; isReply: 1

@jeffdeskins issue deprecated by https://t.co/utUU4oxdMX

@jeffdeskins 提出的问题，已被 https://t.co/utUU4oxdMX 取代（或解决 / 废弃）。

### 449

作者: @karpathy
时间: 2022-09-23
链接: https://x.com/karpathy/status/1573123790795837440
互动: Likes: 4,920; Retweets: 395; Replies: 90; Quotes: 87; Views: 0; Bookmarks: 682; isReply: 0

Playing with Whisper. Fed in a 1m25s audio snippet from one of my lectures. I speak fast. I correct myself and backtrack a bit. I use technical terms (MLP, RNN, GRU). ~10 seconds later the (292 word) transcription is perfect except "Benjio et al. 2003" should be Bengio. Impressed https://t.co/HDvaxZO37v

我试用了 Whisper。我将一段来自我讲座的 1 分 25 秒的音频片段输入其中。我的语速很快，讲话时会有自我纠正和一些回溯的情况。我在讲座中使用了技术术语，例如多层感知机（MLP）、循环神经网络（RNN）和门控循环单元（GRU）。大约 10 秒后，这段包含 292 个词的转录文本几乎完美，唯一的错误是「Benjio et al. 2003」应该拼写为 Bengio。这让我印象非常深刻。https://t.co/HDvaxZO37v

### 450

作者: @karpathy
时间: 2022-09-23
链接: https://x.com/karpathy/status/1573133383147855873
互动: Likes: 306; Retweets: 10; Replies: 6; Quotes: 2; Views: 0; Bookmarks: 50; isReply: 1

( sorry context https://t.co/bY6VXrYrA0 )

大语言模型（LLMs）和生成式 AI（Generative AI）（例如 DALL-E 3、Midjourney、Stable Diffusion、Sora）发展迅速，它们在广泛任务中展现出了令人惊叹的能力，从自然语言处理到图像和视频生成无所不能 [1–5]。

### 451

作者: @karpathy
时间: 2022-09-23
链接: https://x.com/karpathy/status/1573133759461068800
互动: Likes: 303; Retweets: 2; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@Gok that would be difficult seeing as this lecture has not yet been published and exists only as a draft on my macbook :)

@Gok 那可就难了，因为这次讲座还没发布，目前只存在于我 macbook 上的草稿里呢 :)

### 452

作者: @karpathy
时间: 2022-09-24
链接: https://x.com/karpathy/status/1573731050177503233
互动: Likes: 5; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@SMcfarnell @lexfridman basically a kind of animal agriculture but on cellular level :)

@SMcfarnell @lexfridman 基本上就是一种动物养殖（animal agriculture），只不过是在细胞层面实现的 :)

### 453

作者: @karpathy
时间: 2022-09-25
链接: https://x.com/karpathy/status/1574170580181393408
互动: Likes: 52; Retweets: 1; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@ilyasut warning: too catchy 😓

@ilyasut 警告：太有感染力了 😓

### 454

作者: @karpathy
时间: 2022-09-25
链接: https://x.com/karpathy/status/1574185388687515648
互动: Likes: 36; Retweets: 3; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@ilyasut 🤦‍♂️ there's a whole channel of these...  https://t.co/BzY8EK3CmT

@ilyasut 🤦‍♂️ 天呐，居然有专门一个频道都是这种内容…… https://t.co/BzY8EK3CmT

### 455

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574474952446615552
互动: Likes: 1,930; Retweets: 79; Replies: 85; Quotes: 14; Views: 0; Bookmarks: 78; isReply: 1

As someone who very much enjoys podcasts I continue to be frustrated that so much information is locked up in opaque audio files. How do we make all of this information accessible, searchable, navigable, linkable, upvotable, etc? Great opportunity if someone does this right, imo.

我作为一个非常喜欢播客的人，一直以来都感到很沮丧，因为有如此多的宝贵信息被「锁定」在难以直接利用的音频文件（opaque audio files）中。我们应该如何才能让所有这些信息变得容易获取、方便搜索、易于导航、可以相互链接、甚至能够被点赞（upvotable）等等呢？在我看来，如果有人能把这件事做好，那将是一个巨大的机会。

### 456

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574474950416617472
互动: Likes: 5,325; Retweets: 583; Replies: 213; Quotes: 94; Views: 0; Bookmarks: 1,102; isReply: 0

Ok so I downloaded all ~322 episodes of @lexfridman podcast and used OpenAI Whisper to transcribe them. I'm hosting the transcriptions on... "Lexicap" ;) : https://t.co/bjYTsE6OgK. Raw vtt transcripts are included for anyone else who'd like to play (they are quite great!) https://t.co/htMalighel

我下载了 @lexfridman 播客的全部约 322 集，并利用 OpenAI Whisper 对它们进行了转录。目前，这些转录文本都托管在一个名为「Lexicap」的网站上 😉 ：https://t.co/bjYTsE6OgK。此外，原始的 VTT 转录文件也已包含在内，欢迎任何感兴趣的朋友下载使用（它们的质量相当不错！） https://t.co/htMalighel

### 457

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574476200801538048
互动: Likes: 846; Retweets: 14; Replies: 45; Quotes: 8; Views: 0; Bookmarks: 57; isReply: 1

Fun AI project for someone: collect a few example segments of Lex speaking and train a classifier on top of  Whisper model features to identify Lex, so we can visualize the speaker in the transcript :)

这里有一个有趣的 AI 项目，大家可以尝试一下：收集 Lex 说话的一些语音片段，然后基于 Whisper 模型提取的特征来训练一个分类器（classifier），以便识别出 Lex 的声音。这样，我们就能在语音转录文本中清晰地标识出说话者了 :)

### 458

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574476529102311425
互动: Likes: 20; Retweets: 0; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@junaidali0300 he called me many times but I reject all podcasts equally atm. I enjoy his pod quite a bit though, maybe later!

@junaidali0300 他给我打了好多次电话，但我目前对所有播客都一视同仁地拒绝。不过，我确实挺喜欢他的播客的，也许以后有机会吧！

### 459

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574476813983547392
互动: Likes: 2; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@thetimeafternow I'll give it a try!

@thetimeafternow 我会试一试！

### 460

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574477534896394240
互动: Likes: 8; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@dahou_yasser I ran 'small' (default) and 'large' for comparison. I'm currently using a nice 4xA100 box on Lambda cloud, which took about ~8 (?) hours to transcribe all episodes on 'small', and is taking about ~24 (?) hours to do 'large'. Very rough estimates.

@dahou_yasser 为了进行比较，我分别尝试了「小型」（默认设置）和「大型」模式。目前，我正在 Lambda cloud 上使用一台性能不错的配备 4 块 A100 GPU 的服务器。在「小型」模式下，它大约用了 8 小时左右来转录所有剧集，而在「大型」模式下，预计需要 24 小时左右。这些都只是非常粗略的估算。

### 461

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574478161655447552
互动: Likes: 3; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@thetimeafternow sounds cool, looking forward to try it out!

@thetimeafternow 听起来很酷，期待去试试！

### 462

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574478595531022337
互动: Likes: 1; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@KarstenW_ yeah... like an embeded player in the page so one doesn't have to go back and forth. it's a good idea!

@KarstenW_ 对啊... 就像页面里能有个嵌入式播放器，这样用户就不用来回跳转了。这真是个好主意！

### 463

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574501715990102016
互动: Likes: 362; Retweets: 18; Replies: 22; Quotes: 1; Views: 0; Bookmarks: 52; isReply: 0

I actually mostly built Lexicap so I could share a few snippets of Nick Lane ep :). (I already read the books so I'm ~familiar with the topics, these snippets are just personally newish+notable). (Maybe a great podcast app would make threads like this much easier!)

我搭建 Lexicap 的主要目的，其实是为了能分享 Nick Lane 某一集节目的一些片段 :）。（我读过他的书，所以对这些主题已经比较熟悉了，这些片段对我个人而言只是比较新颖且值得注意）。（或许一个好用的播客应用能让像这样分享片段变得更轻松！）

### 464

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574501720394039296
互动: Likes: 96; Retweets: 6; Replies: 6; Quotes: 2; Views: 0; Bookmarks: 6; isReply: 1

"A cell is basically just a micro version of the planet."
https://t.co/3whZUVx8cC haven't thought about it this way before. https://t.co/ZoRZMj0R6Y

一个细胞，从本质上讲，就像是这个星球的一个微缩版。
https://t.co/3whZUVx8cC 以前从未这样思考过。https://t.co/ZoRZMj0R6Y

### 465

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574501724206735360
互动: Likes: 35; Retweets: 2; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

"[Organisms] are just a kind of an outgrowth of the earth"
https://t.co/SXV1X5A5bY (pourous, alkaline) hydrothermal vents on active wet rocky planet create a gradual path from "sterile inorganic planet" to "living cells". Pockets &amp; membranes protect and power early life chemistry https://t.co/ajbCZS5vYp

「生物不过是地球的一种‘衍生物'。」
https://t.co/SXV1X5A5bY 在活跃的湿润岩石行星上，（多孔、碱性）热液喷口创造了一条从「贫瘠无机的行星」走向「活细胞」的渐进之路。微型囊腔和膜结构保护并为早期生命化学反应提供能量 https://t.co/ajbCZS5vYp

### 466

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574501728308719616
互动: Likes: 38; Retweets: 3; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

"but by that definition, a rabbit is not alive."
https://t.co/GzaFAWv5r9 haha - on the difficulty (and relative lack of utility) of arguing about definitions of life. https://t.co/bXiF2jpE7R

"但按照那个定义，兔子也不算活物了。"
https://t.co/GzaFAWv5r9 哈哈 —— 这真体现了探讨生命定义之难 （以及其相对而言的意义不大）。https://t.co/bXiF2jpE7R

### 467

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574501732398219265
互动: Likes: 43; Retweets: 3; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 4; isReply: 1

"Basically, you're taking hydrogen and you're sticking it onto CO2 and it's powered by the sun."
https://t.co/NMMTmiZU0r life is hydrogenating carbon dioxide. Photosynthesis takes it from water but you could also take it from hydrogen sulfide, ferrious iron, etc... https://t.co/pW70obUZVm

简单来说，你正在将氢气加到二氧化碳上，而这个过程由太阳提供能量。
https://t.co/NMMTmiZU0r 生命的本质就是对二氧化碳进行氢化作用（hydrogenating carbon dioxide）。光合作用（Photosynthesis）从水中获取氢，但氢也可以来自硫化氢（hydrogen sulfide）、亚铁离子（ferrious iron）等等。https://t.co/pW70obUZVm

### 468

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574501736063979520
互动: Likes: 96; Retweets: 3; Replies: 10; Quotes: 1; Views: 0; Bookmarks: 2; isReply: 1

Ok semi-arbitrarily truncating here because there's too much to link otherwise :). My main interest in these topics is to understand the Fermi paradox: The impediments to life, the probability of overcoming them and the inevitability (or lack there of) of specific solutions.

好的，姑且在这里打住，不然要链接的内容实在太多了 ：）。我对这些主题的主要兴趣在于理解费米悖论（Fermi paradox)：生命面临的阻碍、克服这些阻碍的可能性，以及特定解决方案的必然性（或者说，它们是否必然出现）。

### 469

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574501734679924736
互动: Likes: 79; Retweets: 7; Replies: 6; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

"How many alien civilizations are out there? Do you think?" https://t.co/FDqcBgzox5 The whole section.
"I expect bacteria to be very common."

你认为有多少外星文明存在呢？
我预计细菌会非常常见。

### 470

作者: @karpathy
时间: 2022-09-26
链接: https://x.com/karpathy/status/1574504153174355968
互动: Likes: 15; Retweets: 1; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@andrey_kurenkov The reality is that yes plenty of companies/people have tried but they have all done a half-hearted and _bad_ job. It's not good.

@andrey_kurenkov 现实情况是，确实有很多公司或个人尝试过，但他们都只是敷衍了事，做得非常糟糕，效果很不理想。

### 471

作者: @karpathy
时间: 2022-09-27
链接: https://x.com/karpathy/status/1574786023275450368
互动: Likes: 1; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@KevinBenSmith @thetimeafternow @snipd_app cool! I checked it out, it's an interesting approach. A bit of a TikTok-ifying podcasts vibes. (the transcript is low quality though, much lower than what I'm used to from Whisper)

@KevinBenSmith @thetimeafternow @snipd_app 酷！我查看了一下，这是一个有趣的方法。有点像把播客「TikTok 化」了的感觉。(不过，转录质量很低，远低于我日常使用的 Whisper 的质量)

### 472

作者: @karpathy
时间: 2022-09-27
链接: https://x.com/karpathy/status/1574841955405553664
互动: Likes: 245; Retweets: 14; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 28; isReply: 0

Reminder of AI Grant application deadline this Saturday.  It's great timing to start an AI-native product company, as an advisor very excited to see what people are thinking about and come up with!

提醒：AI Grant 申请截止日期就在本周六。现在正是创办 AI 原生（AI-native）产品公司的好时机，作为一名顾问，我非常期待看到大家有什么想法和创意！

### 473

作者: @karpathy
时间: 2022-09-27
链接: https://x.com/karpathy/status/1574843974400954368
互动: Likes: 4; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Yoann_Buzenet strange, a large number of people have joined the channel fine?

@Yoann_Buzenet 奇怪，很多人都正常加入了频道啊？

### 474

作者: @karpathy
时间: 2022-09-27
链接: https://x.com/karpathy/status/1574906895453675521
互动: Likes: 1,360; Retweets: 82; Replies: 64; Quotes: 14; Views: 0; Bookmarks: 111; isReply: 0

It would be best if people made strong statements that are understood to be only 90% true, and ignore the counterexample police. This saves time and makes direction of statements clear.

最好是大家在提出有力的主张时，能接受这些主张可能只有九成正确，并且不用理会那些专门挑刺、寻找反例的人（即「反例警察」）。这样做可以节省时间，并能让言论（观点）的方向更加明确。

### 475

作者: @karpathy
时间: 2022-09-27
链接: https://x.com/karpathy/status/1574907961092190209
互动: Likes: 45; Retweets: 0; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 0

@pranayaryal my tweet is eg :p

@pranayaryal 我的这条推文只是个例子而已 :p

### 476

作者: @karpathy
时间: 2022-09-27
链接: https://x.com/karpathy/status/1574908528388558849
互动: Likes: 203; Retweets: 6; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

making false statements that are mostly true is also more fun so there is that too.

编造那些大体属实却并非完全真实的说法，也更加有趣，这也是一个额外的优点吧。

### 477

作者: @karpathy
时间: 2022-09-28
链接: https://x.com/karpathy/status/1574914684930514946
互动: Likes: 1; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Yoann_Buzenet ty for the heads up, I fixed the link in the description! (discord expires them in 7 days by default, but it's possible to change, as I did now)

@Yoann_Buzenet 多谢提醒，我已经修复了描述中的链接！（Discord 默认会在 7 天后让链接失效，但可以更改，我现在就改了)

### 478

作者: @karpathy
时间: 2022-09-28
链接: https://x.com/karpathy/status/1574919517611839490
互动: Likes: 33; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@DanielFein7 interesting point. you get an excuse to be efficient.

@DanielFein7 这个观点很有意思。这下你有理由可以光明正大地提高效率了。

### 479

作者: @karpathy
时间: 2022-09-28
链接: https://x.com/karpathy/status/1574958600551735301
互动: Likes: 52; Retweets: 0; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@michael_nielsen drop the "often". it's cleaner :)

@michael_nielsen 把「经常」这个词去掉吧。这样更简洁 :)

### 480

作者: @karpathy
时间: 2022-09-28
链接: https://x.com/karpathy/status/1575208587692892161
互动: Likes: 6; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@kaalam_ai @lexfridman Lex didn't add them to the playlist for some reason. I just processed all videos in his podcast playlist.

@kaalam_ai @lexfridman Lex 不知何故没有将它们添加到播放列表。我刚刚处理了他播客播放列表中的所有视频。

### 481

作者: @karpathy
时间: 2022-09-28
链接: https://x.com/karpathy/status/1575214222614462465
互动: Likes: 2,101; Retweets: 120; Replies: 65; Quotes: 13; Views: 0; Bookmarks: 25; isReply: 0

Super excited for Tesla AI Day later this week!! 🤖🧠
(👇cool event art by @DennisHongRobot  that I stumbled by on reddit, tried to beat it with stable diffusion but it's not quite there yet :D) https://t.co/DrwAtk53ZD

对本周晚些时候的 Tesla AI Day（特斯拉人工智能日）超级期待！！ 🤖🧠
(👇这是 @DennisHongRobot 创作的精彩活动海报，我在 reddit（美国社交新闻网站）上偶然看到的，尝试用 Stable Diffusion（稳定扩散）模型生成类似作品，但效果还差一点意思 :D）https://t.co/DrwAtk53ZD

### 482

作者: @karpathy
时间: 2022-09-28
链接: https://x.com/karpathy/status/1575216747421892608
互动: Likes: 482; Retweets: 8; Replies: 22; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@WholeMarsBlog @DennisHongRobot in spirit :)

@WholeMarsBlog @DennisHongRobot 理念上是如此 :)

### 483

作者: @karpathy
时间: 2022-09-29
链接: https://x.com/karpathy/status/1575544750274318336
互动: Likes: 19; Retweets: 0; Replies: 0; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@julien_c @ykilcher @victormustar love this track 👏

@julien_c @ykilcher @victormustar 太喜欢这首歌了 👏

### 484

作者: @karpathy
时间: 2022-09-30
链接: https://x.com/karpathy/status/1575669434538024960
互动: Likes: 1,116; Retweets: 45; Replies: 59; Quotes: 6; Views: 0; Bookmarks: 34; isReply: 0

Dear Apple I am not able to keep track of and get back to conversations across 10 apps. Needs some OS-level help to sort notifications into fyis and todos that you can sort through, mark as “unread” and deal with when you’re able. Sad as the concept is.

亲爱的 Apple，我发现自己无法有效地管理并及时回复来自 10 个不同应用里的对话。我急需操作系统层面的帮助，希望能把通知分成「仅供参考（FYI）」和「待办事项（To-do）」，这样我就可以方便地进行筛选、标记为「未读」，并在我有空的时候集中处理。尽管提出这样的需求本身就有点让人沮丧。

### 485

作者: @karpathy
时间: 2022-09-30
链接: https://x.com/karpathy/status/1575714561738432512
互动: Likes: 64; Retweets: 0; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@hardmaru @StabilityAI I wish! I can't make the GPUs come out very well sad :) https://t.co/Elk7J95qGv

@hardmaru @StabilityAI 我真希望如此！可惜我没法把 GPU 画得很好 :( https://t.co/Elk7J95qGv

### 486

作者: @karpathy
时间: 2022-09-30
链接: https://x.com/karpathy/status/1575718848300400641
互动: Likes: 74; Retweets: 4; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@hardmaru @StabilityAI I remember back when AI was a bit more raging hot, NVIDIA held a party at GTC for AI attendees and everyone in attendance got a surprise free GPU (TITAN X iirc). Fun times. https://t.co/o9znmo1QRb

@hardmaru @StabilityAI 我记得以前当 AI（Artificial Intelligence）的热度更高、风头正劲的时候，NVIDIA（英伟达）在 GTC 大会上为 AI 参会者举办了一场派对，当时所有与会者都收到了一块免费的 GPU（图形处理器）(TITAN X，如果我没记错的话）作为惊喜礼物。那段时光真是有趣啊。https://t.co/o9znmo1QRb

### 487

作者: @karpathy
时间: 2022-09-30
链接: https://x.com/karpathy/status/1575719821429207040
互动: Likes: 43; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@hardmaru @StabilityAI (I am reminded because Jensen announced it on the stage at the event, very much an Oprah "Everybody gets a GPU" moment irl :))

@hardmaru @StabilityAI（我之所以想起这件事，是因为 Jensen 在活动现场宣布时，那场面简直就像奥普拉（Oprah）在节目中说「每个人都得到一块 GPU」一样，非常振奋人心 :）)

### 488

作者: @karpathy
时间: 2022-09-30
链接: https://x.com/karpathy/status/1575720097812910080
互动: Likes: 39; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@hardmaru @StabilityAI THE CROWD WENT WILD

@hardmaru @StabilityAI 现场观众都沸腾了！

### 489

作者: @karpathy
时间: 2022-09-30
链接: https://x.com/karpathy/status/1575928090663866368
互动: Likes: 1,595; Retweets: 151; Replies: 62; Quotes: 10; Views: 0; Bookmarks: 86; isReply: 0

I was asked about what AI will look like in 3 decades. Reminder: it has not even been 1 decade yet since the ImageNet moment (though the anniversary is very close, imo October 13, 2022 per https://t.co/NPg2sm2Ojm). Imagining that much change, but 3X, and on an exponential is 🤯

有人问我，人工智能（AI）在未来三十年里会发展成什么样子。提请大家注意：自 ImageNet 时刻以来，其实还不到十年（虽然 ImageNet 时刻的周年纪念日已近在眼前，我个人判断其应为 2022 年 10 月 13 日 [引自 https://t.co/NPg2sm2Ojm]）。设想一下，在 ImageNet 时刻以来已发生的巨大变革基础上，再乘以三倍，并且这种变化还是以指数级速度发展，这简直令人难以置信 🤯

### 490

作者: @karpathy
时间: 2022-10-01
链接: https://x.com/karpathy/status/1576041953493282816
互动: Likes: 316; Retweets: 6; Replies: 5; Quotes: 1; Views: 0; Bookmarks: 6; isReply: 1

@tszzl (except imo there is a pretty big difference about whether your HD map is for direct use at test time, or for offline generation of labels to train neural nets)

@tszzl（不过在我看来，你的高清地图（HD map）是用于在测试时直接使用，还是用于离线生成标签来训练神经网络，这两者之间存在相当大的区别。)

### 491

作者: @karpathy
时间: 2022-10-01
链接: https://x.com/karpathy/status/1576044650938236928
互动: Likes: 3,033; Retweets: 100; Replies: 180; Quotes: 38; Views: 0; Bookmarks: 19; isReply: 0

My friends are forcing me to take 5 shots if anyone says “Software 2.0”

我的朋友们告诉我，如果有人提到「Software 2.0」，我就要喝 5 杯酒。

### 492

作者: @karpathy
时间: 2022-10-01
链接: https://x.com/karpathy/status/1576047566059278336
互动: Likes: 796; Retweets: 53; Replies: 8; Quotes: 9; Views: 0; Bookmarks: 8; isReply: 1

@JonathanGuito Not at all rote, loving the presentation so far! A lot of this was infant stages / abstract ideas at best earlier in the year. Amazing to see

@JonathanGuito 丝毫没有机械僵硬的感觉，我非常喜欢目前的展示！今年早些时候，很多内容充其量都只是萌芽阶段的抽象想法。能看到这些进展真是太棒了！

### 493

作者: @karpathy
时间: 2022-10-01
链接: https://x.com/karpathy/status/1576048864624140288
互动: Likes: 190; Retweets: 1; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@teslavangelist @DirtyTesLa try “two orders of magnitude” ;)

@teslavangelist @DirtyTesLa 可以试试「两个数量级」;)

### 494

作者: @karpathy
时间: 2022-10-01
链接: https://x.com/karpathy/status/1576057698092580864
互动: Likes: 9,022; Retweets: 248; Replies: 217; Quotes: 60; Views: 0; Bookmarks: 46; isReply: 0

my last tweet of the night i think... 😵‍💫🤪 https://t.co/KMGPKB9Fss

我想…… 这大概是我今晚发的最后一条推文了吧……😵‍💫🤪 https://t.co/KMGPKB9Fss

### 495

作者: @karpathy
时间: 2022-10-01
链接: https://x.com/karpathy/status/1576284295332696064
互动: Likes: 34; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@simonkalouche The sky isn’t designed for birds but the world is designed for humans

@simonkalouche 天空不是为鸟类设计的，但世界是为人类设计的

### 496

作者: @karpathy
时间: 2022-10-01
链接: https://x.com/karpathy/status/1576331766977077248
互动: Likes: 18; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@simonkalouche There will be a bit of both but imo one of those directions will progress a lot faster

@simonkalouche 两者兼而有之，但我认为其中一个方向会发展得快得多。

### 497

作者: @karpathy
时间: 2022-10-04
链接: https://x.com/karpathy/status/1577349418503716864
互动: Likes: 325; Retweets: 36; Replies: 13; Quotes: 2; Views: 0; Bookmarks: 51; isReply: 0

I am looking forward to when entire consortiums of variously-trained GPT experts and "Software 1.0" experts (calculators, google search, databases, ...) argue it out in extended reasoning documents before the final "judge GPT" reviews the evidence and decides the final answer.

我非常期待看到这样的场景：由各类经过不同训练的 GPT 专家和「软件 1.0」专家（例如计算器、谷歌搜索、数据库等使用者）组成的整个联盟，先在充分的论证文档中进行深入的探讨和博弈，最终再由「裁判 GPT」来审查所有证据，并给出最终的裁决。

### 498

作者: @karpathy
时间: 2022-10-04
链接: https://x.com/karpathy/status/1577350692057980929
互动: Likes: 1,855; Retweets: 163; Replies: 76; Quotes: 29; Views: 0; Bookmarks: 187; isReply: 0

I have about ~100 open tabs across 4 tab groups of papers/posts/github repos I am supposed to look at, but new &amp; more relevant ones come out before I can do so. Just a little bit out of control.

我大概有 100 个打开的标签页，分散在 4 个标签页组里，里面都是我本该看的论文、帖子和 GitHub 仓库。然而，我还没来得及处理这些，新的、更相关的资料就已经不断涌现了。这情况确实有点失控了。

### 499

作者: @karpathy
时间: 2022-10-04
链接: https://x.com/karpathy/status/1577351926391271424
互动: Likes: 7; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@KevinBenSmith @lexfridman it's not even close 👍

@KevinBenSmith @lexfridman 差远了 👍

### 500

作者: @karpathy
时间: 2022-10-04
链接: https://x.com/karpathy/status/1577352184466788352
互动: Likes: 4; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@johannes_hage @lexfridman wow, very cool!!

@johannes_hage @lexfridman 哇，非常酷！！

### 501

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577461393158115328
互动: Likes: 884; Retweets: 74; Replies: 78; Quotes: 7; Views: 0; Bookmarks: 270; isReply: 0

proof that sex is great: https://t.co/PxjuMqZ1Fw haha no but seriously i'm trying to build a simple model that explains why sexual reproduction is so overwhelmingly ubiquotous in complex life. the model here shows an advantage but not sure if right

开个玩笑，「性很棒」的证明在这里：https://t.co/PxjuMqZ1Fw。不过说正经的，我正在尝试构建一个简单的模型，来解释为何在复杂生命中，有性生殖（sexual reproduction）会如此普遍。目前这个模型显示了有性生殖的一项优势，但我不确定它是否完全正确。

### 502

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577474454103302144
互动: Likes: 2; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@crizcraig there are a lot of what seems to me 2nd+ order terms. the super simple model above shows an advantage already, is it the majority of the explanation?

@crizcraig 在我看来，这里有很多二阶或更高阶的项（2nd+ order terms）。上面那个非常简单的模型（super simple model）已经展现出了优势，这能解释大部分现象吗？

### 503

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577729009856614405
互动: Likes: 744; Retweets: 54; Replies: 10; Quotes: 1; Views: 0; Bookmarks: 61; isReply: 0

wow 🤯 very strong results 👏

哇，这些结果真是令人惊叹 🤯，非常出色 👏！

### 504

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577732652563501056
互动: Likes: 8; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@marcelsalathe thank you for the refs! (I was a little surprised by an advantage seen in the very simple model in the notebook, which I still only half-understand, intuitively)

@marcelsalathe 谢谢你的参考资料！（我对在 notebook 中那个非常简单的模型所展现出的优势感到有些惊讶，对此，我凭直觉也只理解了一半)

### 505

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577733414731456512
互动: Likes: 1; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@jbrownkramer but that by itself isn't the full story because just increasing the rate of mutation (increased std) in asexual repro works much worse.

@jbrownkramer 但这本身并不能说明全部情况，因为仅仅增加无性生殖中的突变率（即标准差 std 增加）效果会差很多。

### 506

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577734399122022401
互动: Likes: 1; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@guillempg i think the model is right. the integers at different positions are different costs because the fitness matrix F is 2-dimensional. so the gene position matters.

@guillempg 我认为这个模型是正确的。由于适应度矩阵 F 是二维的，因此不同位置上的整数所代表的成本是不同的。这意味着基因的位置确实很重要。

### 507

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577742743987552256
互动: Likes: 2,474; Retweets: 231; Replies: 37; Quotes: 12; Views: 0; Bookmarks: 541; isReply: 0

Yesterday I uploaded a new (1h56m) Lecture #4 https://t.co/019R9JJ8Yz 
We dive into statistics of deeper networks and:
- improve init (overconfident softmax, oversaturated tanh, kaiming init)
- build BatchNorm layer
- intro health diagnostics (act/grad histos, update:data ratio)

昨天我上传了最新的一节课，第 4 讲（总时长 1 小时 56 分钟）https://t.co/019R9JJ8Yz
在这节课中，我们深入探讨了更深层神经网络的统计特性，具体内容涵盖：
- 如何改进网络初始化（init），解决诸如「过于自信的 softmax（overconfident softmax）」、「过饱和的 tanh（oversaturated tanh）」等问题，并介绍了 Kaiming 初始化（Kaiming init）方法。
- 如何构建 BatchNorm（批归一化）层。
- 引入了网络健康诊断的方法，例如通过观察激活值 / 梯度直方图（act/grad histos）和更新与数据比率（update:data ratio）来判断网络状态。

### 508

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577746577463967745
互动: Likes: 611; Retweets: 28; Replies: 17; Quotes: 5; Views: 0; Bookmarks: 31; isReply: 1

@janvesp I'd like to make it easier for people to get into AI and believe it would lead to more prosperity more faster.

我希望让人们更容易接触人工智能（AI），并相信这将更快地带来更多繁荣。

### 509

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577747724144762880
互动: Likes: 509; Retweets: 19; Replies: 15; Quotes: 5; Views: 0; Bookmarks: 8; isReply: 1

@_jameshatfield_ Teaching is just a means to an end, not end by itself. What I missed is more the lowering of the barrier for people to get into AI, if I can be helpful. Teaching itself can sometimes be a bit exhausting, but I don't hate it.

@_jameshatfield_ 教学仅仅是一种达成目标的途径，而非目标本身。我更看重的是，如果我能有所助益，就是能进一步降低人们进入 AI（人工智能）领域的门槛。虽然教学本身有时确实会让人感到疲惫，但我不排斥它。

### 510

作者: @karpathy
时间: 2022-10-05
链接: https://x.com/karpathy/status/1577774165574025217
互动: Likes: 3; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@marcelsalathe wow, a lot to look through here 😅, thank you so much!!

@marcelsalathe 哇，这里有好多内容要细看呢😅，非常感谢您！！

### 511

作者: @karpathy
时间: 2022-10-06
链接: https://x.com/karpathy/status/1577825457495384064
互动: Likes: 7; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@edb0ss there's a unique optimum in this static problem and they both find it. but if the populations were under pressure in a common environment one would take over the other. maybe another version of the sim would directly simulate a pool of 50:50 a/sexual and let that run.

@edb0ss 在这个静态问题中，存在一个唯一的最佳结果，而且两者都能找到它。但如果这些种群在一个共同的环境中面临竞争压力，其中一方就会胜出并取代另一方。也许模拟的另一个版本可以直接模拟一个包含 50% 无性个体和 50% 有性个体的混合群体，然后观察其演化。

### 512

作者: @karpathy
时间: 2022-10-09
链接: https://x.com/karpathy/status/1579156548408201216
互动: Likes: 210; Retweets: 5; Replies: 12; Quotes: 2; Views: 0; Bookmarks: 2; isReply: 0

OH: “it should be short for high performance communication” :D

OH:「它应该就是高性能通信的缩写吧」:D

### 513

作者: @karpathy
时间: 2022-10-10
链接: https://x.com/karpathy/status/1579562367213793280
互动: Likes: 242; Retweets: 2; Replies: 10; Quotes: 1; Views: 0; Bookmarks: 6; isReply: 1

@ykilcher All departments, stakeholders, committees and boards have completed their review and signed off on this message. A user study showed that 65% of people rate this message as 3/5 or above, and are "likely" to adjust their view of the company in the positive direction.

@ykilcher 所有部门、利益相关者、委员会和董事会都已完成对这条消息的审核并予以批准。一项用户研究表明，65% 的人将这条消息评为 3/5 或以上，并且「很可能」会对公司的看法产生更积极的转变。

### 514

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903468986212352
互动: Likes: 74; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

We backprop through both cross entropy and batchnorm in two ways: 1) breaking them up, or better, 2) analytically deriving the gradient formula and implementing it. In the end we find that for our MLP, PyTorch autograd in loss.backward() "hides" only 20 lines of code. Not scary.

我们通过两种方式对交叉熵（cross entropy）和批归一化（batchnorm）进行反向传播（backprop)：1）将它们的计算过程分解开来，或者更好的做法是，2）解析推导梯度公式（gradient formula）并加以实现。最终我们发现，对于我们的多层感知机（MLP），PyTorch 的自动求导（autograd）在执行 `loss.backward（)` 时，其实只「隐藏」了区区 20 行代码。这听起来并不复杂。

### 515

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903467635716096
互动: Likes: 151; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

(yes I had a lot of fun with the thumbnail :D)

(是的，我做这个缩略图的时候玩得很开心 :D)

### 516

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903465609785344
互动: Likes: 1,574; Retweets: 154; Replies: 24; Quotes: 8; Views: 0; Bookmarks: 358; isReply: 0

🥷New (1h55m) Lecture #5: "Becoming a Backprop Ninja" https://t.co/ekZgAQON3O 
We take the 2-layer MLP from last lecture and backprop through all of it manually: cross entropy loss, linear layer 2, tanh, batchnorm, linear layer 1, embedding table. I give away answers in the video https://t.co/WQVSWJ0KLk

🥷新课上线（1 小时 55 分钟）第五讲："成为反向传播（Backprop）高手」https://t.co/ekZgAQON3O
我们将从上一讲的双层感知机（MLP）开始，手动实现对其所有组成部分的反向传播（Backprop)：包括交叉熵损失（Cross Entropy Loss）、第二层线性层（Linear Layer 2）、tanh 激活函数、批量归一化（Batch Norm）、第一层线性层（Linear Layer 1）和嵌入表（Embedding Table）。视频中会给出详细答案 https://t.co/WQVSWJ0KLk

### 517

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903473109241857
互动: Likes: 154; Retweets: 9; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 28; isReply: 1

This lecture is not meant to be 'watched', it is just an answer key to the Exercises 1-4 on this google colab, where you do the backpropagation for our MLP, and refer to the video when stuck: https://t.co/lpYRlcH4pn good luck!

这个讲座不是用来「看」的，它只是你在这个 google colab 上完成练习 1-4 的一个参考答案。你可以在这个 colab 上实现我们 MLP 的反向传播（backpropagation），如果遇到困难，再对照视频进行参考：https://t.co/lpYRlcH4pn 祝你好运！

### 518

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903471850950656
互动: Likes: 173; Retweets: 5; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 17; isReply: 1

I made this video because I don't believe that autograd "magically makes your neural net train". Backprop is well worth understanding and gaining an intuition for to become better at innovating on and debugging modern neural nets. See my earlier post: https://t.co/3xoaPXEN7S

我之所以制作这个视频，是因为我不认为自动求导（autograd）会「神奇地让你的神经网络训练起来」。相反，深入理解反向传播（Backprop）并对其建立直观认识，对于更好地创新和调试现代神经网络（neural net）来说非常重要，是十分值得的。更多内容请参考我之前的文章：https://t.co/3xoaPXEN7S

### 519

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903473109241857
互动: Likes: 154; Retweets: 9; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 28; isReply: 1

This lecture is not meant to be 'watched', it is just an answer key to the Exercises 1-4 on this google colab, where you do the backpropagation for our MLP, and refer to the video when stuck: https://t.co/lpYRlcH4pn good luck!

本课程并非旨在「观看」，它仅仅是这份 Google Colab 上练习 1-4 的解答。您可以在这个 Colab 环境中对我们的多层感知机（MLP）进行反向传播（Backpropagation）操作，并在遇到困难时参考视频：https://t.co/lpYRlcH4pn 祝您好运！

### 520

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903471850950656
互动: Likes: 173; Retweets: 5; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 17; isReply: 1

I made this video because I don't believe that autograd "magically makes your neural net train". Backprop is well worth understanding and gaining an intuition for to become better at innovating on and debugging modern neural nets. See my earlier post: https://t.co/3xoaPXEN7S

我制作这个视频，是因为我不相信自动微分（autograd）会「神奇地让你的神经网络（neural net）训练起来」。反向传播（Backprop）这个概念非常值得深入理解并培养直觉，这样才能更好地在现代神经网络的创新和调试方面取得进展。请看我之前的帖子：https://t.co/3xoaPXEN7S

### 521

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579903470282280960
互动: Likes: 55; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

We already had some intuition for backprop in micrograd, but that's just a tiny scalar-valued engine. Here everything gets more real &amp; efficient: 1) we backward pass with Torch tensors (data batches, lots of broadcasting) and 2) we use calculus to collapse gradients formulas.

我们虽然已经在 micrograd 中对反向传播（backprop）有了初步的直观感受，但那毕竟只是一个只能处理标量值的微型引擎。而在这里，情况变得更加真实，效率也大大提高： 一方面，我们使用 Torch 张量（Torch tensors）（它们能处理批次数据，并支持大量的广播（broadcasting）操作）进行反向传播；另一方面，我们运用微积分来简化和归并复杂的梯度公式。

### 522

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579984108058333184
互动: Likes: 336; Retweets: 37; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 94; isReply: 0

excellent snapshot of AI (as usual :))

对 AI 的一次精彩呈现（一如既往 :）)

### 523

作者: @karpathy
时间: 2022-10-11
链接: https://x.com/karpathy/status/1579984108058333184
互动: Likes: 336; Retweets: 37; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 94; isReply: 0

excellent snapshot of AI (as usual :))

一份出色的 AI 概览（一如既往地棒 :))

### 524

作者: @karpathy
时间: 2022-10-12
链接: https://x.com/karpathy/status/1580218930416582656
互动: Likes: 228; Retweets: 11; Replies: 7; Quotes: 2; Views: 0; Bookmarks: 20; isReply: 1

@natfriedman I was playing Stellaris recently a bit (great game!), encountered a "Fallen Empire" https://t.co/twjuku5b7X civ in the game and thought of Google.

@natfriedman 我最近玩了一会儿 Stellaris（一款很棒的游戏！），在游戏中遇到一个「堕落帝国（Fallen Empire）」文明 https://t.co/twjuku5b7X，这让我联想到了 Google。

### 525

作者: @karpathy
时间: 2022-10-12
链接: https://x.com/karpathy/status/1580219222575132673
互动: Likes: 474; Retweets: 26; Replies: 13; Quotes: 7; Views: 0; Bookmarks: 39; isReply: 1

@natfriedman "All Fallen Empires begin the game in Sleeping status. Despite their complete development and immense power, the Fallen Empires will remain passive, staying within their borders and taking no action unless provoked." haha

@natfriedman「所有堕落帝国（Fallen Empires）在游戏开局时都处于休眠状态。尽管它们已经发展完善并拥有雄厚的实力，这些堕落帝国仍将保持被动，固守自己的边界，除非受到挑衅，否则绝不会采取任何行动。」哈哈

### 526

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581516923279314945
互动: Likes: 14,555; Retweets: 1,021; Replies: 2,812; Quotes: 134; Views: 0; Bookmarks: 3,723; isReply: 0

Movies that I've seen 5+ times but ready &amp; willing to keep watching: Interstellar, Gladiator, Contact, Good Will Hunting, The Matrix, LotR 1/2/3, HP 1, Avatar, The Fifth Element, The Independence Day, Rush Hour, Armageddon, Stargate, Anchorman, Mean Girls, Terminator 2, more=? :)

那些我刷过 5 遍以上，但仍乐此不疲、随时准备重温的电影：星际穿越（Interstellar）、角斗士（Gladiator）、接触（Contact）、心灵捕手（Good Will Hunting）、黑客帝国（The Matrix）、指环王 1/2/3（LotR 1/2/3）、哈利波特 1（HP 1）、阿凡达（Avatar）、第五元素（The Fifth Element）、独立日（The Independence Day）、尖峰时刻（Rush Hour）、绝世天劫（Armageddon）、星际之门（Stargate）、王牌播音员（Anchorman）、贱女孩（Mean Girls）、终结者 2（Terminator 2）。还有更多吗？:)

### 527

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581517858139385856
互动: Likes: 188; Retweets: 3; Replies: 27; Quotes: 1; Views: 0; Bookmarks: 4; isReply: 1

@mystickago I didn't super like it :( I think because I read the short story first and it's hard to live up to, or something. It's missing some major themes that I love in the text, and just generally twists the story oddly

@mystickago 我不是特别喜欢它。我认为这可能是因为我先阅读了那篇短篇小说，而电影版（或改编版）很难达到原作的水准。它缺少了原作中我喜爱的一些主要主题，并且总体上以一种令人费解的方式改编了故事。

### 528

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581518329109311491
互动: Likes: 33; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@MSadeghee i like it a lot but only saw ~2 times i think, didn't have as much sticking potential for me

@MSadeghee 我非常喜欢它，但我觉得我只看了大约 2 次，它对我来说没有那么大的吸引力，让我反复观看。

### 529

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581518597347647488
互动: Likes: 55; Retweets: 0; Replies: 3; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@javierluraschi Of course, I like last 1/3 of the book much more, but I like first 2/3 of the movie much more :)

@javierluraschi 当然，我更喜欢书的后三分之一，但我更喜欢电影的前三分之二 :)

### 530

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581519829499932672
互动: Likes: 211; Retweets: 0; Replies: 11; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@JLrumberger Personally I really like 1,2,3, maaaaybe 4, but it's downhill fast from there imo. 1 is by far my favorite, has the spark that made the world so unique and beautiful. "You're a wizard Harry". "I'm a .... what?" 💀

@JLrumberger 我个人真的很喜欢第 1、2、3 部，可能勉强算第 4 部吧，但从那之后，在我看来就迅速走下坡路了。第 1 部是我最爱的一部，它拥有让那个世界如此独特和美丽的神韵。「你是个巫师，哈利。」「我是一个…… 什么？」（此处表达震惊或困惑的语气）

### 531

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581520131640856576
互动: Likes: 70; Retweets: 2; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@doki_jerry ♥️♥️♥️ Contact I may be at closer to 10

@doki_jerry ♥️♥️♥️ 我大概在 10 点左右可以联系。

### 532

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581520671183511553
互动: Likes: 578; Retweets: 10; Replies: 61; Quotes: 4; Views: 0; Bookmarks: 8; isReply: 1

@groccy1 Interstellar is soooo goood. Actually it triggered the tweet, as I was thinking of rewatching it again. I didn't love it at first, it was a bit disorienting, but my love for it somehow continues to grow over time.

@groccy1 《星际穿越》真是太棒了。实际上，是它促使我发了这条推文，因为我正想着再看一遍。我起初并不那么喜欢它，觉得有点让人困惑，但随着时间的推移，我对它的喜爱不知怎的持续增长。

### 533

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581523021725999104
互动: Likes: 17; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@TechRonic9876 I don't get how that could possibly be, but I did watch it and liked it, but didn't find it that re-watchable :)

@TechRonic9876 我不太明白这怎么可能，但我确实看过它，也挺喜欢，就是没觉得它特别值得重看 :)

### 534

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581523820489252864
互动: Likes: 85; Retweets: 0; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@darelcarey I do love Inception a lot, also very re-watchable (I think I'm only at ~3)

@darelcarey 我确实非常喜欢《盗梦空间》，也非常值得重看（我想我只看了大约 3 遍)

### 535

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581525401251110912
互动: Likes: 533; Retweets: 5; Replies: 27; Quotes: 5; Views: 0; Bookmarks: 5; isReply: 1

@OstynHyss Cooper what are you doing?
Docking.
It's not possible.
No... it's necessary.

OstynHyss Cooper 你在做什么？
对接（Docking）。
这不可能。
不…… 这是必须的。

### 536

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581525887379337216
互动: Likes: 120; Retweets: 2; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 10; isReply: 1

@josh_bickett The Fountain is heavily underrated

@josh_bickett 《The Fountain》被严重低估了。

### 537

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581526324501364736
互动: Likes: 14; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@karpuscul I don't know I just don't really like it  ¯\_(ツ)_/¯. Seems to come up often though.

@karpuscul 我也不知道，就是不太喜欢它 ¯\_(ツ)_/¯。不过，这东西好像还挺常见的。

### 538

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581528440087248896
互动: Likes: 38; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Pizzakiller85 @JLrumberger oh my god thanks for ruining my evening

@Pizzakiller85 @JLrumberger 天哪，你们真是把我晚上给毁了。

### 539

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581530401813602304
互动: Likes: 12; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 0

@scrollymctrolly @groccy1 Thank you, yes. It's not even that great but somehow I like it a lot anyway.

@scrollymctrolly @groccy1 谢谢，是的。它其实没有那么出色，但不知怎么的，我还是非常喜欢它。

### 540

作者: @karpathy
时间: 2022-10-16
链接: https://x.com/karpathy/status/1581530955625340928
互动: Likes: 11; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@ChrisGuthrie it's what plants crave :D

@ChrisGuthrie 植物就爱这一口！:D

### 541

作者: @karpathy
时间: 2022-10-17
链接: https://x.com/karpathy/status/1581865256933937152
互动: Likes: 336; Retweets: 23; Replies: 13; Quotes: 1; Views: 0; Bookmarks: 52; isReply: 0

Yep, good hints of what it will look like to give gadgets to GPTs

是的，这为我们预示了未来将 GPT（Generative Pre-trained Transformer）接入外部工具（或「小工具」）后的情景。

### 542

作者: @karpathy
时间: 2022-10-17
链接: https://x.com/karpathy/status/1582123501405601793
互动: Likes: 1,526; Retweets: 38; Replies: 92; Quotes: 5; Views: 0; Bookmarks: 18; isReply: 0

When you visit https://t.co/85TsRak6oG . Maybe if they added just one more prompt… https://t.co/oXAqm5WD0U

当你访问 https://t.co/85TsRak6oG 时。或许他们再多加一个提示词（prompt）就好了… https://t.co/oXAqm5WD0U

### 543

作者: @karpathy
时间: 2022-10-19
链接: https://x.com/karpathy/status/1582807370412937217
互动: Likes: 258; Retweets: 10; Replies: 10; Quotes: 1; Views: 0; Bookmarks: 10; isReply: 1

(2) because of residual connections, layer normalizations, and softmax attention. Absence of any flat tails. Residual connections support a kind of ability to learn short algorithms (think low LOC) fast and first, then gradually extend them longer during training.

(2）这是因为网络中存在残差连接（residual connections）、层归一化（layer normalizations）和 softmax 注意力（softmax attention）机制。它们避免了「平坦尾巴」（flat tails）现象。残差连接使得模型能够快速、优先地学习「短」算法（可以理解为代码行数较少，即低 LOC 的算法），然后在训练过程中，逐步将这些算法扩展得更长、更复杂。

### 544

作者: @karpathy
时间: 2022-10-19
链接: https://x.com/karpathy/status/1582807369234251776
互动: Likes: 293; Retweets: 9; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 13; isReply: 1

(1) because its message-passing-like architecture is general (i.e. completeness) and powerful (i.e. efficiency), able to cover many real-world algorithms and in a small number of compute steps; an an empirical finding.

(1）这是因为它的消息传递式架构（message-passing-like architecture）既通用（即具有完备性），又强大（即具有高效率），这意味着它能涵盖许多现实世界的算法，并且只需要少量计算步骤就能完成；这是一个通过实践得出的经验性发现。

### 545

作者: @karpathy
时间: 2022-10-19
链接: https://x.com/karpathy/status/1582807367988654081
互动: Likes: 4,086; Retweets: 553; Replies: 51; Quotes: 53; Views: 0; Bookmarks: 2,429; isReply: 0

The Transformer is a magnificient neural network architecture because it is a general-purpose differentiable computer. It is simultaneously:
1) expressive (in the forward pass)
2) optimizable (via backpropagation+gradient descent)
3) efficient (high parallelism compute graph)

Transformer 是一种卓越的神经网络架构（neural network architecture），因为它本质上是一个通用的可微分计算机（differentiable computer）。它同时具备以下特点：
1）强大的表达能力（在前向传播（forward pass）过程中)
2）易于优化（通过反向传播（backpropagation）和梯度下降（gradient descent）进行训练)
3）高效率（具有高度并行的计算图（compute graph))

### 546

作者: @karpathy
时间: 2022-10-19
链接: https://x.com/karpathy/status/1582807372841373696
互动: Likes: 318; Retweets: 9; Replies: 11; Quotes: 1; Views: 0; Bookmarks: 14; isReply: 1

Its success lies in a single architecture that simultaneously satisfies all of these properties. The original Attention Is All You Need paper is a bit haphazard and undersells the magnitude of these insights, their history and motivations. But there's a lot going on :)

它的成功秘诀在于一个单一的架构（architecture），这个架构能够同时满足所有这些特性。虽然最初的 Attention Is All You Need 论文在组织上略显随意，并且低估了这些核心见解、它们的演变历程和背后动机的重要性。但实际上，这篇论文中蕴含的思想和影响是极其深远的。

### 547

作者: @karpathy
时间: 2022-10-19
链接: https://x.com/karpathy/status/1582807371528622080
互动: Likes: 261; Retweets: 10; Replies: 2; Quotes: 2; Views: 0; Bookmarks: 18; isReply: 1

(3) because the compute graph is shallow and wide, mapping significantly better to our high-parallelism compute architectures (think GPUs). An earlier attempt that understood the significance and optimized for this property was the Neural GPU paper (https://t.co/d8eFjBkclh)

(3）因为计算图（compute graph）呈现出「浅而宽」的结构，这使得它能够显著更好地匹配我们具有高并行度（high-parallelism）的计算架构（比如图形处理器 GPU）。Neural GPU 论文（https://t.co/d8eFjBkclh）就曾较早地认识到这一特性并为此进行了优化尝试。

### 548

作者: @karpathy
时间: 2022-10-19
链接: https://x.com/karpathy/status/1582810859172077568
互动: Likes: 444; Retweets: 23; Replies: 20; Quotes: 3; Views: 0; Bookmarks: 35; isReply: 1

So I probably would have called the paper something like "Transformer: A general-purpose, efficient, optimizable computer" and presented it alongside the Neural Turing Machine, NeuralGPU and friends, then applied it to translation as an example. Something like that, but ok :)

所以，我或许会把那篇论文起名为「Transformer：一种通用、高效、可优化的计算机」，并将其与神经网络图灵机（Neural Turing Machine）、神经网络 GPU（NeuralGPU）等同类模型一同介绍，然后以机器翻译作为应用范例。大概就是这么个思路吧，不过也行 :)

### 549

作者: @karpathy
时间: 2022-10-19
链接: https://x.com/karpathy/status/1582822820140109824
互动: Likes: 377; Retweets: 15; Replies: 12; Quotes: 2; Views: 0; Bookmarks: 18; isReply: 1

A few people have (correctly) pointed out the hindsight here, which is fair. I don't suspect the authors would have known that 5 years later that architecture will have taken over most of AI ~unchanged, except for a re-shuffling of layernorms. Calls for a followup paper :)

一些人（正确地）指出，这其中带有「后见之明」的意味，这种看法很公正。我并不认为作者们当时会预见到，5 年后这种架构将几乎未作改动地主导绝大多数人工智能（AI）领域，仅仅调整了层归一化（Layer Normalization，简称 layernorms）的位置。这确实值得撰写一篇后续论文来探讨 :)

### 550

作者: @karpathy
时间: 2022-10-21
链接: https://x.com/karpathy/status/1583496375643144192
互动: Likes: 23; Retweets: 3; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@Dan_Jeffries1 not really a debate, more like a small united revolt in a state of confusion and disillusionment calling out what is perceived to be an abstract and inauthentic post

@Dan_Jeffries1 这不太像一场辩论，更像是一场小规模的、联合的反抗，这些人在困惑与幻灭中，质疑着一个被他们认为是抽象且不真实的帖子。

### 551

作者: @karpathy
时间: 2022-10-21
链接: https://x.com/karpathy/status/1583551309365407744
互动: Likes: 107; Retweets: 4; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 11; isReply: 1

@ID_AA_Carmack PyTorch ring Generator has a note in manual_seed that a good seed should have a balance of 0s and 1s, but they don’t mention why https://t.co/YDjYI8UFIQ

@ID_AA_Carmack PyTorch 的随机数生成器（Generator）在其 `manual_seed` 函数的文档中提到，一个好的种子（seed）在二进制表示中，0 和 1 的数量应该保持平衡，但并未说明具体原因 https://t.co/YDjYI8UFIQ

### 552

作者: @karpathy
时间: 2022-10-21
链接: https://x.com/karpathy/status/1583551843480702976
互动: Likes: 5; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@JoshuaA20190612 @ID_AA_Carmack I’m not able to yet I tried

@JoshuaA20190612 @ID_AA_Carmack 我试过了，但目前还不行。

### 553

作者: @karpathy
时间: 2022-10-29
链接: https://x.com/karpathy/status/1586450844723032064
互动: Likes: 5,580; Retweets: 344; Replies: 211; Quotes: 44; Views: 0; Bookmarks: 216; isReply: 0

Thanks Lex, I've enjoyed many of the previous episodes so it was a pleasure to come on! 
(we've known each other from before the podcast (via MIT/autonomy), it's been awesome to watch you grow it so successfully over time 👏)

谢谢 Lex，我非常喜欢之前的很多期节目，所以很高兴能来做客。
（我们早在播客开播前就认识了（通过 MIT/autonomy），很高兴看到你的播客随着时间发展得如此成功 👏）

### 554

作者: @karpathy
时间: 2022-11-02
链接: https://x.com/karpathy/status/1587921333702139904
互动: Likes: 151; Retweets: 8; Replies: 5; Quotes: 1; Views: 0; Bookmarks: 8; isReply: 1

Sometimes it's difficult to put the look&amp;feel of what you're after into text. You end up re-rolling results over and over again, looking for the needle in a haystack. stableboost flips it around - you create a large haystack of variations, then narrow in on the needle visually.

有时，很难用文字准确描述你想要的视觉风格和整体感受（look&amp;feel）。结果就是你不得不一遍又一遍地反复尝试，如同大海捞针般寻找那个完美的结果。stableboost 则反其道而行之 —— 它让你先生成海量的变体，然后通过视觉筛选的方式，精确锁定你想要的目标。

### 555

作者: @karpathy
时间: 2022-11-02
链接: https://x.com/karpathy/status/1587922343132684288
互动: Likes: 8; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@ArtirKel from my own experience you want something interactive and change your mind around quite a bit. so you're building the positive set, seeing the results, then tweaking your positive set over time. it's an incremental iterative thing.

### 556

作者: @karpathy
时间: 2022-11-02
链接: https://x.com/karpathy/status/1587923528061370369
互动: Likes: 154; Retweets: 7; Replies: 9; Quotes: 1; Views: 0; Bookmarks: 7; isReply: 1

e.g. I used stableboost for this earlier tweet :) - the prompt by itself gives bad, too diverse, not amazing results, but once I generated ~1000 I could visually narrow in on the composition I liked. Not sure how I'd get that by tuning the prompt alone  https://t.co/FOPJs52Gl9

例如，我在这条早先的推文中使用过 stableboost :）。仅仅依靠提示语（prompt）本身，生成的结果可能不尽理想，质量不高，或者过于多样化，难以达到令人惊艳的效果。然而，当我生成了大约 1000 个结果后，我便可以通过视觉筛选，精确地找到我偏爱的构图。我并不确定仅凭调整提示语这一种方法能否实现同样的效果 https://t.co/FOPJs52Gl9

### 557

作者: @karpathy
时间: 2022-11-02
链接: https://x.com/karpathy/status/1587925118759862273
互动: Likes: 241; Retweets: 2; Replies: 12; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@matttalbert @lexfridman @Tesla @elonmusk wow, very cool! done manually :O :)

@matttalbert @lexfridman @Tesla @elonmusk 哇，太酷了！手动完成的 :O :)

### 558

作者: @karpathy
时间: 2022-11-03
链接: https://x.com/karpathy/status/1588159965046272002
互动: Likes: 4; Retweets: 1; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@AMZoellner Base stable diffusion has a decent guess about me 🤷‍♂️

@AMZoellner 基础版 Stable Diffusion 对我猜得还挺准的 🤷‍♂️

### 559

作者: @karpathy
时间: 2022-11-07
链接: https://x.com/karpathy/status/1589419994370441216
互动: Likes: 409; Retweets: 32; Replies: 17; Quotes: 0; Views: 0; Bookmarks: 63; isReply: 0

AI Pub reaching for that @_akhaliq level of usefulness on AI twitter :)

AI Pub 正在努力变得像 @_akhaliq 在 AI 推特上那样实用和有价值 :)

### 560

作者: @karpathy
时间: 2022-11-08
链接: https://x.com/karpathy/status/1589905704303104000
互动: Likes: 353; Retweets: 8; Replies: 13; Quotes: 0; Views: 0; Bookmarks: 3; isReply: 1

@sharifshameem borderline unbelievable

@sharifshameem 简直令人难以置信

### 561

作者: @karpathy
时间: 2022-11-10
链接: https://x.com/karpathy/status/1590604672557252609
互动: Likes: 3,437; Retweets: 191; Replies: 235; Quotes: 34; Views: 0; Bookmarks: 143; isReply: 0

Not sure if there is a name for (I think no) the feeling of a deep discomfort when the probability of an interruption is &gt; 0 while trying to work. It’s a kind of fear.

我不太确定是否存在一个词来形容（我想可能没有）那种感觉：当尝试工作时，一旦中断的可能性大于零，就会产生一种强烈的不适。这其实是一种恐惧。

### 562

作者: @karpathy
时间: 2022-11-10
链接: https://x.com/karpathy/status/1590606186067681281
互动: Likes: 104; Retweets: 1; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@AnthonyLewayne Germans indeed have a significantly expanded vocabulary of feelings and situations. Much better job of compression!

@AnthonyLewayne 德国人确实拥有更加丰富的情感和情境词汇。他们在表达上做得更精炼！

### 563

作者: @karpathy
时间: 2022-11-10
链接: https://x.com/karpathy/status/1590855210699980802
互动: Likes: 7; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@skulpter I love this, exactly

@skulpter 我喜欢这个，完全正确

### 564

作者: @karpathy
时间: 2022-11-11
链接: https://x.com/karpathy/status/1590873229434159105
互动: Likes: 99; Retweets: 11; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 21; isReply: 0

MLPerf benchmark needs some of these mitigations https://t.co/yuAcUE6o4N

MLPerf 基准需要其中一些缓解措施 https://t.co/yuAcUE6o4N

### 565

作者: @karpathy
时间: 2022-11-11
链接: https://x.com/karpathy/status/1590881355961106433
互动: Likes: 542; Retweets: 66; Replies: 17; Quotes: 7; Views: 0; Bookmarks: 195; isReply: 0

Excellent post about applying insights from ML (overfitting control) to a much broader class of systems that optimize against an objective: politics, science, orgs, daily life. 

Underfitting is underrated.

这是一篇很棒的帖子，它探讨了如何将机器学习（ML）中的见解 （尤其是对过拟合的控制） 应用到更广泛的系统类别中。这些系统通常会针对某个目标进行优化，比如政治、科学、机构和日常生活。

欠拟合的价值常被低估。

### 566

作者: @karpathy
时间: 2022-11-11
链接: https://x.com/karpathy/status/1590908271665500161
互动: Likes: 2; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@JWonz exactly

@JWonz 没错

### 567

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592715502664970240
互动: Likes: 662; Retweets: 63; Replies: 15; Quotes: 7; Views: 0; Bookmarks: 165; isReply: 0

Is it the number of examples that matters or the number of presentations to the model during training? E.g. humans used spaced repetition to memorize facts but there are no equivalents of similar techniques in LLMs where the typical training regime is uniform random.

究竟是样本的数量更重要，还是在训练过程中向模型展示的次数更关键？举例来说，人类会使用间隔重复（spaced repetition）的方法来记忆事实，但在大语言模型（LLM）中，目前还没有类似技术的对应方法，因为它们通常采用的训练方案是均匀随机的。

### 568

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592715506825318401
互动: Likes: 102; Retweets: 4; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

4) ignore text because it's clearly just an outcome of a known algorithm and not "worth remembering", e.g. expansion of pi
5) some text is best written down on a piece of paper and not worth remembering
etc

4）忽略某些文本，因为它显然只是一个已知算法的结果，并没有记忆的价值，例如圆周率的数值展开
5）有些文本最好写在一张纸上，因为它不值得记忆等等

### 569

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592715504841809926
互动: Likes: 148; Retweets: 9; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 6; isReply: 1

More generally a few remarkable strategies people use during their training:
1) skim text because they already know it
2) ignore text because it's clearly noise (e.g. they won't memorize SHA256 hashes. LLMs will.)
3) revisit parts that are learnable but not yet learned

更广泛地说，人类在学习训练时会采用一些引人注目的策略：
1）快速浏览已知信息，因为他们已经掌握了这些内容。
2）忽略明显是「噪音」的文本，例如，人们不会去记住 SHA256 散列（hash），但大语言模型（Large Language Model，LLM）却会这么做。
3）回过头来学习那些可以掌握但尚未学会的部分。

### 570

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592715508855382017
互动: Likes: 248; Retweets: 12; Replies: 16; Quotes: 1; Views: 0; Bookmarks: 9; isReply: 1

Feels like a lot of fertile ground is left in managing the "attention" of an LLM during its training via a meta-learning policy, instead of the typical "memorize dataset uniformly at random" strategy. And giving it a calculator and a scratch pad.

我们感觉，在训练大语言模型（LLM）的过程中，通过元学习（meta-learning）策略来管理其「注意力」机制，而非沿用那种「随机均匀地记忆数据集」的传统方法，仍有广阔的探索空间。此外，如果能为它配备一个计算器和一个草稿本，潜力将更加巨大。

### 571

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592719390969311233
互动: Likes: 1,398; Retweets: 96; Replies: 50; Quotes: 12; Views: 0; Bookmarks: 109; isReply: 0

Prompt: "You are a GPT and you're in charge of training an even better GPT, congrats! You have a dataset here &lt;api&gt;. You can train it on document chunks like this: &lt;api&gt; and sample its current understanding like this: &lt;api&gt;. And here's a calculator and a scratchpad &lt;api&gt;. Begin:"

你是一个 GPT，现在你肩负着训练一个更强大的 GPT 的任务，恭喜你！你拥有一个数据集 &lt;api&gt;。你可以用像这样的文档块来训练它：&lt;api&gt;，并像这样评估它当前的理解程度：&lt;api&gt;。此外，这里还有一个计算器和一个草稿本供你使用：&lt;api&gt;。请开始你的工作：

### 572

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592721155982385152
互动: Likes: 7; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Thom_Wolf - ignore parts because they don't make sense yet (revisit later)
- summarize long passages into shorter cliff notes
- ...

@Thom_Wolf - 暂时忽略那些不具备当前意义的部分（稍后重新审视）
- 将冗长的段落总结为更简洁的要点
- ...

### 573

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592723027347013632
互动: Likes: 449; Retweets: 9; Replies: 23; Quotes: 0; Views: 0; Bookmarks: 7; isReply: 1

"Finally, we are very concerned that this GPT could be unaligned with humans. This would be bad. We want this to be a nice GPT that deeply loves all humans and is always considerate and helpful. Thanks"

最后，我们非常担心这个 GPT 可能未能与人类对齐（unaligned with humans）。这将带来不良后果。我们希望它是一个友善的 GPT，能深刻理解并始终体恤和帮助所有人类。

### 574

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592723025157558273
互动: Likes: 293; Retweets: 8; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 7; isReply: 1

"Obviously anything that looks useless (like SHA hashes or other noise) is not worth training on and is just wasting training capacity and time"
"You may want to start with simpler topics and work up to more complex later, just like in human school"

显然，任何看起来无用的数据（比如 SHA 哈希或其他噪声）都不值得用于训练，那样只会白白浪费训练资源和时间。
你可能希望从更简单的主题开始，然后逐步深入到更复杂的内容，就像人类在学校里学习一样。

### 575

作者: @karpathy
时间: 2022-11-16
链接: https://x.com/karpathy/status/1592756764453507072
互动: Likes: 82; Retweets: 1; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 10; isReply: 1

@johnowhitaker like! tiny idea tiny code, strips away the formalism except the high level idea (iterative denoising on a schedule)

@johnowhitaker 很棒！这是个小点子，代码也很精简，它剥离了所有的形式主义，只保留了核心思想（即有计划的迭代去噪）。

### 576

作者: @karpathy
时间: 2022-11-17
链接: https://x.com/karpathy/status/1593081701454204930
互动: Likes: 697; Retweets: 77; Replies: 22; Quotes: 10; Views: 0; Bookmarks: 253; isReply: 0

Good post. A lot of interest atm in wiring up LLMs to a wider compute infrastructure via text I/O (e.g. calculator, python interpreter, google search, scratchpads, databases, ...). The LLM becomes the "cognitive engine" orchestrating resources, its thought stack trace in raw text

这确实是一个有意思的观点。当前，人们对如何通过文本输入 / 输出方式（例如，利用计算器、Python 解释器、Google 搜索、草稿本、数据库等）将大语言模型（LLM）接入更广泛的计算基础设施抱有浓厚兴趣。在这种模式下，大语言模型将化身为「认知引擎」，负责调度和编排各种计算资源，而其「思考」过程 —— 就像程序运行时的堆栈追踪（stack trace）一样 —— 则以原始文本的形式被记录下来。

### 577

作者: @karpathy
时间: 2022-11-17
链接: https://x.com/karpathy/status/1593085221003755520
互动: Likes: 105; Retweets: 6; Replies: 8; Quotes: 0; Views: 0; Bookmarks: 5; isReply: 1

Interestingly the native and most general medium of existing infrastructure wrt I/O are screens and keyboard/mouse/touch. But pixels are computationally intractable atm, relatively speaking. So it's faster to adapt (textify/compress) the most useful ones so LLMs can act over them

有趣的是，就输入 / 输出（I/O）而言，现有基础设施最原生和最通用的媒介是屏幕，以及键盘、鼠标或触摸等交互方式。然而，相对而言，像素目前在计算上是难以处理的。因此，更快捷的方案是：将那些最有用的信息进行适配（比如文本化或压缩），以便大语言模型（LLMs）能够处理它们。

### 578

作者: @karpathy
时间: 2022-11-17
链接: https://x.com/karpathy/status/1593086746182705152
互动: Likes: 94; Retweets: 4; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 14; isReply: 1

Extending LLMs from text to vision will probably take time but, interestingly, can be made incremental. E.g. Flamingo (https://t.co/miFezjlZ3H (pdf)) processes both modalities simultaneously in one LLM.

将大语言模型（LLMs）从处理文本扩展到处理视觉信息，可能需要一些时间，但有趣的是，这个过程可以分阶段逐步实现。例如，Flamingo（https://t.co/miFezjlZ3H（pdf)）就能在一个大语言模型中同时处理文本和视觉这两种模态（modality）。

### 579

作者: @karpathy
时间: 2022-11-17
链接: https://x.com/karpathy/status/1593091486148489216
互动: Likes: 1,356; Retweets: 110; Replies: 103; Quotes: 27; Views: 0; Bookmarks: 137; isReply: 0

🤔automated companies made up just of LLMs (CEO LLM, manager LLMs, IC LLMs), running asynchronously and communicating over a Slack-like interface in text...

🤔想象一下：完全由大语言模型（大语言模型）组成的自动化公司，其中包含首席执行官（CEO）大语言模型、经理（Manager）大语言模型以及个体贡献者（IC）大语言模型。这些大语言模型异步运行，并通过类似 Slack 的文本界面相互沟通...

### 580

作者: @karpathy
时间: 2022-11-17
链接: https://x.com/karpathy/status/1593098615127695362
互动: Likes: 59; Retweets: 3; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@RuudNL they don't maximize rewards, they are given a prompt (a kind of inception) and continue the sequence

@RuudNL 它们的目的不是为了最大化奖励，而是接收到一个提示（就像在《盗梦空间》里被植入一个想法），然后沿着这个提示继续生成后续内容。

### 581

作者: @karpathy
时间: 2022-11-17
链接: https://x.com/karpathy/status/1593100340274298880
互动: Likes: 70; Retweets: 0; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 4; isReply: 1

@eladgil haha, I'm high level familiar with DAOs and I don't think so. LLM LLCs are about AI Power, not about decentralization, transparency, or governance. Actually in many ways opposite of DAOs in a basic execution of the idea.

@eladgil 哈哈，我对 DAO 有所了解，但我认为并非如此。大语言模型有限责任公司（LLM LLCs）关注的是 AI 的力量，而非去中心化、透明度或治理。实际上，在理念的实际执行上，它们在许多方面都与 DAO 相反。

### 582

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417974433517569
互动: Likes: 1,228; Retweets: 180; Replies: 39; Quotes: 13; Views: 0; Bookmarks: 414; isReply: 0

An interesting historical note is that neural language models have actually been around for a very long time but noone really cared anywhere near today's extent. LMs were thought of as specific applications, not as mainline research unlocking new general AI paths and capabilities

一个有趣的历史背景是，神经网络语言模型（neural language models）其实已经问世很久了，但那时候它们受到的关注度远不及今天。当时，人们认为语言模型（LMs）只是特定场景下的应用工具，并没有将其视为能够开启新的通用人工智能（General AI）发展方向和能力的 ** 主流研究 ** 领域。

### 583

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417979101732864
互动: Likes: 204; Retweets: 19; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 22; isReply: 1

E.g. ~20 years ago Bengio et al 2003 (pdf: https://t.co/br8txs304U) trained a neural language model. The state of the art GPT+friends of today are the exact same (autoregressive) model, except the neural net architecture is upgraded from an MLP to a Transformer. https://t.co/ZqoxCoxAIF

例如，大约在 20 年前，Bengio et al 2003（pdf：https://t.co/br8txs304U）训练了一个神经网络语言模型。令人惊讶的是，今天最先进的 GPT（Generative Pre-trained Transformer）及其同类模型，其核心正是这个完全相同的（自回归）模型，只不过神经网络架构从多层感知机（MLP）升级成了 Transformer。https://t.co/ZqoxCoxAIF

### 584

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417984646619136
互动: Likes: 225; Retweets: 21; Replies: 5; Quotes: 3; Views: 0; Bookmarks: 25; isReply: 1

The non-obvious crux of the shift is an empirical finding, emergent only at scale, and well-articulated in the GPT-3 paper (https://t.co/HhrwtZ4WQd). Basically, Transformers demonstrate the ability of "in-context" learning. At run-time, in the activations. No weight updates. https://t.co/W0atCg1d8K

这种转变不那么显而易见的核心是一个经验发现，它只有在模型达到一定规模时才会显现，并在 GPT-3 论文中得到了很好的阐述（https://t.co/HhrwtZ4WQd）。基本上，Transformer 模型展示了「情境学习（in-context learning）」的能力。这意味着模型在运行时，通过其内部的激活状态就能学习，而无需进行权重更新。https://t.co/W0atCg1d8K

### 585

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417989830848512
互动: Likes: 174; Retweets: 11; Replies: 1; Quotes: 2; Views: 0; Bookmarks: 33; isReply: 1

So the first critical "unlock technology" is the Transformer, a neural net architecture powerful enough to become a general-purpose computer. I've written more about this here: 1) https://t.co/So2JNYhIIN and 2) https://t.co/EFRDBa9UYu

那么，第一个关键的「解锁技术」就是 Transformer，它是一种神经网络架构（neural net architecture），强大到足以成为一台通用计算机。我曾在这里更详细地介绍过它：1）https://t.co/So2JNYhIIN 和 2）https://t.co/EFRDBa9UYu

### 586

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417987687473152
互动: Likes: 220; Retweets: 20; Replies: 3; Quotes: 5; Views: 0; Bookmarks: 56; isReply: 1

If previous neural nets are special-purpose computers designed for a specific task, GPT is a general-purpose computer, reconfigurable at run-time to run natural language programs. Programs are given in prompts (a kind of inception). GPT runs the program by completing the document

如果说以往的神经网络（neural nets）就像是为特定任务量身定制的专用计算机，那么 GPT 则更像是一台通用计算机。它可以在运行时被重新配置，从而执行自然语言程序。这些「程序」以提示词（prompts）的形式给出（这有点像《盗梦空间》中一层层梦境嵌套的感觉），而 GPT 则通过补全文档来「运行」这些程序。

### 587

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417993886654464
互动: Likes: 142; Retweets: 6; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 7; isReply: 1

Turns out language modeling (i.e. ~next word prediction; equivalent to compression) of internet text is this excellent objective - v simple to define and collect data for at scale. It forces the neural net to learn a lot about the world, "multi-tasking" across many domains.

事实证明，对互联网文本进行语言建模（language modeling)—— 也就是预测下一个词，这等同于数据压缩 —— 是一个非常出色的训练目标。它不仅极易定义，而且能够大规模地收集到相应的数据。这种方法促使神经网络（neural net）学习到关于世界的丰富知识，使其能够跨越多个领域进行「多任务处理」(multi-tasking）。

### 588

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417991940513797
互动: Likes: 151; Retweets: 5; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 8; isReply: 1

The second critical ingredient is that while a Transformer seems ~able to act as a general-purpose computer in principle, the training objective has to be hard enough to actually force the optimization to discover and converge onto it in the "weights space" of the network.

第二个关键要素在于，虽然 Transformer（Transformer）从理论上讲似乎有潜力像通用计算机一样运作，但训练目标必须足够有挑战性，才能真正促使优化过程在网络的「权重空间」（即所有可能权重配置的集合）中，发现并最终收敛到这种通用计算能力。

### 589

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417999318335488
互动: Likes: 252; Retweets: 20; Replies: 4; Quotes: 1; Views: 0; Bookmarks: 20; isReply: 1

But I still mispredicted in how much fertile ground there was in scaling up the paradigm. Like many others in AI I got distracted by Reinforcement Learning too soon, a kind of putting the cart before the horse, ...

但我仍然错误地估计了在扩展这一范式方面有多少广阔的发展空间。像许多其他 AI 领域的研究者一样，我过早地被强化学习（Reinforcement Learning）分散了注意力，这有点像是本末倒置，...

### 590

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417997133021184
互动: Likes: 160; Retweets: 11; Replies: 3; Quotes: 0; Views: 0; Bookmarks: 15; isReply: 1

I wrote this thread because I spent the last ~decade, obsessing over directions that would make fastest progress in AI, and was very interested in language models (e.g. my semi-famous 2015 post "The Unreasonable Effectiveness of Recurrent Neural Networks" https://t.co/z84SzhrnyR)

我之所以写下这篇内容，是因为在过去大约十年里，我一直沉浸于探索能让人工智能（AI）取得最快进展的方向，并且对语言模型抱有浓厚的兴趣（例如，我曾在 2015 年发表过一篇广为人知的文章，题为《循环神经网络（Recurrent Neural Networks）的不合理有效性》https://t.co/z84SzhrnyR）。

### 591

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593417995497316353
互动: Likes: 187; Retweets: 14; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 8; isReply: 1

TLDR: LMs have been around forever. Not obvious finding: turns out that if you scale up the training set and use a powerful enough neural net (Transformer), the network becomes a kind of general-purpose computer over text.

一句话总结：语言模型（Language Models）早已存在。一个出人意料的发现是，如果你扩大训练数据集并使用一个足够强大的神经网络（Transformer），这个网络就会变成一种处理文本的通用计算机。

### 592

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593418001235120129
互动: Likes: 280; Retweets: 20; Replies: 14; Quotes: 2; Views: 0; Bookmarks: 18; isReply: 1

when the core unlock was achieving a kind of general-purpose computer neural net via simple scalable objectives that have strong training signal (many bits of contraints per training example). Like language modeling, and not like reinforcement learning.
So that was interesting :D

当时，核心突破在于通过简单且可扩展的目标，实现一种通用计算机神经网络。这些目标拥有强大的训练信号（即每个训练示例都包含大量的有效约束信息）。这就像语言建模，而不是强化学习。
因此，这确实令人振奋 :D

### 593

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593421314030649344
互动: Likes: 43; Retweets: 0; Replies: 8; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@BorneRune actually a great benchmark imo 👍

@BorneRune 确实是一个很棒的基准，我认为值得称赞。

### 594

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593441919828271105
互动: Likes: 28; Retweets: 0; Replies: 5; Quotes: 1; Views: 0; Bookmarks: 0; isReply: 1

@bbabenko ? The carrot is building Twitter.

@bbabenko ？ 是胡萝卜在创建 Twitter 吗？

### 595

作者: @karpathy
时间: 2022-11-18
链接: https://x.com/karpathy/status/1593477319963705345
互动: Likes: 23; Retweets: 0; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@bbabenko I don't think that's giving enough credit to what Twitter already is today in the information age and where it can still go.

@bbabenko 我不认为这充分肯定了 Twitter 在信息时代已有的地位和其未来的发展潜力。

### 596

作者: @karpathy
时间: 2022-11-21
链接: https://x.com/karpathy/status/1594537380144623616
互动: Likes: 12; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@anri_m_lombard @mike64_t Very nice notes! 👍👍

@anri_m_lombard @mike64_t 笔记写得真棒！👍👍

### 597

作者: @karpathy
时间: 2022-11-21
链接: https://x.com/karpathy/status/1594573460478431232
互动: Likes: 20; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@hashhashbleep next up

@hashhashbleep 接下来是：

### 598

作者: @karpathy
时间: 2022-11-22
链接: https://x.com/karpathy/status/1594859647785369600
互动: Likes: 807; Retweets: 21; Replies: 29; Quotes: 10; Views: 0; Bookmarks: 43; isReply: 1

@realGeorgeHotz I search twitter on google with site:https://t.co/95zJm8fttQ . Works quite well

@realGeorgeHotz 我在 Google 上搜索 Twitter 的时候，会用 `site:https://t.co/95zJm8fttQ` 这个搜索指令。效果还挺不错的。

### 599

作者: @karpathy
时间: 2022-11-22
链接: https://x.com/karpathy/status/1594887729917366272
互动: Likes: 323; Retweets: 8; Replies: 22; Quotes: 0; Views: 0; Bookmarks: 16; isReply: 1

@hardmaru It works well when it’s force constrained to sites like reddit twitter etc. it just can’t be trusted to find good sites

@hardmaru 当它被强制限制在 Reddit、Twitter 等网站时，它能很好地工作。但它就是不可靠，无法找到真正好的网站。

### 600

作者: @karpathy
时间: 2022-11-23
链接: https://x.com/karpathy/status/1595218969295667201
互动: Likes: 379; Retweets: 3; Replies: 26; Quotes: 1; Views: 0; Bookmarks: 4; isReply: 1

@julien_c People get quieter when there is a dumpster fire in their timeline? I felt discouraged to share some stuff because it was not current thing

@julien_c 当人们在他们的社交媒体时间线上看到「一团糟」（即「垃圾箱着火」）的情况时，他们会变得更沉默吗？我因此打消了分享一些内容的念头，因为那些内容并不是当时的「热门话题」。

### 601

作者: @karpathy
时间: 2022-11-25
链接: https://x.com/karpathy/status/1595954036498649088
互动: Likes: 1,147; Retweets: 71; Replies: 45; Quotes: 11; Views: 0; Bookmarks: 96; isReply: 0

plot twist: stable diffusion 2.0 looks quite a bit worse on the few prompts i've tried so far compared to 1.5 (even not including celebrities/artists). Running theory seems to be this is due to an aggressive data sanitization campaign since the original release (?).

意想不到的是：Stable Diffusion 2.0 在我目前尝试过的少数提示词上，与 1.5 版本相比表现明显更差（即使不考虑生成名人 / 艺术家的内容）。目前流行的理论认为，这可能是由于自最初发布以来，模型进行了一次大规模的数据清洗（data sanitization）所致。

### 602

作者: @karpathy
时间: 2022-11-25
链接: https://x.com/karpathy/status/1595954041112039424
互动: Likes: 415; Retweets: 25; Replies: 22; Quotes: 1; Views: 0; Bookmarks: 19; isReply: 1

easy to compare a lot of images from both models on https://t.co/eIwkwiBOPg , e.g. "cute dog cooking tacos, photorrealistic", grid of boosted images from 1.5 (left) and 2.0 (right). 2.0 looking more distorted, cartoony, simpler, ignores text more. may need more prompt engineering https://t.co/U15M1TNDSF

通过访问 https://t.co/eIwkwiBOPg，可以方便地比较来自这两个模型（model）的许多图像。例如，在搜索「可爱的狗在煮玉米饼，超现实主义」这样的提示（prompt）后，我们能看到一个图像网格：其中 1.5 模型的增强图像在左侧，而 2.0 模型的增强图像在右侧。结果显示，2.0 模型生成的图像看起来更扭曲、更具卡通风格、更简单，并且对文本提示的关注度更低。这可能意味着它需要进行更多的提示工程（prompt engineering）[https://t.co/U15M1TNDSF]。

### 603

作者: @karpathy
时间: 2022-11-25
链接: https://x.com/karpathy/status/1595971244796440576
互动: Likes: 242; Retweets: 6; Replies: 21; Quotes: 0; Views: 0; Bookmarks: 6; isReply: 0

Is anyone able to steelman onward ticket travel requirements? Isn’t it a time (and process bloat) tax on 99.999% of good actors that the 0.001% bad actors can also easily circumvent?

有人能为「离境机票要求」提出一个充分的理由或正面论证吗？难道这不正是对 99.999% 的守规矩旅客征收的时间（和繁琐流程）成本，而剩下 0.001% 的恶意投机者却能轻易绕过吗？

### 604

作者: @karpathy
时间: 2022-11-28
链接: https://x.com/karpathy/status/1597329264059482112
互动: Likes: 717; Retweets: 54; Replies: 24; Quotes: 0; Views: 0; Bookmarks: 310; isReply: 0

quite enjoying "The Theory of Everything: The Quest to Explain All Reality" https://t.co/vCXXSSo5zv . (I listen to it as an audiobook on Audible +accompanying pdf but probably easier as video). Well-presented, insightful, good level of abstraction on a lot of modern physics.

最近在听《万物之理：解释所有现实的探索》（The Theory of Everything：The Quest to Explain All Reality）https://t.co/vCXXSSo5zv ，非常享受。（我在 Audible 上听它的有声书，还有附带的 PDF，不过可能看视频版会更容易理解）。这本书内容呈现得很好，富有洞察力，对很多现代物理学概念的抽象处理也恰到好处。

### 605

作者: @karpathy
时间: 2022-11-28
链接: https://x.com/karpathy/status/1597330500724883457
互动: Likes: 137; Retweets: 5; Replies: 8; Quotes: 0; Views: 0; Bookmarks: 10; isReply: 1

(more generally the Great Courses series is an awesome alternative to audiobooks on Audible, a lot of great lecture series and high quality concent)

(更广泛地说，The Great Courses 系列为 Audible 上的有声读物提供了一个绝佳的替代选择，其中包含了大量精彩的讲座系列和高品质内容)

### 606

作者: @karpathy
时间: 2022-11-28
链接: https://x.com/karpathy/status/1597331184123805697
互动: Likes: 17; Retweets: 4; Replies: 1; Quotes: 1; Views: 0; Bookmarks: 3; isReply: 1

@janbhwilhelm @mrdbourke @Suhail @chipro @lilianweng (I think he means my new NN: Zero to Hero series https://t.co/yh8L0mkG2r , which I'm still building out)

@janbhwilhelm @mrdbourke @Suhail @chipro @lilianweng（我猜他指的是我的新系列「神经网络（Neural Network)：从零到英雄」https://t.co/yh8L0mkG2r ，这个系列我仍在完善中)

### 607

作者: @karpathy
时间: 2022-11-28
链接: https://x.com/karpathy/status/1597347835514851329
互动: Likes: 204; Retweets: 15; Replies: 19; Quotes: 2; Views: 0; Bookmarks: 40; isReply: 0

Stumbled by the “Live vs Dead” player distinction a long while ago but often come back to. Applies very broadly in scale from people to organizations

很久以前偶然接触到「活 vs 死」这种对参与者（或实体）的区分概念，之后便经常回味。这个概念在从个人到组织的不同尺度上都非常适用。

### 608

作者: @karpathy
时间: 2022-11-28
链接: https://x.com/karpathy/status/1597353601030311937
互动: Likes: 4; Retweets: 0; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@rasbt I consume it ok with audio + having the accompanying pdf open. Without the pdf would be more mixed

@rasbt 我在听音频的时候，如果同时打开随附的 PDF，就能理解得不错。要是没有 PDF，理解起来就会比较困难了。

### 609

作者: @karpathy
时间: 2022-11-29
链接: https://x.com/karpathy/status/1597706870227030016
互动: Likes: 468; Retweets: 52; Replies: 14; Quotes: 2; Views: 0; Bookmarks: 134; isReply: 0

Nice! Like the track of work. Equations of Transformer are a bit like low-level microcode, this track tries to "go up" to uncover an implied assembly instruction set (e.g. RAW "read-arithmetic-write" operator?), and implemented algorithms on top of that for e.g. ridge regression.

不错！我很喜欢这个工作方向。Transformer 的公式有点像底层的微代码（microcode），这项研究试图「向上」发掘一套隐含的汇编指令集（assembly instruction set）(例如，可能是 RAW「读 - 运算 - 写」操作符？），并在此基础上实现了例如岭回归（ridge regression）等算法。

### 610

作者: @karpathy
时间: 2022-11-29
链接: https://x.com/karpathy/status/1597706872487743488
互动: Likes: 88; Retweets: 9; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 7; isReply: 1

A lot of fun in the Appendix, e.g. how GeLU can be used for multiplication / bypassing it as identity, use of LayerNorm for division, or bypassing that as identity, etc.

附录中藏着不少有趣发现，例如，GeLU （一种激活函数）如何能用于实现乘法或作为恒等函数（identity）被旁路（bypassing），以及 LayerNorm （层归一化）如何能用于实现除法或作为恒等函数被旁路等。

### 611

作者: @karpathy
时间: 2022-11-29
链接: https://x.com/karpathy/status/1597720856343744512
互动: Likes: 3; Retweets: 1; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@sebasibarguen I was listening to that one too. But depressing last lecture on his take on Fermi paradox, focusing on pre-bio part of things

@sebasibarguen 我也在听那个。但最后一堂关于他对费米悖论的讲座让人沮丧，它主要关注的是生命出现前（pre-bio）的那些阶段。

### 612

作者: @karpathy
时间: 2022-11-30
链接: https://x.com/karpathy/status/1597808109287723008
互动: Likes: 676; Retweets: 85; Replies: 17; Quotes: 4; Views: 0; Bookmarks: 238; isReply: 0

- https://t.co/lP9fSRy2E2
- https://t.co/l1NQibRabM
- https://t.co/IcaZawRNgX
- https://t.co/TJgZhJKml5
- https://t.co/HWPzelR7tM
- https://t.co/chNI8Jw39H
among only a few of the recent examples https://t.co/YV2TL9v7ug

- https://t.co/lP9fSRy2E2
- https://t.co/l1NQibRabM
- https://t.co/IcaZawRNgX
- https://t.co/TJgZhJKml5
- https://t.co/HWPzelR7tM
- https://t.co/chNI8Jw39H
这只是最近的几个例子中的一部分 https://t.co/YV2TL9v7ug

### 613

作者: @karpathy
时间: 2022-11-30
链接: https://x.com/karpathy/status/1597810327831257088
互动: Likes: 251; Retweets: 22; Replies: 11; Quotes: 0; Views: 0; Bookmarks: 26; isReply: 1

(diffusion is a new class of generative models, an alternative to the autoregressive generative modeling framework, independent of transformers. Feels intuitively more pleasing, flexible and powerful)

(扩散模型（diffusion model）是一类新型的生成模型，它们不同于自回归生成建模框架，并且不依赖于 Transformer 模型。从直观上看，扩散模型似乎更精妙、更灵活、也更强大。)

### 614

作者: @karpathy
时间: 2022-12-02
链接: https://x.com/karpathy/status/1598545143455698945
互动: Likes: 76; Retweets: 4; Replies: 1; Quotes: 2; Views: 0; Bookmarks: 4; isReply: 1

@ID_AA_Carmack @LambdaAPI +1 same, 👍👍 @LambdaAPI

@ID_AA_Carmack @LambdaAPI 我也一样，👍👍 @LambdaAPI

### 615

作者: @karpathy
时间: 2022-12-02
链接: https://x.com/karpathy/status/1598545756469989377
互动: Likes: 26; Retweets: 2; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@Suhail @ID_AA_Carmack @LambdaAPI *shudder* car dealership vibes

@Suhail @ID_AA_Carmack @LambdaAPI * 啊，真是让人不寒而栗 * 这感觉就像是到了汽车经销商那儿。

### 616

作者: @karpathy
时间: 2022-12-02
链接: https://x.com/karpathy/status/1598547827382448130
互动: Likes: 3,075; Retweets: 298; Replies: 78; Quotes: 35; Views: 0; Bookmarks: 175; isReply: 0

Best ChatGPT prompt so far 😂

迄今为止最好的 ChatGPT 提示词 😂

### 617

作者: @karpathy
时间: 2022-12-02
链接: https://x.com/karpathy/status/1598766710362079232
互动: Likes: 504; Retweets: 16; Replies: 50; Quotes: 4; Views: 0; Bookmarks: 1; isReply: 1

@rmarcilhoo @ShaneBeGood @realGeorgeHotz i'm bored

@rmarcilhoo @ShaneBeGood @realGeorgeHotz 我无聊

### 618

作者: @karpathy
时间: 2022-12-03
链接: https://x.com/karpathy/status/1598942462252572672
互动: Likes: 218; Retweets: 3; Replies: 11; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@realGeorgeHotz All engagement being equal yes

@realGeorgeHotz 是的，在所有互动（或参与度）都相同的情况下。

### 619

作者: @karpathy
时间: 2022-12-03
链接: https://x.com/karpathy/status/1599152286672248832
互动: Likes: 23,911; Retweets: 1,117; Replies: 766; Quotes: 92; Views: 0; Bookmarks: 157; isReply: 0

Plan is to throw a party in the Andromeda galaxy 1B years from now. Everyone welcome, except for those who litter

我们计划在从现在算起的 10 亿年后，在仙女座星系（Andromeda galaxy）举办一场盛大派对。欢迎所有人参加，但乱扔垃圾者除外。

### 620

作者: @karpathy
时间: 2022-12-04
链接: https://x.com/karpathy/status/1599488637422694400
互动: Likes: 1,994; Retweets: 215; Replies: 80; Quotes: 42; Views: 0; Bookmarks: 264; isReply: 0

The deepest unintuitive disconnect w.r.t. psychology of ChatGPT is that it doesn't get "time to think". It has a small, fixed amount of thought for each output token. A bit like human forced to speak very fast. Asking them to produce more text is giving them more time to think.

关于 ChatGPT 心理学最深层、反直觉的认知偏差在于它并没有「思考时间」。对于每个输出 Token（标记），它都只能分配到少量固定的思考资源。这有点像人类被迫语速飞快地说个不停。因此，要求 ChatGPT 生成更多文本，实际上就是给了它更多思考的时间。
</step3_reflined_translation>

### 621

作者: @karpathy
时间: 2022-12-04
链接: https://x.com/karpathy/status/1599493848165933056
互动: Likes: 570; Retweets: 32; Replies: 20; Quotes: 3; Views: 0; Bookmarks: 21; isReply: 1

When humans generate text (articles, posts, papers, etc) they spend very different amount of time per token, create intermediate work, make edits, etc. Very different from GPTs that just go chunk chunk chunk. But there seem to be enough puzzle pieces out and about to remedy.

当人类撰写文本（如文章、帖子、论文等）时，他们会在每个 Token（标记）上花费截然不同的时间，还会进行中间性的思考、草稿，以及反复修改等。这与 GPT 模型「一气呵成」地逐块生成文本的方式截然不同。不过，现在似乎已经具备了足够的零散要素或思路，有望解决这一问题。

### 622

作者: @karpathy
时间: 2022-12-05
链接: https://x.com/karpathy/status/1599641985824198658
互动: Likes: 110; Retweets: 6; Replies: 8; Quotes: 1; Views: 0; Bookmarks: 1; isReply: 1

@zswitten The funny thing is that humans don’t use “um” in writing, so GPT doesn’t know to use it. Even then, the input space has to be a kind of working memory for intermediate results, so just saying um wouldn’t be as helpful

@zswitten 有趣的是，人类在写作时并不会使用「嗯」这样的语气词，所以 GPT 自然也不知道该如何使用。即便如此， 输入空间（input space）本身就相当于一种存储中间结果的工作记忆（working memory），因此仅仅说「嗯」并不会带来太多帮助。

### 623

作者: @karpathy
时间: 2022-12-05
链接: https://x.com/karpathy/status/1599852921541128194
互动: Likes: 2,714; Retweets: 388; Replies: 64; Quotes: 73; Views: 0; Bookmarks: 623; isReply: 0

Potentially nitpicky but competitive advantage in AI goes not so much to those with data but those with a data engine: iterated data aquisition, re-training, evaluation, deployment, telemetry. And whoever can spin it fastest. Slide from Tesla to ~illustrate but concept is general https://t.co/6O2KxZBg17

这或许有些「吹毛求疵」，但在人工智能（AI）领域，真正的竞争优势与其说在于拥有大量数据，不如说在于拥有一个高效的「数据引擎（data engine）」。这个引擎包括了一系列迭代过程：数据获取、模型再训练、性能评估、模型部署和遥测（telemetry）系统。谁能最快地运转这套系统，谁就能占据上风。虽然这里引用的是 Tesla 的一张幻灯片来大致说明，但这个核心概念具有普遍意义。https://t.co/6O2KxZBg17

### 624

作者: @karpathy
时间: 2022-12-05
链接: https://x.com/karpathy/status/1599864243121455105
互动: Likes: 49; Retweets: 3; Replies: 0; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@dogestylz nice

@dogestylz 不错

### 625

作者: @karpathy
时间: 2022-12-05
链接: https://x.com/karpathy/status/1599889788223754241
互动: Likes: 1,356; Retweets: 128; Replies: 37; Quotes: 10; Views: 0; Bookmarks: 117; isReply: 0

We’ll come full hilarious circle when people use LLMs both to 1) expand a simple message like “execute faster” into email and 2) summarize an email back into the original simple message. It’s like compression/decompression into formalese

当人们开始用大语言模型（LLMs）做两件事时，一个充满黑色幽默的「完整循环」就形成了：一是把「执行得更快」这样简单的信息扩展成一封电子邮件，二是再把这封邮件总结回最初那个简单的信息。这简直就像是把信息「压缩 / 解压缩」成了一堆官样文章。

### 626

作者: @karpathy
时间: 2022-12-06
链接: https://x.com/karpathy/status/1599977711803723776
互动: Likes: 202; Retweets: 1; Replies: 10; Quotes: 1; Views: 0; Bookmarks: 12; isReply: 1

@alexandr_wang @goodside @scale_AI Still mulling it over but my temptation is more along the lines of “LLM psychologist”. Doesn’t have enough eng in it though

@alexandr_wang @goodside @scale_AI 还在思考，但我更倾向于「大语言模型（LLM）心理学家」这个说法。不过，它听起来技术含量不够。

### 627

作者: @karpathy
时间: 2022-12-06
链接: https://x.com/karpathy/status/1600012576825360384
互动: Likes: 11,289; Retweets: 613; Replies: 294; Quotes: 58; Views: 0; Bookmarks: 158; isReply: 0

How long until we measure wealth inequality in FLOPS

还需要多久，我们才能用 FLOPS（每秒浮点运算次数）来衡量财富不平等呢？

### 628

作者: @karpathy
时间: 2022-12-06
链接: https://x.com/karpathy/status/1600031572442218497
互动: Likes: 224; Retweets: 12; Replies: 7; Quotes: 2; Views: 0; Bookmarks: 21; isReply: 0

😂 stop Riley probably up there as someone who talks more to LLMs than other humans

😂 别开玩笑了，Riley 和大语言模型（LLM）对话的时间可能比和人类说话的时间还多呢！

### 629

作者: @karpathy
时间: 2022-12-06
链接: https://x.com/karpathy/status/1600214083206193153
互动: Likes: 3,283; Retweets: 261; Replies: 134; Quotes: 42; Views: 0; Bookmarks: 197; isReply: 0

My observations on applications of ChatGPT to society https://t.co/3eDy3vAUcC

我对于 ChatGPT 在社会中应用的几点观察 https://t.co/3eDy3vAUcC

### 630

作者: @karpathy
时间: 2022-12-06
链接: https://x.com/karpathy/status/1600216226034118656
互动: Likes: 510; Retweets: 18; Replies: 27; Quotes: 5; Views: 0; Bookmarks: 15; isReply: 1

(imo simple poem crafting is right in the thick of Moravec's paradox - difficult for humans to generate but quite tractable for an LLM to keep track of the statistics of all the possible words and how they rhyme)

(在我看来，简单的诗歌创作完美体现了莫拉维克悖论（Moravec's paradox）—— 这类任务对人类来说很难创作，但对于大语言模型（LLM）而言，追踪所有可能词汇的统计数据及其押韵规律却相当容易处理)

### 631

作者: @karpathy
时间: 2022-12-07
链接: https://x.com/karpathy/status/1600578169555529728
互动: Likes: 3,066; Retweets: 363; Replies: 50; Quotes: 22; Views: 0; Bookmarks: 1,186; isReply: 0

Dreambooth (stable diffusion finetuning for personal profile pictures) has been going viral last few days as well, for good reasons it's super fun; Unlike other places https://t.co/eIwkwiTY3o lets you play with infinite variations and experiment and play with your own prompts:

Dreambooth（一种针对个人头像进行稳定扩散微调（finetuning）的技术）在过去几天也风靡全网，它超级有趣，自然有其流行的道理；与其他平台不同的是，https://t.co/eIwkwiTY3o 允许你尝试无限多种变化，并尽情试验和发挥你自己的提示词（prompts)：

### 632

作者: @karpathy
时间: 2022-12-07
链接: https://x.com/karpathy/status/1600578178531340288
互动: Likes: 819; Retweets: 20; Replies: 24; Quotes: 2; Views: 0; Bookmarks: 20; isReply: 1

Turns out in a parallel Universe I'd look awesome as a samurai, cowboy and... saint? :D https://t.co/QCEdh7Gzve

看来在某个平行宇宙里，我扮演武士、牛仔甚至圣人都会很帅？:D https://t.co/QCEdh7Gzve

### 633

作者: @karpathy
时间: 2022-12-07
链接: https://x.com/karpathy/status/1600578187141840896
互动: Likes: 780; Retweets: 18; Replies: 24; Quotes: 2; Views: 0; Bookmarks: 19; isReply: 1

Stableboost auto-suggests a few hundred prompts by default but you can generate additional variations for any one prompt that seems to be giving fun/interesting results, or adjust it in any way: https://t.co/qWmadiXftP

Stableboost 默认会自动建议数百个提示，但如果你发现某个提示给出了有趣的结果，也可以为其生成更多变体，或者进行任意调整：https://t.co/qWmadiXftP

### 634

作者: @karpathy
时间: 2022-12-07
链接: https://x.com/karpathy/status/1600582722228875264
互动: Likes: 3,690; Retweets: 69; Replies: 168; Quotes: 31; Views: 0; Bookmarks: 78; isReply: 1

nice. 😂 https://t.co/U13tGLpv0V

不错。😂 https://t.co/U13tGLpv0V

### 635

作者: @karpathy
时间: 2022-12-07
链接: https://x.com/karpathy/status/1600583014899064832
互动: Likes: 297; Retweets: 16; Replies: 5; Quotes: 0; Views: 0; Bookmarks: 12; isReply: 1

Stableboost works really well for pictures of couples and animals not just individuals. Eg here’s our family dog looking grand and cute :) https://t.co/YEdGBHJLSw

Stableboost 对于情侣和动物的照片处理效果非常好，不仅仅局限于个人照片。例如，这是我们家狗狗威风又可爱的样子 :）https://t.co/YEdGBHJLSw

### 636

作者: @karpathy
时间: 2022-12-07
链接: https://x.com/karpathy/status/1600583461613412352
互动: Likes: 444; Retweets: 29; Replies: 24; Quotes: 0; Views: 0; Bookmarks: 83; isReply: 1

It’s really crazy to me that one can generate results this incredible and fun in just seconds, on demand, for any prompt you just think up on the spot. Upload ~20 images and try it out yourself https://t.co/eIwkwiBOPg

这真的太不可思议了：只需几秒钟，就能根据你即时想到的任何提示词，按需生成如此惊艳又充满乐趣的结果。上传大约 20 张图片，你也可以亲自体验一下 https://t.co/eIwkwiBOPg

### 637

作者: @karpathy
时间: 2022-12-07
链接: https://x.com/karpathy/status/1600589585167183873
互动: Likes: 15; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@poolio It's weird because about half of the photos I uploaded as training data I am smiling! Not sure why dreambooth so frowny

@poolio 这很奇怪，因为我上传作为训练数据的大约一半照片我都在笑！不确定为什么 dreambooth 生成的图像总是愁眉苦脸的。

### 638

作者: @karpathy
时间: 2022-12-08
链接: https://x.com/karpathy/status/1600646309853024257
互动: Likes: 38; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 11; isReply: 1

@techno_yoda lol the prompt was "a photoshoot of shirtless [subject], muscular, glistening six-pack" :D

@techno_yoda 哈哈哈，那个提示语是「给 [subject] 拍一组赤膊照，要肌肉发达，有清晰分明的腹肌（six-pack），并且油光锃亮」:D

### 639

作者: @karpathy
时间: 2022-12-08
链接: https://x.com/karpathy/status/1600791065552097281
互动: Likes: 82; Retweets: 3; Replies: 7; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@hardmaru Let’s talk about the real applications of AI

@hardmaru 我们来聊聊 AI 的实际应用吧

### 640

作者: @karpathy
时间: 2022-12-14
链接: https://x.com/karpathy/status/1603121952620630016
互动: Likes: 3; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 2; isReply: 1

@meetZaki the Prologue chapter of A Fire Upon the Deep

@meetZaki 《深渊上的火》的序章

### 641

作者: @karpathy
时间: 2022-12-14
链接: https://x.com/karpathy/status/1603149667256049664
互动: Likes: 914; Retweets: 40; Replies: 47; Quotes: 5; Views: 0; Bookmarks: 54; isReply: 0

A number of people have apparently joined me in celebrating #pioclock since this tweet so I am doubling down on making it a thing :D. Celebrate transcendence, irrationality, infinity and... circles: Set daily alarm for 3:14pm and take a picture with proof. Defy tau reformists!🔵

自从这条推文发布以来，似乎有不少朋友加入我一起庆祝 #pioclock 这个特别的时刻，所以我决定再接再厉，把它彻底「搞起来」:D。让我们一起庆祝圆周率（pi）所代表的超越性（transcendence）、无理性（irrationality）、无限性（infinity)…… 以及圆本身：每天下午 3:14 设置闹钟，然后拍下照片作为凭证。一起抵制那些推崇 tau 的「改革派」吧！🔵

### 642

作者: @karpathy
时间: 2022-12-14
链接: https://x.com/karpathy/status/1603171360812826624
互动: Likes: 937; Retweets: 37; Replies: 40; Quotes: 5; Views: 0; Bookmarks: 9; isReply: 1

Out and about with Shadowfax 🐎 ❤️ https://t.co/G7J3b3YDTF

和 Shadowfax 一起出门逛逛啦 🐎 ❤️ https://t.co/G7J3b3YDTF

### 643

作者: @karpathy
时间: 2022-12-14
链接: https://x.com/karpathy/status/1603175182775898112
互动: Likes: 13; Retweets: 0; Replies: 1; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@astrophileblog I’m right handed but prefer it on right. Apple Watch also supposed to be flipped around but I like it better this way. Rebel things 🤷‍♂️

@astrophileblog 我是右撇子，但我更喜欢把它戴在右边。Apple Watch 本来也应该翻个面（佩戴），但我更喜欢这样戴。就是这么特立独行 🤷‍♂️

### 644

作者: @karpathy
时间: 2022-12-15
链接: https://x.com/karpathy/status/1603194803528704000
互动: Likes: 136; Retweets: 4; Replies: 7; Quotes: 1; Views: 0; Bookmarks: 8; isReply: 1

References:
- LoTR movie intro https://t.co/GERNPNeWhX 🥲
- "show us the meaning of haste" https://t.co/dOyfcZRgVT 💀
- wiki https://t.co/qaZpRnH7RS
- lore video https://t.co/Uc4MROpCxW
one of the Mearas, capable of comprehending human speech, faster than the wind 🌪️✨

参考文献：
- 《指环王》电影开场片段 https://t.co/GERNPNeWhX 🥲
-「向我们展示急速的意义」https://t.co/dOyfcZRgVT 💀
- 维基 https://t.co/qaZpRnH7RS
- 背景故事视频 https://t.co/Uc4MROpCxW
其中一匹米拉斯（Mearas），它能理解人类的语言，速度比风还要快 🌪️✨

### 645

作者: @karpathy
时间: 2022-12-15
链接: https://x.com/karpathy/status/1603197214959882240
互动: Likes: 4; Retweets: 0; Replies: 2; Quotes: 0; Views: 0; Bookmarks: 0; isReply: 1

@goodsonNYC 👍 the most mysterious of the Istari. Was just recently reading Silmarillion / re-reading lotr

@goodsonNYC 👍 Istari 中最神秘的人物！最近刚读了《精灵宝钻》（Silmarillion），也重温了《魔戒》（lotr）。

### 646

作者: @karpathy
时间: 2022-12-15
链接: https://x.com/karpathy/status/1603304485907968001
互动: Likes: 1,123; Retweets: 126; Replies: 34; Quotes: 12; Views: 0; Bookmarks: 109; isReply: 0

The year is 2030. Legacy human-human interactions account for less than 1% of conversations on the internet 🤦‍♂️😅

到了 2030 年，传统的人与人之间的直接交流，在互联网的对话中占比将不足 1% 🤦‍♂️😅

### 647

作者: @karpathy
时间: 2022-12-15
链接: https://x.com/karpathy/status/1603327309372526592
互动: Likes: 26; Retweets: 1; Replies: 4; Quotes: 0; Views: 0; Bookmarks: 1; isReply: 1

@dfirmenich That this take is incorrect is I think one of the deepest and least intuitive truths

@dfirmenich 我认为，这种观点是错误的，这本身就是最深刻、也最反直觉的真理之一。

### 648

作者: @karpathy
时间: 2022-12-15
链接: https://x.com/karpathy/status/1603500185820151808
互动: Likes: 60; Retweets: 6; Replies: 1; Quotes: 0; Views: 5,971; Bookmarks: 4; isReply: 1

@shivon I also love that if you dig deeper into LOTR lore Shadowfax is one of the mearas (top tier horses that surpasses other horses in intelligence, speed and strength), understands human speech, can be summoned, and "knows" where to go much more autnomously. Just like the car :) ✨

@shivon 我也喜欢深入挖掘《指环王》（LOTR）的传说，你会发现「影疾」(Shadowfax）是米亚拉斯（mearas）之一，这种顶级马匹在智力、速度和力量上都远超其他马匹。它能听懂人类语言，可以被召唤，而且能非常自主地「知道」要去哪里。这简直就像那辆车一样呀！ :）✨

### 649

作者: @karpathy
时间: 2022-12-16
链接: https://x.com/karpathy/status/1603592108786319360
互动: Likes: 1,249; Retweets: 58; Replies: 43; Quotes: 13; Views: 165,972; Bookmarks: 27; isReply: 0

Avatar: The Way of Water 🌊  is beautiful, sentimental and Awesome. After decade+ of eagerly waiting. Plot a bit simple and stretched but the visuals and world building delivered at 11/10. Actually I’d like to watch just a Pandora documentary with exactly no plot.

《阿凡达：水之道》🌊 画面瑰丽、情感真挚，整体表现更是令人赞叹。在影迷们翘首以盼十多年后，这部影片终于登场。尽管故事情节略显单薄和拖沓，但其呈现的视觉奇观和宏大的世界构建，无疑达到了超乎满分的水平。说实话，我甚至希望能直接看到一部关于潘多拉星球的纯纪录片，完全不需要任何剧情。

### 650

作者: @karpathy
时间: 2022-12-16
链接: https://x.com/karpathy/status/1603603833128505344
互动: Likes: 25; Retweets: 1; Replies: 5; Quotes: 0; Views: 3,152; Bookmarks: 1; isReply: 1

@whitehotsand I did 3D IMAX, but the 3D I am not a fan of. Maybe too old. Also not sure I felt the frame rate was weird sometimes too high sometimes too low…

@whitehotsand 我看了 3D IMAX，但我不太喜欢 3D 效果。也许是我年纪大了。我也不太确定，感觉帧率（frame rate）有时太高，有时又太低，有点奇怪…

### 651

作者: @karpathy
时间: 2022-12-16
链接: https://x.com/karpathy/status/1603826699711303680
互动: Likes: 166; Retweets: 6; Replies: 13; Quotes: 0; Views: 96,927; Bookmarks: 24; isReply: 0

peak internet content, favorite historian on why Rings of Power feels like a non-sensical theater stage play (from an excellent history blog more generally). I did make it through all the episodes by use of very deep breaths

这是一段优质的互联网内容：我最喜欢的历史学家分析了《力量之戒》为何像一场毫无意义的舞台剧（这出自一个更出色的历史博客）。说实话，我真的是靠着深呼吸才勉强看完了所有剧集。

### 652

作者: @karpathy
时间: 2022-12-16
链接: https://x.com/karpathy/status/1603835488443330560
互动: Likes: 807; Retweets: 104; Replies: 18; Quotes: 1; Views: 197,677; Bookmarks: 179; isReply: 0

Nice work, app shows application to twitter search but the deeper demo is how good GPTs are in writing SQL. Very broadly applicable. wrt UIUX I like that the decoded SQL is available for verification, imo necessary for higher stake applications.

这项工作做得不错，这个应用程序展示了如何应用于推特搜索，但更深层的演示实际上是 GPTs 在编写 SQL（结构化查询语言）方面表现出的出色能力。这显然具有非常广泛的应用前景。在 UIUX（用户界面用户体验）方面，我很喜欢解码后的 SQL 代码是公开可供验证的，在我看来，这对于那些涉及较高风险的应用程序来说是必不可少的。

### 653

作者: @karpathy
时间: 2022-12-16
链接: https://x.com/karpathy/status/1603871648880349185
互动: Likes: 319; Retweets: 22; Replies: 19; Quotes: 9; Views: 67,329; Bookmarks: 11; isReply: 1

@sedielem pixels are the universal interface.

@sedielem 像素（pixel）是一种通用界面。

### 654

作者: @karpathy
时间: 2022-12-17
链接: https://x.com/karpathy/status/1603937801652690951
互动: Likes: 14; Retweets: 0; Replies: 0; Quotes: 0; Views: 2,816; Bookmarks: 0; isReply: 1

@djseo8 just the ones that tickled, personally :)

@djseo8 只是个人觉得合胃口的 :)

### 655

作者: @karpathy
时间: 2022-12-17
链接: https://x.com/karpathy/status/1603972442975657984
互动: Likes: 1,697; Retweets: 115; Replies: 53; Quotes: 17; Views: 296,490; Bookmarks: 101; isReply: 0

normally you'd compress then decompress. 
now we're going to decompress then compress.
yay https://t.co/RAalqRUh1F

通常情况下，我们习惯于先压缩数据再进行解压缩。
而现在，我们要反其道而行之，先解压缩再进行压缩！
太棒了！ https://t.co/RAalqRUh1F

### 656

作者: @karpathy
时间: 2022-12-17
链接: https://x.com/karpathy/status/1604204068565417984
互动: Likes: 950; Retweets: 61; Replies: 42; Quotes: 4; Views: 347,235; Bookmarks: 149; isReply: 0

Great video on helion fusion. Few thoughts:
- "no steam turbine" umm SOLD :)
- triggers my hard tech envy for natural sciences, sometimes feel deep learning is not that deep
- how can systems like chatgpt++ help accelerate this kind of work? how "intelligence constrained" is it?

-「没有蒸汽涡轮机」嗯，我心动了 :)
- 这让我对自然科学领域的硬核技术产生了羡慕之情，有时觉得深度学习（deep learning）似乎没那么‘深'了
- 像 ChatGPT++ 这样的系统如何帮助加速这类工作？这项工作在多大程度上受到智能（或认知）能力的限制？

### 657

作者: @karpathy
时间: 2022-12-17
链接: https://x.com/karpathy/status/1604207465482313728
互动: Likes: 20; Retweets: 0; Replies: 2; Quotes: 0; Views: 2,031; Bookmarks: 0; isReply: 1

@michalwols @ylecun dislike branded shirts, never had free food at work, never went to burning man, hate meditation, strong regrets touching Medium. I barely belong here :)

@michalwols @ylecun 我不喜欢品牌衬衫，工作时从未享受过免费餐食，也没去过火人节，讨厌冥想，并且非常后悔用过 Medium （一个在线发布平台）。我好像跟大家格格不入 :)

### 658

作者: @karpathy
时间: 2022-12-17
链接: https://x.com/karpathy/status/1604230274140684288
互动: Likes: 433; Retweets: 45; Replies: 19; Quotes: 1; Views: 160,094; Bookmarks: 118; isReply: 0

Good reading on AI alignment, I've been wondering how one could steer LLMs with an equivalent of Three Laws of Robotics

这是一篇关于 AI 对齐（AI alignment）的精彩文章，我一直在思考，如何才能用一套类似于机器人三定律（Three Laws of Robotics）的规则来引导或约束大语言模型（LLMs）。

### 659

作者: @karpathy
时间: 2022-12-17
链接: https://x.com/karpathy/status/1604244442164400128
互动: Likes: 82; Retweets: 0; Replies: 9; Quotes: 0; Views: 20,097; Bookmarks: 4; isReply: 1

@dpkingma 👍I guess I'm a bit more interested in chatgpt++ for scientific discovery more broadly and what that would take / look like.

@dpkingma 👍 我想，我对 chatgpt++ 在更广泛的科学发现领域的应用，以及实现这种应用所需的条件和可能呈现出的形态，更感兴趣。

### 660

作者: @karpathy
时间: 2022-12-18
链接: https://x.com/karpathy/status/1604352813793914881
互动: Likes: 81; Retweets: 2; Replies: 4; Quotes: 0; Views: 8,198; Bookmarks: 0; isReply: 1

@BigTechAlert @Tesla @michael_nielsen Go home @BigTechAlert you’re drunk I’ve followed Michael for many years

@BigTechAlert @Tesla @michael_nielsen 歇歇吧 @BigTechAlert 你在胡说八道。我关注 Michael 很多年了。

### 661

作者: @karpathy
时间: 2022-12-25
链接: https://x.com/karpathy/status/1607104323175211008
互动: Likes: 2,748; Retweets: 150; Replies: 70; Quotes: 22; Views: 530,764; Bookmarks: 103; isReply: 0

My code comments were there to help the humans. 
Now they are there to help the copilot.
Before they were for humans, now they aid the AI,
It's a new way of coding, I can't deny.

我的代码注释曾是为了帮助人类。
现在它们是为了帮助 copilot。
过往为人类而设，如今它们辅助 AI，
这是一种全新的编码方式，我无法否认。

### 662

作者: @karpathy
时间: 2022-12-25
链接: https://x.com/karpathy/status/1607104818509905920
互动: Likes: 1,481; Retweets: 54; Replies: 37; Quotes: 6; Views: 348,064; Bookmarks: 31; isReply: 0

Why write a tweet without a poem,
When ChatGPT can translate it with grace,
Turning mundane words into a beautiful ode,
Giving your message a new artistic face.

有了 ChatGPT（ChatGPT）能优雅地转换，
何苦只发一条平淡无诗的推文？
它能将寻常语句化作优美颂歌，
让你的信息焕发新的艺术神韵。

### 663

作者: @karpathy
时间: 2022-12-26
链接: https://x.com/karpathy/status/1607417502610644993
互动: Likes: 123; Retweets: 6; Replies: 7; Quotes: 0; Views: 34,610; Bookmarks: 0; isReply: 1

@fastml_extra Hey don’t make fun of ChatGPT it’s just trying to be a helpful language model

@fastml_extra 嘿，别取笑 ChatGPT，它只是想成为一个有用的大语言模型。

### 664

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607791539258003457
互动: Likes: 514; Retweets: 16; Replies: 22; Quotes: 2; Views: 97,220; Bookmarks: 24; isReply: 1

Context I realized I have to split up minGPT because I can't properly simultaneously satisfy both 1) educational and 2) efficient in one repo. So I'm separately writing 1) the maximally educational minGPT (+video etc.) and 2) a more efficient (still ~clean) version that has teeth

我意识到 minGPT 项目需要拆分，因为我无法在一个代码库（repo）中妥善兼顾 1）教学用途和 2）运行效率这两个需求。因此，我正在分别开发：1）一个最侧重教学的 minGPT 版本（+ 视频等），以及 2）一个性能更高（仍然很简洁）且更具竞争力的版本。

### 665

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607791537978748929
互动: Likes: 1,425; Retweets: 73; Replies: 36; Quotes: 3; Views: 378,648; Bookmarks: 332; isReply: 0

having fun optimizing minGPT today
- base: 495ms
- zero_grad(set_to_none=True): 492
- torch.jit.script gelu: 463
- OMP_PROC_BIND=CLOSE: 453
- torch.backends.cuda.matmul.allow_tf32: 143
- torch.autocast(torch.bfloat16): 121
- FlashAttention: 102
now: more fused kernels more better

今天在优化 minGPT 模型时取得了一些进展：
- ** 基线性能 **：495ms
- ** 优化零梯度（zero_grad（set_to_none=True))**：492ms
- ** 使用 torch.jit.script 编译 GELU 激活函数（torch.jit.script gelu)**：463ms
- ** 设置 OMP 进程绑定（OMP_PROC_BIND=CLOSE)**：453ms
- ** 启用 TF32 精度的 CUDA 矩阵乘法（torch.backends.cuda.matmul.allow_tf32)**：143ms
- ** 使用 bfloat16 混合精度自动类型转换（torch.autocast（torch.bfloat16))**：121ms
- ** 应用 FlashAttention 技术 **：102ms
未来的优化方向是：更多地采用融合核函数（fused kernels），以期获得更好的性能。

### 666

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607793133454258179
互动: Likes: 40; Retweets: 1; Replies: 1; Quotes: 0; Views: 7,149; Bookmarks: 0; isReply: 1

@zaptrem great! yes i think i can get to today

@zaptrem 太棒了！是的，我想我今天能赶到。

### 667

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607795455458672641
互动: Likes: 16; Retweets: 2; Replies: 2; Quotes: 0; Views: 2,313; Bookmarks: 0; isReply: 1

@realohtweets educational: the code is for the human
efficient: the code is for the computer

@realohtweets
重视可读性：代码是写给人类看的追求效率：代码是写给计算机运行的

### 668

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607797912037380097
互动: Likes: 14; Retweets: 0; Replies: 1; Quotes: 0; Views: 7,857; Bookmarks: 0; isReply: 1

@itsclivetime yeah fp16 is a little more efficient atm for the code as I have it right now but then need gradient scaler ;s

@itsclivetime 没错，就我目前的代码而言，fp16 现在效率更高一些，但接下来需要用到梯度缩放器（gradient scaler）。

### 669

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607799470305185793
互动: Likes: 21; Retweets: 1; Replies: 1; Quotes: 0; Views: 4,870; Bookmarks: 6; isReply: 1

@itsclivetime the high level picture is easy enough but keeping track of the mixed precision around the whole network, the dynamical behavior of the values and ranges, the support for them and their conversions across all the various kernels and library versions everywhere, is the nightmare https://t.co/hOAg5lSQW0

@itsclivetime 从宏观上看，情况很容易理解，但真正令人头疼的是，要追踪整个网络中的混合精度（mixed precision），包括值的动态行为和范围，以及确保在各种内核和库版本中对这些精度及其转换的支持，这简直就是一场噩梦 https://t.co/hOAg5lSQW0

### 670

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607815228678602752
互动: Likes: 19; Retweets: 2; Replies: 2; Quotes: 0; Views: 7,923; Bookmarks: 0; isReply: 1

@rasbt Yeah I think it’s best to sequence them, 1 then 2

@rasbt 是的，我认为最好是按这个顺序来：先 1 后 2。

### 671

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607865786676412416
互动: Likes: 3; Retweets: 1; Replies: 1; Quotes: 0; Views: 1,023; Bookmarks: 0; isReply: 1

@vgoklani_api careful see https://t.co/PZgGGzJXvo

我们发现，许多常见的媒体文件（例如 JPEG、PNG、FLAC、MP4 文件）和压缩档案（例如 TAR、ZIP 文件）的压缩比（原始文件大小与压缩后文件大小的比值）大约是 1.0。这表明这些文件已经通过专门算法进行了高度压缩，所以像 Brotli 和 Gzip 这样的通用压缩器几乎无法对其进行进一步压缩。图 1 展示了这一现象，表 1 则详细列出了不同文件类型的平均压缩比。我们的研究确定了压缩数据阻止通用压缩器有效进行进一步压缩的几个关键特性：（1）信息熵低；（2）存在自定义文件头和元数据；（3）缺乏足够规模的冗余模式供通用算法利用。

### 672

作者: @karpathy
时间: 2022-12-27
链接: https://x.com/karpathy/status/1607866216412254208
互动: Likes: 8; Retweets: 1; Replies: 1; Quotes: 0; Views: 3,366; Bookmarks: 0; isReply: 1

@benjamin_bolte yep great repo

@benjamin_bolte 嗯，这个仓库（repo）很棒。

### 673

作者: @karpathy
时间: 2022-12-28
链接: https://x.com/karpathy/status/1608022195443503104
互动: Likes: 622; Retweets: 11; Replies: 16; Quotes: 1; Views: 75,851; Bookmarks: 14; isReply: 1

@amasad It’s almost like… they don’t go there for the lectures… 🤔

@amasad 就好像是… 他们去那里可不是为了听讲座的… 🤔

### 674

作者: @karpathy
时间: 2022-12-29
链接: https://x.com/karpathy/status/1608288941425172480
互动: Likes: 5; Retweets: 1; Replies: 1; Quotes: 0; Views: 902; Bookmarks: 1; isReply: 1

@silfen2 @natalietran Haha I watched too much communitychannel circa ~2008 (ish?) and here we are... :D

@silfen2 @natalietran 鉴于我大约在 2008 年左右观看了大量的 communitychannel 内容，便有了如今的局面。

### 675

作者: @karpathy
时间: 2022-12-29
链接: https://x.com/karpathy/status/1608348299060449282
互动: Likes: 7; Retweets: 1; Replies: 0; Quotes: 0; Views: 1,249; Bookmarks: 1; isReply: 1

@wbrenton3 @iamtrask @seb_ruder let's introduce a hashtag and just use twitter? how about #lossfunctiontumblr ? :)

@wbrenton3 @iamtrask @seb_ruder 不如我们发起一个话题标签，然后只用 Twitter 怎么样？#lossfunctiontumblr 如何？ :)

### 676

作者: @karpathy
时间: 2022-12-30
链接: https://x.com/karpathy/status/1608627941101146114
互动: Likes: 18; Retweets: 1; Replies: 1; Quotes: 2; Views: 6,739; Bookmarks: 1; isReply: 1

@zaptrem To follow up, I had a chance to try it btw:
before: 212ms / iter
&gt;&gt;&gt; model = torch.compile(model)
after: 135ms / iter
nice :)

@zaptrem 接着上次的话题，我抽空尝试了一下：
优化前：212 毫秒 / 迭代
>>> model = torch.compile（model)
优化后：135 毫秒 / 迭代效果很好 :)

### 677

作者: @karpathy
时间: 2022-12-30
链接: https://x.com/karpathy/status/1608632631591329792
互动: Likes: 4; Retweets: 1; Replies: 1; Quotes: 0; Views: 1,243; Bookmarks: 0; isReply: 1

@zaptrem Ah, I reverted FlashAttention in this run because it made code messier. Will look into incorporating it back, but yes not sure how nicely it plays with torch.compile. The usual problem with taking on large dependencies you don't understand ;(

@zaptrem 啊，我在这轮运行中撤销了 FlashAttention 的使用，因为它导致代码变得更混乱了。我会考虑把它重新集成进来，但确实不确定它与 torch.compile 的兼容性怎么样。这正是使用你不熟悉的大型依赖库时常遇到的问题；(

### 678

作者: @karpathy
时间: 2022-12-30
链接: https://x.com/karpathy/status/1608895190672211968
互动: Likes: 1,423; Retweets: 38; Replies: 42; Quotes: 10; Views: 154,220; Bookmarks: 34; isReply: 1

I was learning Rust yesterday so I disabled it briefly to complete some coding exercises and I felt a sense of dread realizing it was just the cursor and I, alone in the text editor 😬

我昨天在学习 Rust 编程语言，所以就暂时关掉了我的编程助手（或者某个辅助工具），想独立完成一些编程练习。但当我意识到文本编辑器里只剩下光标和我孤零零地对着时，心里突然涌上了一股莫名的不安 😬

### 679

作者: @karpathy
时间: 2022-12-30
链接: https://x.com/karpathy/status/1608895189078380544
互动: Likes: 4,169; Retweets: 520; Replies: 76; Quotes: 160; Views: 1,839,615; Bookmarks: 1,594; isReply: 0

Nice read on reverse engineering of GitHub Copilot 🪄. Copilot has dramatically accelerated my coding, it's hard to imagine going back to "manual coding". Still learning to use it but it already writes ~80% of my code, ~80% accuracy. I don't even really code, I prompt. &amp; edit.

读到了一篇关于 GitHub Copilot 逆向工程的精彩文章 🪄。GitHub Copilot 已经极大地加速了我的编程效率，现在很难想象再回到「手动写代码」的日子。虽然我还在学习如何使用它，但它已经能帮我生成大约 80% 的代码，而且准确率也高达约 80%。我甚至感觉自己不再是单纯地写代码，更多地是在给出提示（prompt）和进行编辑。

### 680

作者: @karpathy
时间: 2022-12-30
链接: https://x.com/karpathy/status/1608937034927964160
互动: Likes: 124; Retweets: 2; Replies: 2; Quotes: 0; Views: 26,701; Bookmarks: 5; isReply: 1

@vgoklani_api ty! i didn't tweet about it yet, still a bit too much work in progress

@vgoklani_api 谢谢！我还没有就此事发布推文，因为目前仍有大量工作正在进行，尚未完全完成。
