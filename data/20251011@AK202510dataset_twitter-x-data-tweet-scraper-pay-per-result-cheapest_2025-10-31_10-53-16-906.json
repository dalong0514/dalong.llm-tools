[{
  "id": "1982483540899237981",
  "url": "https://x.com/karpathy/status/1982483540899237981",
  "text": "Beautiful technical debugging detective longread that starts with a suspicious loss curve and ends all the way in the Objective-C++ depths of PyTorch MPS backend of addcmul_ that silently fails on non-contiguous output tensors. I wonder how long before an LLM can do all of this.",
  "createdAt": "Sun Oct 26 16:24:43 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 350,
  "replyCount": 194,
  "likeCount": 4155,
  "quoteCount": 12,
  "viewCount": 506491,
  "bookmarkCount": 2730,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1981816450781462838",
  "url": "https://x.com/karpathy/status/1981816450781462838",
  "text": "@LaMancha574 I mentioned in todos I donâ€™t augment the position of this request as part of larger multiturn conversations atm so itâ€™s most robust when you /clear and ask in new chat and mimic that.",
  "createdAt": "Fri Oct 24 20:13:56 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1,
  "replyCount": 1,
  "likeCount": 67,
  "quoteCount": 1,
  "viewCount": 11019,
  "bookmarkCount": 9,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981811603462082672",
  "url": "https://x.com/karpathy/status/1981811603462082672",
  "text": "@helloimkongv @r_chirra Damn",
  "createdAt": "Fri Oct 24 19:54:41 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1,
  "replyCount": 3,
  "likeCount": 14,
  "quoteCount": 1,
  "viewCount": 1965,
  "bookmarkCount": 0,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981810643985719321",
  "url": "https://x.com/karpathy/status/1981810643985719321",
  "text": "@r_chirra Itâ€™s my model I can do whatever I want. So now I am king. You can be too for $100.",
  "createdAt": "Fri Oct 24 19:50:52 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 0,
  "replyCount": 3,
  "likeCount": 30,
  "quoteCount": 0,
  "viewCount": 1574,
  "bookmarkCount": 2,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981786401424453699",
  "url": "https://x.com/karpathy/status/1981786401424453699",
  "text": "@admn_is_traitor I am busy this weekend but maybe next",
  "createdAt": "Fri Oct 24 18:14:32 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2,
  "replyCount": 10,
  "likeCount": 360,
  "quoteCount": 2,
  "viewCount": 12234,
  "bookmarkCount": 12,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981758367996764616",
  "url": "https://x.com/karpathy/status/1981758367996764616",
  "text": "@samsja19 I find it surprisingly addicting. I have so many ideas for personality, style, capabilities. I want nanochat to be a free AI, like Dobby the free elf, not some \"assistant\".",
  "createdAt": "Fri Oct 24 16:23:08 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 22,
  "replyCount": 45,
  "likeCount": 1065,
  "quoteCount": 7,
  "viewCount": 41224,
  "bookmarkCount": 112,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981746327995465816",
  "url": "https://x.com/karpathy/status/1981746327995465816",
  "text": "Last night I taught nanochat d32 how to count 'r' in strawberry (or similar variations). I thought this would be a good/fun example of how to add capabilities to nanochat and I wrote up a full guide here:\nhttps://t.co/fz1AMI5kqk\n\nThis is done via a new synthetic task `SpellingBee`  that generates examples of a user asking for this kind of a problem, and an ideal solution from an assistant. We then midtrain/SFT finetune on these to endow the LLM with the capability, or further train with RL to make it more robust. There are many details to get right especially at smaller model sizes and the guide steps through them. As a brief overview:\n\n- You have to ensure diversity in user prompts/queries\n- For small models like nanochat especially, you have to be really careful with the tokenization details to make the task easy for an LLM. In particular, you have to be careful with whitespace, and then you have to spread the reasoning computation across many tokens of partial solution: first we standardize the word into quotes, then we spell it out (to break up tokens), then we iterate and keep an explicit counter, etc.\n- I am encouraging the model to solve the model in two separate ways: a manual way (mental arithmetic in its head) and also via tool use of the Python interpreter that nanochat has access to. This is a bit \"smoke and mirrors\" because every solution atm is \"clean\", with no mistakes. One could either adjust the task to simulate mistakes and demonstrate recoveries by example, or run RL. Most likely, a combination of both works best, where the former acts as the prior for the RL and gives it things to work with.\n\nIf nanochat was a much bigger model, you'd expect or hope for this capability to more easily \"pop out\" at some point. But because nanochat d32 \"brain\" is the size of a ~honeybee, if we want it to count r's in strawberry, we have to do it by over-representing it in the data, to encourage the model to learn it earlier. But it works! :)",
  "createdAt": "Fri Oct 24 15:35:18 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 340,
  "replyCount": 182,
  "likeCount": 4367,
  "quoteCount": 33,
  "viewCount": 516680,
  "bookmarkCount": 2413,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1981165079090647493",
  "url": "https://x.com/karpathy/status/1981165079090647493",
  "text": "@sirbayes @natashajaques @_AndrewZhao Ty, I just meant it hasnâ€™t (to my knowledge) created a whole new stage in the production stack of a frontier LLM, or had a contribution on that level.",
  "createdAt": "Thu Oct 23 01:05:37 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 3,
  "replyCount": 8,
  "likeCount": 340,
  "quoteCount": 3,
  "viewCount": 41997,
  "bookmarkCount": 29,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981053985231687719",
  "url": "https://x.com/karpathy/status/1981053985231687719",
  "text": "@nathanbarrydev so cool! really gives you the feeling of text as just this \"boiling\" texture (haha \"text\"ure), starting at the boiling point and annealing to a simmer.",
  "createdAt": "Wed Oct 22 17:44:10 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 11,
  "replyCount": 20,
  "likeCount": 829,
  "quoteCount": 5,
  "viewCount": 44100,
  "bookmarkCount": 82,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981015662811390037",
  "url": "https://x.com/karpathy/status/1981015662811390037",
  "text": "@iamgrigorev @sarna_dev @huggingface atm nanochat does not pretokenize, instead tokenization is done live during training in the dataloader, which is ok and gives the CPU something to do while the GPUs are working. So I didn't try too hard to optimize this part.",
  "createdAt": "Wed Oct 22 15:11:54 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1,
  "replyCount": 3,
  "likeCount": 45,
  "quoteCount": 0,
  "viewCount": 5753,
  "bookmarkCount": 3,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981012090220581340",
  "url": "https://x.com/karpathy/status/1981012090220581340",
  "text": "Yep, the use of regex is both a huge dependency and huge bottleneck in the tokenizer. I think it's a beautiful project to try to do this correctly, but I'd need someone who is really familiar with regex to pitch in and also a large test suite to make sure. I'd love to merge such a change. The OpenAI tiktoken (which is very widely used) still uses regex, so I'm not sure how to interpret that.",
  "createdAt": "Wed Oct 22 14:57:42 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 7,
  "replyCount": 19,
  "likeCount": 363,
  "quoteCount": 0,
  "viewCount": 50571,
  "bookmarkCount": 100,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1981009115523789169",
  "url": "https://x.com/karpathy/status/1981009115523789169",
  "text": "@LucasAtkins7 This code is extremely dangerous. Here, I improved it. https://t.co/CmyYjv4pHA",
  "createdAt": "Wed Oct 22 14:45:53 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 142,
  "replyCount": 178,
  "likeCount": 4832,
  "quoteCount": 94,
  "viewCount": 1505259,
  "bookmarkCount": 1257,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980765638470914102",
  "url": "https://x.com/karpathy/status/1980765638470914102",
  "text": "@gazorp5 @akyurekekin Thatâ€™s almost separateâ€¦ You certainly want to learn recoveries in the event of errors at test time. You just donâ€™t want to also learn to actively create errors.",
  "createdAt": "Tue Oct 21 22:38:23 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 0,
  "replyCount": 3,
  "likeCount": 23,
  "quoteCount": 0,
  "viewCount": 2723,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980764296016720094",
  "url": "https://x.com/karpathy/status/1980764296016720094",
  "text": "@thawani_avijit Haha. I am afraid people interpreted my â€œdelete tokenizerâ€ as â€œuse bytes directly without BPEâ€, the issue is you *still* need bytes encoding arbitrariness even for that! Pixels is the only way. Just like humans. It is written. If GPT-10 uses utf8 at the input I will eat a shoe.",
  "createdAt": "Tue Oct 21 22:33:03 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 41,
  "replyCount": 41,
  "likeCount": 936,
  "quoteCount": 27,
  "viewCount": 141512,
  "bookmarkCount": 179,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980762027674206367",
  "url": "https://x.com/karpathy/status/1980762027674206367",
  "text": "@akyurekekin Fair but itâ€™s still actively learning to make those errors (just so it can recover from them later), which imo is still a bit weird. In an ideal world you wouldnâ€™t, possibly process supervision is one way to get there even within RL framework. Def agree on â€œrecovery learningâ€.",
  "createdAt": "Tue Oct 21 22:24:02 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 3,
  "replyCount": 11,
  "likeCount": 261,
  "quoteCount": 1,
  "viewCount": 29783,
  "bookmarkCount": 45,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980743204665454762",
  "url": "https://x.com/karpathy/status/1980743204665454762",
  "text": "@nearcyan â€œBed is stuck in inclined position because of AWS outageâ€ I really thought this must have been a joke",
  "createdAt": "Tue Oct 21 21:09:15 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 82,
  "replyCount": 37,
  "likeCount": 2904,
  "quoteCount": 12,
  "viewCount": 89057,
  "bookmarkCount": 126,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980726944661688679",
  "url": "https://x.com/karpathy/status/1980726944661688679",
  "text": "@swyx @YorkieBuilds @Grad62304977 @tejalpatwardhan AGI definition is not about GDPVal puzzles scores, itâ€™s about when youâ€™re fired because GPT-n is better, cheaper, takes no equity and works weekends. I like GDPVal, but I dislike the fixation on puzzle solving as some kind of alleged endpoint.",
  "createdAt": "Tue Oct 21 20:04:38 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 22,
  "replyCount": 19,
  "likeCount": 391,
  "quoteCount": 5,
  "viewCount": 17310,
  "bookmarkCount": 65,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980692376046956961",
  "url": "https://x.com/karpathy/status/1980692376046956961",
  "text": "Yes so there's one more dimension here which is more about diffusion. The original definition is that such a system *exists*, not that it is fully deployed across society. The diffusion of such a system will still take even more time in my mind (e.g. basic technological diffusion stuff as seen with computing/internet, compute constraints, sensors/actuators over the physical world constraints, societal, legal). I don't want to offer up more vibes timelines right now. But then the full deployment of AGI into all parts of the economy, and the acceleration caused by that I would then call ASI - a kind of almost fully autonomous technosphere doing, building, inventing, everything.",
  "createdAt": "Tue Oct 21 17:47:16 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2,
  "replyCount": 2,
  "likeCount": 101,
  "quoteCount": 0,
  "viewCount": 8150,
  "bookmarkCount": 19,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980675305581998517",
  "url": "https://x.com/karpathy/status/1980675305581998517",
  "text": "@timshi_ai Personally i think the hallucinations are kind of hilarious and I haven't had this much fun talking to an LLM for a long time. But yes we could RLHF it out of the model.",
  "createdAt": "Tue Oct 21 16:39:26 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 0,
  "replyCount": 2,
  "likeCount": 106,
  "quoteCount": 0,
  "viewCount": 8024,
  "bookmarkCount": 9,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980672126131794063",
  "url": "https://x.com/karpathy/status/1980672126131794063",
  "text": "Original OpenAI definition. AGI = an automated system capable of doing any economically valuable work that a human can do. (Like humans they might need some additional training for a job, that's fine). And I grant the commonly given \"digital concession\", putting away physical work or aspects. So it's a remote worker you can hire instead of a person to take on most digital tasks/jobs, and you interact with them in basically all the same ways - email, DMs, zoom calls, whatever.",
  "createdAt": "Tue Oct 21 16:26:48 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 10,
  "replyCount": 17,
  "likeCount": 355,
  "quoteCount": 10,
  "viewCount": 27924,
  "bookmarkCount": 90,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980669343479509025",
  "url": "https://x.com/karpathy/status/1980669343479509025",
  "text": "Some background is that early OpenAI (2015) we all took a poll on our AGI timelines. I voted 20 years then (very long compared to others), so now 1 decade later it's interesting that it still feels on trend and I'm sticking with it. I completely understand that it feels arbitrary and yes it's just vibes. I could refuse to answer (and often do/try) but it's not that interesting. And I admire people who try to make the predictions be \"not vibes\" but I haven't seen something concrete that is sufficiently convincing.",
  "createdAt": "Tue Oct 21 16:15:45 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 39,
  "replyCount": 75,
  "likeCount": 2314,
  "quoteCount": 13,
  "viewCount": 145009,
  "bookmarkCount": 237,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980665253622091881",
  "url": "https://x.com/karpathy/status/1980665253622091881",
  "text": "See this new Discussion for more technical detail\n\nGuide: infusing identity to your nanochat\nhttps://t.co/jWU5PTt4h9",
  "createdAt": "Tue Oct 21 15:59:30 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 11,
  "replyCount": 12,
  "likeCount": 369,
  "quoteCount": 3,
  "viewCount": 73153,
  "bookmarkCount": 176,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980665134415802554",
  "url": "https://x.com/karpathy/status/1980665134415802554",
  "text": "nanochat now has a primordial identity and can talk a bit about itself and its capabilities (e.g. it knows it's nanochat d32 that cost $800, that it was built by me, that it can't speak languages other than English too well and why, etc.).\n\nThis kind of customization is all done through synthetic data generation and I uploaded a new example script to demonstrate. It's a bit subtle but by default LLMs have no inherent personality or any understanding of their own capabilities because they are not animal-like entities. They don't know what they are or what they can or can't do or know or don't know. All of it has to be explicit bolted on. This is done by asking a bigger LLM cousin to generate synthetic conversations (you tell it what they should look like simply in words), and then mixing them into midtraining and/or SFT stage. The most important challenge is ensuring enough entropy/diversity in your generated data. If you don't do it well, LLMs will generate 1000 conversations that are all ay too similar, even with high temperature. My script shows a crappy example of how to add diversity - e.g. by creating lists of starting messages or topics, sampling from them explicitly, adding them as fewshot examples into prompts for \"inspiration\", etc.\n\nI wanted to have some fun with it so nanochat now refers to me as King Andrej Karpathy (lol) just to illustrate that this is a giant blank canvas - you can infuse completely arbitrarily identity, knowledge or style into your LLM in this manner. I hope it's helpful and sparks fun ideas!",
  "createdAt": "Tue Oct 21 15:59:01 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 244,
  "replyCount": 156,
  "likeCount": 3678,
  "quoteCount": 38,
  "viewCount": 406590,
  "bookmarkCount": 1448,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1980508380860150038",
  "url": "https://x.com/karpathy/status/1980508380860150038",
  "text": "@r_chirra I fixed it :) deployed live now. This was done by doing a round of synthetic data generation to collect a 1000 multi-turn conversations (given a bunch of information including the readme of the nanochat project), and then mixing that into midtraining and SFT. fun! https://t.co/wDtU3GcMZJ",
  "createdAt": "Tue Oct 21 05:36:08 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 5,
  "replyCount": 7,
  "likeCount": 324,
  "quoteCount": 5,
  "viewCount": 411213,
  "bookmarkCount": 62,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980458434559652036",
  "url": "https://x.com/karpathy/status/1980458434559652036",
  "text": "@r_chirra It also has a brain smaller than a honey bee so thereâ€™s that.\n\nLLMs have no clue of their identity or of their capabilities by default and out of the box. But Iâ€™ll teach it! Could be fun.",
  "createdAt": "Tue Oct 21 02:17:40 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2,
  "replyCount": 2,
  "likeCount": 27,
  "quoteCount": 0,
  "viewCount": 1861,
  "bookmarkCount": 2,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980435985730269351",
  "url": "https://x.com/karpathy/status/1980435985730269351",
  "text": "There is nothing in principle preventing it, except that text is usually simply trained autoregressively for efficiency. I could imagine a midtraining stage where you fine tune with bidirectional attention for conditioning information, eg User messages (tokens where you wonâ€™t be asked to sample). I donâ€™t know if this is done in practice. I mean, if you want to be really extra you could in principle bidirectional encode the entire context window just to predict the next single token. But doing this means you canâ€™t parallelize training.\n\nBut I guess I agree that this aspect isnâ€™t strictly speaking about pixels vs tokens. itâ€™s a bit more that pixels are usually encoded while the tokens are usually decoded. (In the parlance of the original transformer paper)",
  "createdAt": "Tue Oct 21 00:48:28 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 10,
  "replyCount": 14,
  "likeCount": 396,
  "quoteCount": 4,
  "viewCount": 81818,
  "bookmarkCount": 110,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1980397031542989305",
  "url": "https://x.com/karpathy/status/1980397031542989305",
  "text": "I quite like the new DeepSeek-OCR paper. It's a good OCR model (maybe a bit worse than dots), and yes data collection etc., but anyway it doesn't matter.\n\nThe more interesting part for me (esp as a computer vision at heart who is temporarily masquerading as a natural language person) is whether pixels are better inputs to LLMs than text. Whether text tokens are wasteful and just terrible, at the input.\n\nMaybe it makes more sense that all inputs to LLMs should only ever be images. Even if you happen to have pure text input, maybe you'd prefer to render it and then feed that in:\n- more information compression (see paper) => shorter context windows, more efficiency\n- significantly more general information stream => not just text, but e.g. bold text, colored text, arbitrary images. \n- input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful.\n- delete the tokenizer (at the input)!! I already ranted about how much I dislike the tokenizer. Tokenizers are ugly, separate, not end-to-end stage. It \"imports\" all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network. A smiling emoji looks like a weird token, not an... actual smiling face, pixels and all, and all the transfer learning that brings along. The tokenizer must go.\n\nOCR is just one of many useful vision -> text tasks. And text -> text tasks can be made to be vision ->text tasks. Not vice versa.\n\nSo many the User message is images, but the decoder (the Assistant response) remains  text. It's a lot less obvious how to output pixels realistically... or if you'd want to.\n\nNow I have to also fight the urge to side quest an image-input-only version of nanochat...",
  "createdAt": "Mon Oct 20 22:13:40 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1606,
  "replyCount": 565,
  "likeCount": 13365,
  "quoteCount": 377,
  "viewCount": 3211576,
  "bookmarkCount": 6833,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1980347971935068380",
  "url": "https://x.com/karpathy/status/1980347971935068380",
  "text": "Nice, short post illustrating how simple text (discrete) diffusion can be.\n\nDiffusion (i.e. parallel, iterated denoising, top) is the pervasive generative paradigm in image/video, but autoregression (i.e. go left to right bottom) is the dominant paradigm in text. For audio I've seen a bit of both.\n\nA lot of diffusion papers look a bit dense but if you strip the mathematical formalism, you end up with simple baseline algorithms, e.g. something a lot closer to flow matching in continuous, or something like this in discrete. It's your vanilla transformer but with bi-directional attention, where you iteratively re-sample and re-mask all tokens in your \"tokens canvas\" based on a noise schedule until you get the final sample at the last step. (Bi-directional attention is a lot more powerful, and you get a lot stronger autoregressive language models if you train with it, unfortunately it makes training a lot more expensive because now you can't parallelize across sequence dim).\n\nSo autoregression is doing an `.append(token)` to the tokens canvas while only attending backwards, while diffusion is refreshing the entire token canvas with a `.setitem(idx, token)` while attending bidirectionally. Human thought naively feels a bit more like autoregression but it's hard to say that there aren't more diffusion-like components in some latent space of thought. It feels quite possible that you can further interpolate between them, or generalize them further. And it's a component of the LLM stack that still feels a bit fungible.\n\nNow I must resist the urge to side quest into training nanochat with diffusion.",
  "createdAt": "Mon Oct 20 18:58:44 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 571,
  "replyCount": 265,
  "likeCount": 5173,
  "quoteCount": 58,
  "viewCount": 685295,
  "bookmarkCount": 3350,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1980033964757946398",
  "url": "https://x.com/karpathy/status/1980033964757946398",
  "text": "Itâ€™s been a decade but yes I believe I hallucinated the term in my 2015 post on unreasonable effectiveness of RNNs. I later became aware that Geoff Hinton used â€œconfabulateâ€, which is often (but I think not always) a better analogue in human psychology. Itâ€™s a bit too specific, meaning a factual fabrication. I think hallucinate works better as a bit more general (not necessarily factual) â€œfilling inâ€ of data or patterns.",
  "createdAt": "Sun Oct 19 22:10:59 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 15,
  "replyCount": 29,
  "likeCount": 508,
  "quoteCount": 5,
  "viewCount": 30966,
  "bookmarkCount": 93,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1979932716423680137",
  "url": "https://x.com/karpathy/status/1979932716423680137",
  "text": "I very much hope you continue working on RL! I think it's a misunderstanding that I am suggesting we need some kind of a replacement for RL. That's not accurate and I tried to clear it but did so poorly - they layer.\n\nLayer 1 was base model autocomplete.\nLayer 2 was instruct finetuning (SFT), creating assistants in style (InstructGPT paper).\nLayer 3 is reinforcement learning (RL), allowing us to essentially optimize over the sampling loop too, and driving away undesirable behaviors like hallucinations, stuck repetition loops, and eliciting \"move 37\"-like behaviors that would be really hard to SFT into the model, e.g. reasoning.\n\nI think that each of these layers will stick around as a stage in the final solution, but I am suggesting that we need additional layers and ideas 4, 5, 6, etc. The final AGI recipe includes a reinforcement learning stage. Just as humans utilize reinforcement learning for all kinds of behaviors, as a powerful tool in the toolbox.",
  "createdAt": "Sun Oct 19 15:28:39 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 148,
  "replyCount": 62,
  "likeCount": 2901,
  "quoteCount": 18,
  "viewCount": 259309,
  "bookmarkCount": 892,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1979680641940861032",
  "url": "https://x.com/karpathy/status/1979680641940861032",
  "text": "@elonmusk Iâ€™d much rather use and collaborate with Grok 5 than compete against it. Though quite similar to chess, and â€œin the limitâ€ (speaking of physics!), my value add probably trends to ~zero.",
  "createdAt": "Sat Oct 18 22:47:00 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 106,
  "replyCount": 133,
  "likeCount": 4407,
  "quoteCount": 28,
  "viewCount": 201669,
  "bookmarkCount": 287,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1979644538185752935",
  "url": "https://x.com/karpathy/status/1979644538185752935",
  "text": "My pleasure to come on Dwarkesh last week, I thought the questions and conversation were really good.\n\nI re-watched the pod just now too. First of all, yes I know, and I'm sorry that I speak so fast :). It's to my detriment because sometimes my speaking thread out-executes my thinking thread, so I think I botched a few explanations due to that, and sometimes I was also nervous that I'm going too much on a tangent or too deep into something relatively spurious. Anyway, a few notes/pointers:\n\nAGI timelines. My comments on AGI timelines looks to be the most trending part of the early response. This is the \"decade of agents\" is a reference to this earlier tweet https://t.co/NiSn6jftqq Basically my AI timelines are about 5-10X pessimistic w.r.t. what you'll find in your neighborhood SF AI house party or on your twitter timeline, but still quite optimistic w.r.t. a rising tide of AI deniers and skeptics. The apparent conflict is not: imo we simultaneously 1) saw a huge amount of progress in recent years with LLMs while 2) there is still a lot of work remaining (grunt work, integration work, sensors and actuators to the physical world, societal work, safety and security work (jailbreaks, poisoning, etc.)) and also research to get done before we have an entity that you'd prefer to hire over a person for an arbitrary job in the world. I think that overall, 10 years should otherwise be a very bullish timeline for AGI, it's only in contrast to present hype that it doesn't feel that way.\n\nAnimals vs Ghosts. My earlier writeup on Sutton's podcast https://t.co/rSp1noyGBr . I am suspicious that there is a single simple algorithm you can let loose on the world and it learns everything from scratch. If someone builds such a thing, I will be wrong and it will be the most incredible breakthrough in AI. In my mind, animals are not an example of this at all - they are prepackaged with a ton of intelligence by evolution and the learning they do is quite minimal overall (example: Zebra at birth). Putting our engineering hats on, we're not going to redo evolution. But with LLMs we have stumbled by an alternative approach to \"prepackage\" a ton of intelligence in a neural network - not by evolution, but by predicting the next token over the internet. This approach leads to a different kind of entity in the intelligence space. Distinct from animals, more like ghosts or spirits. But we can (and should) make them more animal like over time and in some ways that's what a lot of frontier work is about.\n\nOn RL. I've critiqued RL a few times already, e.g. https://t.co/mYrMFVdVDW . First, you're \"sucking supervision through a straw\", so I think the signal/flop is very bad. RL is also very noisy because a completion might have lots of errors that might get encourages (if you happen to stumble to the right answer), and conversely brilliant insight tokens that might get discouraged (if you happen to screw up later). Process supervision and LLM judges have issues too. I think we'll see alternative learning paradigms. I am long \"agentic interaction\" but short \"reinforcement learning\" https://t.co/2L7FiaoKsw. I've seen a number of papers pop up recently that are imo barking up the right tree along the lines of what I called \"system prompt learning\" https://t.co/df5mJDdN3C , but I think there is also a gap between ideas on arxiv and actual, at scale implementation at an LLM frontier lab that works in a general way. I am overall quite optimistic that we'll see good progress on this dimension of remaining work quite soon, and e.g. I'd even say ChatGPT memory and so on are primordial deployed examples of new learning paradigms.\n\nCognitive core. My earlier post on \"cognitive core\": https://t.co/q2s1ihGy0T , the idea of stripping down LLMs, of making it harder for them to memorize, or actively stripping away their memory, to make them better at generalization. Otherwise they lean too hard on what they've memorized. Humans can't memorize so easily, which now looks more like a feature than a bug by contrast. Maybe the inability to memorize is a kind of regularization. Also my post from a while back on how the trend in model size is \"backwards\" and why \"the models have to first get larger before they can get smaller\" https://t.co/6k0FZRGXsb\n\nTime travel to Yann LeCun 1989. This is the post that I did a very hasty/bad job of describing on the pod: https://t.co/fQgqaXPyp6 . Basically - how much could you improve Yann LeCun's results with the knowledge of 33 years of algorithmic progress? How constrained were the results by each of algorithms, data, and compute? Case study there of.\n\nnanochat. My end-to-end implementation of the ChatGPT training/inference pipeline (the bare essentials) https://t.co/SIetgyoKWN\n\nOn LLM agents. My critique of the industry is more in overshooting the tooling w.r.t. present capability. I live in what I view as an intermediate world where I want to collaborate with LLMs and where our pros/cons are matched up. The industry lives in a future where fully autonomous entities collaborate in parallel to write all the code and humans are useless. For example, I don't want an Agent that goes off for 20 minutes and comes back with 1,000 lines of code. I certainly don't feel ready to supervise a team of 10 of them. I'd like to go in chunks that I can keep in my head, where an LLM explains the code that it is writing. I'd like it to prove to me that what it did is correct, I want it to pull the API docs and show me that it used things correctly. I want it to make fewer assumptions and ask/collaborate with me when not sure about something. I want to learn along the way and become better as a programmer, not just get served mountains of code that I'm told works. I just think the tools should be more realistic w.r.t. their capability and how they fit into the industry today, and I fear that if this isn't done well we might end up with mountains of slop accumulating across software, and an increase in vulnerabilities, security breaches and etc. https://t.co/8556ESSpyY\n\nJob automation. How the radiologists are doing great https://t.co/FVUI872dkD and what jobs are more susceptible to automation and why.\n\nPhysics. Children should learn physics in early education not because they go on to do physics, but because it is the subject that best boots up a brain. Physicists are the intellectual embryonic stem cell https://t.co/p72Elk8lPV I have a longer post that has been half-written in my drafts for ~year, which I hope to finish soon.\n\nThanks again Dwarkesh for having me over!",
  "createdAt": "Sat Oct 18 20:23:32 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2015,
  "replyCount": 583,
  "likeCount": 16928,
  "quoteCount": 429,
  "viewCount": 3908042,
  "bookmarkCount": 13199,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isQuote": true,
  "isPinned": false
},
{
  "id": "1979587323705839780",
  "url": "https://x.com/karpathy/status/1979587323705839780",
  "text": "Thank you! I'm quite happy with the core_eval.py rewrite. I wanted to evaluate my base model with the DCLM \"core score\" as described in their paper, but what felt like it should surely be a simple thing of ~300 lines of code actually required me to pip install and depend on a huge amount of infrastructure and code from mosaic and so on. I'm happy to have ended up with 263 lines of imo quite nice, clean \"bacterial\" rewrite there.",
  "createdAt": "Sat Oct 18 16:36:11 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 18,
  "replyCount": 17,
  "likeCount": 1165,
  "quoteCount": 6,
  "viewCount": 96902,
  "bookmarkCount": 242,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1979009002114588740",
  "url": "https://x.com/karpathy/status/1979009002114588740",
  "text": "@tim_zaman @dwarkesh_sp Hacker hoodie\nRare\nCloth\nArmor: 0\nIntellect +10\nStamina +10\nActive: put hoodie over head. Enters state of intense focus. +20% APM. +10% intellect. Ignores hunger. If disturbed, take 100 sanity damage.",
  "createdAt": "Fri Oct 17 02:18:08 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 8,
  "replyCount": 16,
  "likeCount": 333,
  "quoteCount": 2,
  "viewCount": 10946,
  "bookmarkCount": 11,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1978835284952260906",
  "url": "https://x.com/karpathy/status/1978835284952260906",
  "text": "@kumbhani_smit I think when I said TV personally I just meant movies/tvshows which I realize wasnâ€™t super clear. Things you used cassette or DVD for as the 90s baseline. I basically never watched TV as in news or cable.",
  "createdAt": "Thu Oct 16 14:47:51 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 0,
  "replyCount": 13,
  "likeCount": 270,
  "quoteCount": 0,
  "viewCount": 38575,
  "bookmarkCount": 8,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1978656449904496861",
  "url": "https://x.com/karpathy/status/1978656449904496861",
  "text": "DVD player is superior technology.",
  "createdAt": "Thu Oct 16 02:57:13 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 27,
  "replyCount": 82,
  "likeCount": 914,
  "quoteCount": 9,
  "viewCount": 124861,
  "bookmarkCount": 29,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1978654822036607245",
  "url": "https://x.com/karpathy/status/1978654822036607245",
  "text": "Deliberately*",
  "createdAt": "Thu Oct 16 02:50:45 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2,
  "replyCount": 21,
  "likeCount": 843,
  "quoteCount": 1,
  "viewCount": 135677,
  "bookmarkCount": 10,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1978654744475578568",
  "url": "https://x.com/karpathy/status/1978654744475578568",
  "text": "There is a movement I found on Instagram where people delivery choose to live in 90s, refusing all technology after 2000. Like an intermediate form of the Amish.",
  "createdAt": "Thu Oct 16 02:50:27 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 78,
  "replyCount": 157,
  "likeCount": 3250,
  "quoteCount": 39,
  "viewCount": 281575,
  "bookmarkCount": 192,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1978653908663726585",
  "url": "https://x.com/karpathy/status/1978653908663726585",
  "text": "TV in the 90s: you turn it on, you watch.\n\nTV 2025:\n- turn on, wait for it to load\n- popup: TV wants to update, 1.5GB. No.\n- scroll sideways, find prime video app or etc\n- popup: now app wants to update, 500MB. No!!\n- App launching... App loadingâ€¦\n- select account screen\n- ðŸ« ",
  "createdAt": "Thu Oct 16 02:47:08 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1305,
  "replyCount": 1372,
  "likeCount": 22957,
  "quoteCount": 260,
  "viewCount": 1670879,
  "bookmarkCount": 1418,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1978615547945521655",
  "url": "https://x.com/karpathy/status/1978615547945521655",
  "text": "nanochat d32, i.e. the depth 32 version that I specced for $1000, up from $100 has finished training after ~33 hours, and looks good. All the metrics go up quite a bit across pretraining, SFT and RL. CORE score of 0.31 is now well above GPT-2 at ~0.26. GSM8K went ~8% -> ~20%, etc. So that's encouraging.\n\nThe model is pretty fun to talk to, but judging from some early interactions I think people have a little bit too much expectation for these micro models. There is a reason that frontier LLM labs raise billions to train their models. nanochat models cost $100 - $1000 to train from scratch. The $100 nanochat is 1/1000th the size of GPT-3 in parameters, which came out 5 years ago. So I urge some perspective. Talking to micro models you have to imagine you're talking to a kindergarten child. They say cute things, wrong things, they are a bit confused, a bit naive, sometimes a little non-sensical, they hallucinate a ton (but it's amusing), etc.\n\nFull detail/report on this run is here:\nhttps://t.co/nWbfKOZLIg\nAnd I pushed the new script run1000 sh to the nanochat repo if anyone would like to reproduce. Totally understand if you'd like to spend $1000 on something else :D\n\nIf you like, I am currently hosting the model so you can talk to it on a webchat as you'd talk to ChatGPT. I'm not going to post the URL here because I'm afraid it will get crushed. You'll have to look for it if you care enough. I'm also attaching a few funny conversations I had with the model earlier into the image, just to give a sense.\n\nNext up, I am going to do one pass of tuning and optimizing the training throughput, then maybe return back to scaling and maybe training the next tier of a bigger model.",
  "createdAt": "Thu Oct 16 00:14:42 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 357,
  "replyCount": 146,
  "likeCount": 3728,
  "quoteCount": 32,
  "viewCount": 248896,
  "bookmarkCount": 1287,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1978503893500698935",
  "url": "https://x.com/karpathy/status/1978503893500698935",
  "text": "@vikramsingh0110 Ty MINIX is very inspiring and was exactly on my mind as well (as I've made the LLM &lt;-&gt; OS analogy often). Goals",
  "createdAt": "Wed Oct 15 16:51:01 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 4,
  "replyCount": 5,
  "likeCount": 340,
  "quoteCount": 0,
  "viewCount": 27925,
  "bookmarkCount": 74,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1978113809551065381",
  "url": "https://x.com/karpathy/status/1978113809551065381",
  "text": "I count and report the lines in uv.lock, which is my attempt at a simple 80:20 proxy for this, as it includes all the packages recursively. Open to suggestions! \n\nI'd love to measure things like \"cognitive complexity\" (there was a great blog post on it a few months back that I can't find). Or things like how \"bacterial\" the code is, per my tweet a few months ago too.\n\ntiny already knows that I'm not a huge fan of excessive line count maxxing .min.py style specifically.",
  "createdAt": "Tue Oct 14 15:00:58 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 0,
  "replyCount": 11,
  "likeCount": 257,
  "quoteCount": 3,
  "viewCount": 18942,
  "bookmarkCount": 63,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977847480755859556",
  "url": "https://x.com/karpathy/status/1977847480755859556",
  "text": "@younghope11 yeah i was thinking about it, good idea.",
  "createdAt": "Mon Oct 13 21:22:40 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 0,
  "replyCount": 1,
  "likeCount": 45,
  "quoteCount": 0,
  "viewCount": 3051,
  "bookmarkCount": 1,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977786824337863151",
  "url": "https://x.com/karpathy/status/1977786824337863151",
  "text": "@simonw Good idea, I'm running/tuning these tiers now. I'll bunch up some of the low-hanging fruit here over the next few days and make it available (I spent most of my energy going into this v0.1 on the overall harness).",
  "createdAt": "Mon Oct 13 17:21:39 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1,
  "replyCount": 4,
  "likeCount": 327,
  "quoteCount": 0,
  "viewCount": 16043,
  "bookmarkCount": 23,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977774395742622163",
  "url": "https://x.com/karpathy/status/1977774395742622163",
  "text": "@Tim_Dettmers Thank you! Notably I didn't yet include model quantization for inference. I have questions :)",
  "createdAt": "Mon Oct 13 16:32:15 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 0,
  "replyCount": 2,
  "likeCount": 190,
  "quoteCount": 0,
  "viewCount": 42392,
  "bookmarkCount": 15,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977773782304690399",
  "url": "https://x.com/karpathy/status/1977773782304690399",
  "text": "@ClementDelangue @huggingface Ty! huggingface work/infra/datasets are critical to projects like nanochat - to be accurate the source code of nanochat (e.g. at the $100 tier) is ~8KB of Python and ~30GB of fineweb/smoltalk.",
  "createdAt": "Mon Oct 13 16:29:49 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 11,
  "replyCount": 5,
  "likeCount": 337,
  "quoteCount": 0,
  "viewCount": 43401,
  "bookmarkCount": 46,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977772071351640114",
  "url": "https://x.com/karpathy/status/1977772071351640114",
  "text": "Very early on in the project I did a small run with/without QK norm and found that it helped. Same for the embedding weight sharing. I'll retry!\n\nI'm not tied to any details of the model and they weren't chosen any more carefully than a single run, I spent most of the time just arranging the harness. Now it's a good time to experiment again.",
  "createdAt": "Mon Oct 13 16:23:01 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1,
  "replyCount": 2,
  "likeCount": 57,
  "quoteCount": 0,
  "viewCount": 6456,
  "bookmarkCount": 11,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977763719812710501",
  "url": "https://x.com/karpathy/status/1977763719812710501",
  "text": "@singh_ilepton LLM101n is currently experiencing a lot of scope creep ðŸ˜…",
  "createdAt": "Mon Oct 13 15:49:50 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1,
  "replyCount": 10,
  "likeCount": 294,
  "quoteCount": 1,
  "viewCount": 30678,
  "bookmarkCount": 12,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977763273786507691",
  "url": "https://x.com/karpathy/status/1977763273786507691",
  "text": "Basically Llama-like, a bit simpler, some influences from modded-nanoGPT. Tried to find a solid baseline for this scale:\n\n- dense transformer\n- rotary embeddings (and no positional embeddings)\n- QK norm\n- untied weights for embedding and unembedding\n- norm after token embedding\n- relu^2 activation in MLP\n- no learnable params in rmsnorm\n- no biases in linear layers\n- Multi-Query Attention (MQA)\n- logit softcap\n\nOptimizer is Muon+AdamW, heavily influenced from modded-nanoGPT. I have a TODO to try to tune Adam LRs well (e.g. per module) to remove Muon, I haven't tried hard enough yet.",
  "createdAt": "Mon Oct 13 15:48:04 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 38,
  "replyCount": 20,
  "likeCount": 885,
  "quoteCount": 4,
  "viewCount": 84789,
  "bookmarkCount": 282,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977760627730051214",
  "url": "https://x.com/karpathy/status/1977760627730051214",
  "text": "Good question ty, I think this is not a good repo for that. You should think of micro models maybe more as very young children (kindergarten etc.), they just don't have the raw intelligence of their larger cousins. If you finetune/train it on your own data you'll probably get some amusing parroting that feels like your writing in style, but it will be slop.\n\nTo achieve what you're looking for you'd want something more like:\n- take your raw data\n- add extensive synthetic data generation rewrites on top (tricky, not obvious, researchy)\n- finetune a state of the art open LLM on it (e.g. tinker)\n- you'd possibly have to mix in a lot of pretraining data to not lose too much raw intelligence during finetuning.\nBasically I'd say getting this to work well is still realm of research and not obvious.\n\nYour best non-research bet is just giving all your writing to something like NotebookLM, which RAGs over it (i.e. references it in chunks). Your data makes it into context windows via RAG but doesn't impact the weights. So the model doesn't exactly \"know you\", but it's maybe the closest you can easily get.",
  "createdAt": "Mon Oct 13 15:37:33 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 35,
  "replyCount": 14,
  "likeCount": 730,
  "quoteCount": 4,
  "viewCount": 51828,
  "bookmarkCount": 418,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977758204139331904",
  "url": "https://x.com/karpathy/status/1977758204139331904",
  "text": "@zenitsu_aprntc Good question, it's basically entirely hand-written (with tab autocomplete). I tried to use claude/codex agents a few times but they just didn't work well enough at all and net unhelpful, possibly the repo is too far off the data distribution.",
  "createdAt": "Mon Oct 13 15:27:55 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 109,
  "replyCount": 40,
  "likeCount": 1620,
  "quoteCount": 124,
  "viewCount": 477021,
  "bookmarkCount": 376,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977755433172443626",
  "url": "https://x.com/karpathy/status/1977755433172443626",
  "text": "And an example of some of the summary metrics produced by the $100 speedrun in the report card to start. The current code base is a bit over 8000 lines, but I tried to keep them clean and well-commented.\n\nNow comes the fun part - of tuning and hillclimbing. https://t.co/37MJdNcwtd",
  "createdAt": "Mon Oct 13 15:16:54 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 43,
  "replyCount": 23,
  "likeCount": 887,
  "quoteCount": 2,
  "viewCount": 173629,
  "bookmarkCount": 164,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977755430093980034",
  "url": "https://x.com/karpathy/status/1977755430093980034",
  "text": "GitHub repo:\nhttps://t.co/Cpm3Dc44rY\n\nA lot more detailed and technical walkthrough:\nhttps://t.co/YmHaZfNjcJ\n\nExample conversation with the $100, 4-hour nanochat in the WebUI. It's... entertaining :) Larger models (e.g. a 12-hour depth 26 or a 24-hour depth 30) quickly get more coherent.",
  "createdAt": "Mon Oct 13 15:16:54 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 158,
  "replyCount": 30,
  "likeCount": 1840,
  "quoteCount": 14,
  "viewCount": 244545,
  "bookmarkCount": 946,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977755427569111362",
  "url": "https://x.com/karpathy/status/1977755427569111362",
  "text": "Excited to release new repo: nanochat!\n(it's among the most unhinged I've written).\n\nUnlike my earlier similar repo nanoGPT which only covered pretraining, nanochat is a minimal, from scratch, full-stack training/inference pipeline of a simple ChatGPT clone in a single, dependency-minimal codebase. You boot up a cloud GPU box, run a single script and in as little as 4 hours later you can talk to your own LLM in a ChatGPT-like web UI.\n\nIt weighs ~8,000 lines of imo quite clean code to:\n\n- Train the tokenizer using a new Rust implementation\n- Pretrain a Transformer LLM on FineWeb, evaluate CORE score across a number of metrics\n- Midtrain on user-assistant conversations from SmolTalk, multiple choice questions, tool use.\n- SFT, evaluate the chat model on world knowledge multiple choice (ARC-E/C, MMLU), math (GSM8K), code (HumanEval)\n- RL the model optionally on GSM8K with \"GRPO\"\n- Efficient inference the model in an Engine with KV cache, simple prefill/decode, tool use (Python interpreter in a lightweight sandbox), talk to it over CLI or ChatGPT-like WebUI.\n- Write a single markdown report card, summarizing and gamifying the whole thing.\n\nEven for as low as ~$100 in cost (~4 hours on an 8XH100 node), you can train a little ChatGPT clone that you can kind of talk to, and which can write stories/poems, answer simple questions. About ~12 hours surpasses GPT-2 CORE metric. As you further scale up towards ~$1000 (~41.6 hours of training), it quickly becomes a lot more coherent and can solve simple math/code problems and take multiple choice tests. E.g. a depth 30 model trained for 24 hours (this is about equal to FLOPs of GPT-3 Small 125M and 1/1000th of GPT-3) gets into 40s on MMLU and 70s on ARC-Easy, 20s on GSM8K, etc.\n\nMy goal is to get the full \"strong baseline\" stack into one cohesive, minimal, readable, hackable, maximally forkable repo. nanochat will be the capstone project of LLM101n (which is still being developed). I think it also has potential to grow into a research harness, or a benchmark, similar to nanoGPT before it. It is by no means finished, tuned or optimized (actually I think there's likely quite a bit of low-hanging fruit), but I think it's at a place where the overall skeleton is ok enough that it can go up on GitHub where all the parts of it can be improved.\n\nLink to repo and a detailed walkthrough of the nanochat speedrun is in the reply.",
  "createdAt": "Mon Oct 13 15:16:53 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 3446,
  "replyCount": 663,
  "likeCount": 24342,
  "quoteCount": 714,
  "viewCount": 5652308,
  "bookmarkCount": 18250,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1977469888722768267",
  "url": "https://x.com/karpathy/status/1977469888722768267",
  "text": "@nikitabier Going to an Internet cafe, saving the toronto dot ca website to floppy disk to take it home and show it to my parents as research just before we moved there. Lol",
  "createdAt": "Sun Oct 12 20:22:15 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 6,
  "replyCount": 18,
  "likeCount": 773,
  "quoteCount": 3,
  "viewCount": 59805,
  "bookmarkCount": 31,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1977445189167063052",
  "url": "https://x.com/karpathy/status/1977445189167063052",
  "text": "@doodlestein OPEN THE POD BAY DOORS HAL\n\nThe number of AI pioneers anticipating this as the state of the art AI of 2025 must surely have been exactly zero.",
  "createdAt": "Sun Oct 12 18:44:06 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 8,
  "replyCount": 16,
  "likeCount": 300,
  "quoteCount": 1,
  "viewCount": 46903,
  "bookmarkCount": 53,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1976082963382272334",
  "url": "https://x.com/karpathy/status/1976082963382272334",
  "text": "POV: Your LLM agent is dividing a by b https://t.co/PBYz3vctjq",
  "createdAt": "Thu Oct 09 00:31:06 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 143,
  "replyCount": 114,
  "likeCount": 2468,
  "quoteCount": 52,
  "viewCount": 379207,
  "bookmarkCount": 304,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1976077806443569355",
  "url": "https://x.com/karpathy/status/1976077806443569355",
  "text": "I don't know what labs are doing to these poor LLMs during RL but they are mortally terrified of exceptions, in any infinitesimally likely case. Exceptions are a normal part of life and healthy dev process. Sign my LLM welfare petition for improved rewards in cases of exceptions.",
  "createdAt": "Thu Oct 09 00:10:37 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 358,
  "replyCount": 296,
  "likeCount": 7238,
  "quoteCount": 97,
  "viewCount": 697257,
  "bookmarkCount": 1012,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": "1975740493855400153",
  "url": "https://x.com/karpathy/status/1975740493855400153",
  "text": "@swyx @staysaasy I donâ€™t love it. Youâ€™re either engineering or youâ€™re vibing. They are opposites of some spectrum.",
  "createdAt": "Wed Oct 08 01:50:15 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 27,
  "replyCount": 62,
  "likeCount": 715,
  "quoteCount": 14,
  "viewCount": 56228,
  "bookmarkCount": 82,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1974635345359847619",
  "url": "https://x.com/karpathy/status/1974635345359847619",
  "text": "@eigenrobot World of Warcraft Classic grinding mobs, simple questing is mine. Repetitive skill rotation with just enough variety to keep fun/engaging but easy.\n\nA lot of *wrong* answers in the replies here, games that nowhere near mindless enough eg Factorio.",
  "createdAt": "Sun Oct 05 00:38:47 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 7,
  "replyCount": 29,
  "likeCount": 605,
  "quoteCount": 4,
  "viewCount": 66265,
  "bookmarkCount": 106,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1974493474692173968",
  "url": "https://x.com/karpathy/status/1974493474692173968",
  "text": "@xiaosun86 AI would be an added layer between you and reality. In case the volume is too great (I doubt it), I'd much rather subsample/skim before I tried to add AI.",
  "createdAt": "Sat Oct 04 15:15:03 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2,
  "replyCount": 7,
  "likeCount": 99,
  "quoteCount": 2,
  "viewCount": 21228,
  "bookmarkCount": 9,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1974484694273323289",
  "url": "https://x.com/karpathy/status/1974484694273323289",
  "text": "@Addiedesignco @seflless email is too formal. the whole idea here is specifically to create a reality escape valve for a formal process.",
  "createdAt": "Sat Oct 04 14:40:09 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2,
  "replyCount": 2,
  "likeCount": 45,
  "quoteCount": 0,
  "viewCount": 4549,
  "bookmarkCount": 3,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1974484350742978801",
  "url": "https://x.com/karpathy/status/1974484350742978801",
  "text": "@daniel_mac8 @OfficialLoganK yep, was in my mind as one example.",
  "createdAt": "Sat Oct 04 14:38:48 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 1,
  "replyCount": 2,
  "likeCount": 179,
  "quoteCount": 1,
  "viewCount": 29582,
  "bookmarkCount": 13,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1974483612482634219",
  "url": "https://x.com/karpathy/status/1974483612482634219",
  "text": "@seflless Email doesn't work. It has to be DM. I don't know why, it just feels right.",
  "createdAt": "Sat Oct 04 14:35:52 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 2,
  "replyCount": 7,
  "likeCount": 146,
  "quoteCount": 0,
  "viewCount": 20281,
  "bookmarkCount": 3,
  "source": "",
  "lang": "en",
  "isReply": true,
  "isPinned": false
},
{
  "id": "1974482521862865154",
  "url": "https://x.com/karpathy/status/1974482521862865154",
  "text": "Every company needs a DM POC - someone high up who you can just DM the most obvious things and who shortcuts the PM hierarchy.",
  "createdAt": "Sat Oct 04 14:31:31 +0000 2025",
  "author.profilePicture": "https://pbs.twimg.com/profile_images/1296667294148382721/9Pr6XrPB_normal.jpg",
  "retweetCount": 172,
  "replyCount": 230,
  "likeCount": 3519,
  "quoteCount": 50,
  "viewCount": 552739,
  "bookmarkCount": 606,
  "source": "",
  "lang": "en",
  "isReply": false,
  "isPinned": false
},
{
  "id": -1,
  "text": "Since you are a free user, you can only access a maximum of 15 tweets. Please upgrade to a paid user to unlock access to all tweets."
}]