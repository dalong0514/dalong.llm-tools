#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Skilljarè§†é¢‘ä¸‹è½½è„šæœ¬
ç”¨äºä¸‹è½½anthropic.skilljar.comä¸Šçš„Claude Code in Actionè¯¾ç¨‹è§†é¢‘
æ”¯æŒä¸‹è½½æ•´ä¸ªè¯¾ç¨‹æˆ–å•ä¸ªé¡µé¢è§†é¢‘

ç”¨æ³•:
1. ä¸‹è½½æ•´ä¸ªè¯¾ç¨‹: python skilljar_download.py [è¾“å‡ºç›®å½•]
2. ä¸‹è½½å•ä¸ªé¡µé¢: python skilljar_download.py [é¡µé¢URL] [è¾“å‡ºç›®å½•]

ç¤ºä¾‹:
- ä¸‹è½½æ•´ä¸ªè¯¾ç¨‹åˆ°é»˜è®¤ç›®å½•: python skilljar_download.py
- ä¸‹è½½æ•´ä¸ªè¯¾ç¨‹åˆ°æŒ‡å®šç›®å½•: python skilljar_download.py /path/to/output
- ä¸‹è½½å•ä¸ªé¡µé¢: python skilljar_download.py https://anthropic.skilljar.com/claude-code-in-action/303235 /path/to/output
"""

import os
import sys
import re
import time
from urllib.parse import urljoin, urlparse
from yt_dlp import YoutubeDL
import requests
from bs4 import BeautifulSoup

# ä»£ç†è®¾ç½®ï¼ˆæ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
PROXY = "http://127.0.0.1:7890"

# è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

def get_page_content(url):
    """è·å–é¡µé¢å†…å®¹"""
    try:
        session = requests.Session()
        proxies = {'http': PROXY, 'https': PROXY} if PROXY else None
        
        response = session.get(url, headers=HEADERS, proxies=proxies, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"è·å–é¡µé¢å†…å®¹å¤±è´¥: {e}")
        return None

def extract_lesson_urls(html_content, base_url):
    """ä»HTMLå†…å®¹ä¸­æå–æ‰€æœ‰è¯¾ç¨‹ç« èŠ‚çš„URL"""
    lesson_urls = []
    
    if not html_content:
        return lesson_urls
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ–¹æ³•1: æŸ¥æ‰¾data-urlå±æ€§çš„è¯¾ç¨‹é“¾æ¥
    for li in soup.find_all('li', {'data-url': True}):
        data_url = li.get('data-url', '')
        if data_url and '/claude-code-in-action/' in data_url:
            full_url = urljoin(base_url, data_url)
            if full_url not in lesson_urls:
                lesson_urls.append(full_url)
    
    # æ–¹æ³•2: æŸ¥æ‰¾è¯¾ç¨‹ç« èŠ‚é“¾æ¥
    curriculum_selectors = [
        'a[href*="/lesson/"]',
        'a[href*="/module/"]', 
        'a[href*="/chapter/"]',
        '.lesson-row a',
        '.lesson-video a',
        '.dp-curriculum a',
        '#curriculum-list a',
        'li.lesson-video a',
        'li[class*="lesson-"] a'
    ]
    
    for selector in curriculum_selectors:
        for link in soup.select(selector):
            href = link.get('href', '')
            if href and ('/lesson/' in href or '/claude-code-in-action/' in href):
                full_url = urljoin(base_url, href)
                if full_url not in lesson_urls:
                    lesson_urls.append(full_url)
    
    # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„é“¾æ¥
    for a in soup.find_all('a', href=True):
        href = a['href']
        if ('/lesson/' in href or '/claude-code-in-action/' in href) and not href.startswith('#'):
            full_url = urljoin(base_url, href)
            if full_url not in lesson_urls:
                lesson_urls.append(full_url)
    
    # æ–¹æ³•4: æŸ¥æ‰¾JavaScriptä¸­çš„è¯¾ç¨‹æ•°æ®
    script_tags = soup.find_all('script')
    for script in script_tags:
        if script.string:
            # æŸ¥æ‰¾è¯¾ç¨‹URLæ¨¡å¼
            patterns = [
                r'/claude-code-in-action/\d+',
                r'lesson/[\w-]+',
                r'https?://[^"\']*skilljar[^"\']*/claude-code-in-action[^"\']*'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, script.string, re.IGNORECASE)
                for match in matches:
                    if match.startswith('http'):
                        lesson_urls.append(match)
                    else:
                        full_url = urljoin(base_url, match)
                        if full_url not in lesson_urls:
                            lesson_urls.append(full_url)
    
    return list(set(lesson_urls))

def extract_video_urls_from_lesson(html_content, base_url, lesson_number):
    """ä»å•ä¸ªè¯¾ç¨‹é¡µé¢æå–è§†é¢‘URL"""
    video_urls = []
    
    if not html_content:
        return video_urls
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ä¿å­˜è¯¾ç¨‹é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•
    debug_dir = "/tmp/skilljar_debug"
    os.makedirs(debug_dir, exist_ok=True)
    debug_file = os.path.join(debug_dir, f"lesson_{lesson_number}.html")
    with open(debug_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # 1. æŸ¥æ‰¾JWPlayeré…ç½®
    script_tags = soup.find_all('script')
    for script in script_tags:
        if script.string:
            # æŸ¥æ‰¾JWPlayeré…ç½®
            jwplayer_patterns = [
                r'videoJsArgs\.assetJWPlayerUrl\s*=\s*["\']([^"\']+)["\']',
                r'jwplayer\(["\']([^"\']+)["\']\)',
                r'content\.jwplatform\.com/players/([^"\']+)',
                r'file["\']?\s*[:=]\s*["\'](https?://[^"\']+\.(?:mp4|m3u8))["\']',
                r'videoUrl["\']?\s*[:=]\s*["\'](https?://[^"\']+)["\']',
                r'jwplayer\(\s*["\']([^"\']+)["\']\s*\)'
            ]
            
            for pattern in jwplayer_patterns:
                matches = re.findall(pattern, script.string, re.IGNORECASE)
                for match in matches:
                    if match.startswith('http'):
                        video_urls.append(match)
                    else:
                        # æ„å»ºJWPlayer URL
                        jw_url = f"https://content.jwplatform.com/players/{match}"
                        video_urls.append(jw_url)
    
    # 2. æŸ¥æ‰¾è§†é¢‘æ ‡é¢˜
    video_title = "Unknown"
    title_selectors = ['h1', 'h2', 'h3', '.video-title', '.lesson-title', '.title']
    for selector in title_selectors:
        title_elem = soup.select_one(selector)
        if title_elem and title_elem.text.strip():
            video_title = title_elem.text.strip()
            break
    
    # 3. iframeåµŒå…¥
    for iframe in soup.find_all('iframe'):
        src = iframe.get('src', '')
        if src and any(keyword in src.lower() for keyword in ['video', 'player', 'vimeo', 'youtube', 'jwplayer']):
            video_urls.append(urljoin(base_url, src))
    
    # 4. videoæ ‡ç­¾
    for video in soup.find_all('video'):
        src = video.get('src', '')
        if src:
            video_urls.append(urljoin(base_url, src))
        
        for source in video.find_all('source'):
            src = source.get('src', '')
            if src:
                video_urls.append(urljoin(base_url, src))
    
    # 5. å…¶ä»–JavaScriptæ•°æ®
    for script in script_tags:
        if script.string:
            patterns = [
                r'https?://[^"\']*\.(?:mp4|m3u8|m4v|mov|avi|webm)[^"\']*',
                r'src[:=]["\'](https?://[^"\']+)["\']',
                r'url[:=]["\'](https?://[^"\']+)["\']',
                r'cloudfront\.net[^"\']*\.(?:mp4|m3u8)',
                r'aws[^"\']*\.(?:mp4|m3u8)',
                r'https?://[^"\']*skilljar[^"\']*\.(?:mp4|m3u8)'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, script.string, re.IGNORECASE)
                for match in matches:
                    if not match.startswith('http'):
                        match = 'https://' + match
                    video_urls.append(match)
    
    # æ·»åŠ è§†é¢‘æ ‡é¢˜ä¿¡æ¯åˆ°URLï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if video_urls and video_title != "Unknown":
        video_urls = [f"{url}?title={video_title}" for url in video_urls]
    
    # å»é‡
    video_urls = list(set(video_urls))
    return video_urls

def download_video(video_url, output_path="/Users/Daglas/Desktop/skilljar_videos"):
    """ä¸‹è½½å•ä¸ªè§†é¢‘"""
    os.makedirs(output_path, exist_ok=True)
    
    ydl_opts = {
        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
        'outtmpl': os.path.join(os.path.abspath(output_path), '%(title)s.%(ext)s'),
        'proxy': PROXY,
        'noplaylist': True,
        'quiet': False,
        'no_warnings': False,
        'retries': 3,  # å‡å°‘é‡è¯•æ¬¡æ•°é¿å…é•¿æ—¶é—´ç­‰å¾…
        'socket_timeout': 15,  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
        'merge_output_format': 'mp4',
        'http_headers': HEADERS,
        'ignoreerrors': True,  # å¿½ç•¥é”™è¯¯ç»§ç»­ä¸‹è½½å…¶ä»–è§†é¢‘
        'timeout': 30,  # æ€»è¶…æ—¶æ—¶é—´
    }
    
    try:
        print(f"æ­£åœ¨ä¸‹è½½: {video_url}")
        with YoutubeDL(ydl_opts) as ydl:
            info_dict = ydl.extract_info(video_url, download=True)
            if info_dict:
                print(f"âœ… ä¸‹è½½æˆåŠŸ: {info_dict.get('title', 'Unknown')}")
                return True
            else:
                print(f"âŒ æ— æ³•è·å–è§†é¢‘ä¿¡æ¯")
                return False
    except Exception as e:
        error_msg = str(e)
        if "timed out" in error_msg or "timeout" in error_msg:
            print(f"â° ä¸‹è½½è¶…æ—¶: {video_url}")
        elif "404" in error_msg or "Not Found" in error_msg:
            print(f"ğŸ” è§†é¢‘ä¸å­˜åœ¨: {video_url}")
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {error_msg}")
        return False

def download_skilljar_course(course_url, output_path="/Users/Daglas/Desktop/skilljar_videos"):
    """ä¸‹è½½æ•´ä¸ªSkilljarè¯¾ç¨‹çš„è§†é¢‘"""
    print(f"å¼€å§‹å¤„ç†è¯¾ç¨‹é¡µé¢: {course_url}")
    
    # è·å–é¡µé¢å†…å®¹
    html_content = get_page_content(course_url)
    if not html_content:
        print("æ— æ³•è·å–è¯¾ç¨‹é¡µé¢å†…å®¹")
        return
    
    # ä¿å­˜HTMLå†…å®¹ç”¨äºè°ƒè¯•
    debug_html_path = os.path.join(output_path, "debug_page.html")
    os.makedirs(output_path, exist_ok=True)
    with open(debug_html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°: {debug_html_path}")
    
    # æå–æ‰€æœ‰è¯¾ç¨‹ç« èŠ‚URL
    lesson_urls = extract_lesson_urls(html_content, course_url)
    
    if not lesson_urls:
        print("æœªæ‰¾åˆ°è¯¾ç¨‹ç« èŠ‚é“¾æ¥ï¼Œå°è¯•ç›´æ¥æå–è§†é¢‘...")
        # å›é€€åˆ°ç›´æ¥æå–è§†é¢‘çš„æ–¹æ³•
        video_urls = extract_video_urls_from_lesson(html_content, course_url)
        
        if not video_urls:
            print("æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥ï¼Œå»ºè®®æ‰‹åŠ¨åˆ†æ...")
            print("\nå»ºè®®æ‰‹åŠ¨åˆ†ææ–¹æ³•:")
            print("1. æ‰“å¼€æµè§ˆå™¨å¼€å‘è€…å·¥å…· (F12)")
            print("2. è®¿é—®è¯¾ç¨‹é¡µé¢")
            print("3. åœ¨ç½‘ç»œ(Network)æ ‡ç­¾é¡µä¸­ç­›é€‰mp4/m3u8æ–‡ä»¶")
            print("4. æ‰¾åˆ°è§†é¢‘URLåï¼Œå¯ä»¥æ‰‹åŠ¨æ·»åŠ åˆ°ä¸‹è½½åˆ—è¡¨ä¸­")
            
            manual_list_path = os.path.join(output_path, "manual_video_list.txt")
            with open(manual_list_path, 'w', encoding='utf-8') as f:
                f.write("# æ‰‹åŠ¨æ·»åŠ è§†é¢‘URLåˆ°è¿™é‡Œï¼Œæ¯è¡Œä¸€ä¸ª\n")
                f.write("# ç¤ºä¾‹: https://example.com/video.mp4\n")
            print(f"æ‰‹åŠ¨ä¸‹è½½æ¨¡æ¿å·²åˆ›å»º: {manual_list_path}")
            return
        
        print(f"æ‰¾åˆ° {len(video_urls)} ä¸ªå¯èƒ½çš„è§†é¢‘é“¾æ¥:")
        for i, url in enumerate(video_urls, 1):
            print(f"  {i}. {url}")
        
        # ä¿å­˜URLåˆ—è¡¨
        url_list_path = os.path.join(output_path, "video_urls.txt")
        with open(url_list_path, 'w', encoding='utf-8') as f:
            for url in video_urls:
                f.write(f"{url}\n")
        print(f"è§†é¢‘URLåˆ—è¡¨å·²ä¿å­˜åˆ°: {url_list_path}")
        
        # ä¸‹è½½è§†é¢‘
        download_videos(video_urls, output_path)
        return
    
    print(f"æ‰¾åˆ° {len(lesson_urls)} ä¸ªè¯¾ç¨‹ç« èŠ‚:")
    for i, url in enumerate(lesson_urls, 1):
        print(f"  {i}. {url}")
    
    # ä¿å­˜è¯¾ç¨‹ç« èŠ‚åˆ—è¡¨
    lesson_list_path = os.path.join(output_path, "lesson_urls.txt")
    with open(lesson_list_path, 'w', encoding='utf-8') as f:
        for url in lesson_urls:
            f.write(f"{url}\n")
    print(f"è¯¾ç¨‹ç« èŠ‚åˆ—è¡¨å·²ä¿å­˜åˆ°: {lesson_list_path}")
    
    # éå†æ¯ä¸ªè¯¾ç¨‹ç« èŠ‚ï¼Œæå–å¹¶ä¸‹è½½è§†é¢‘
    all_video_urls = []
    
    for i, lesson_url in enumerate(lesson_urls, 1):
        print(f"\nå¤„ç†ç¬¬ {i}/{len(lesson_urls)} ä¸ªè¯¾ç¨‹ç« èŠ‚:")
        print(f"URL: {lesson_url}")
        
        # è·å–è¯¾ç¨‹ç« èŠ‚é¡µé¢å†…å®¹
        lesson_html = get_page_content(lesson_url)
        if not lesson_html:
            print(f"âŒ æ— æ³•è·å–è¯¾ç¨‹ç« èŠ‚é¡µé¢: {lesson_url}")
            continue
        
        # æå–è§†é¢‘URL
        video_urls = extract_video_urls_from_lesson(lesson_html, lesson_url, i)
        
        if video_urls:
            print(f"  æ‰¾åˆ° {len(video_urls)} ä¸ªè§†é¢‘é“¾æ¥")
            for video_url in video_urls:
                if video_url not in all_video_urls:
                    all_video_urls.append(video_url)
                    print(f"    - {video_url}")
        else:
            print("  æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥")
            
        # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘å¤„ç†ä¸­çš„æ¶ˆæ¯
        if "video is still being processed" in lesson_html:
            print("  âš ï¸  è§†é¢‘ä»åœ¨å¤„ç†ä¸­ï¼Œå¯èƒ½éœ€è¦ç­‰å¾…æˆ–éœ€è¦è®¤è¯")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(1)
    
    if not all_video_urls:
        print("\nåœ¨æ‰€æœ‰è¯¾ç¨‹ç« èŠ‚ä¸­å‡æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥")
        return
    
    print(f"\næ€»å…±æ‰¾åˆ° {len(all_video_urls)} ä¸ªè§†é¢‘é“¾æ¥:")
    for i, url in enumerate(all_video_urls, 1):
        print(f"  {i}. {url}")
    
    # ä¿å­˜è§†é¢‘URLåˆ—è¡¨
    video_list_path = os.path.join(output_path, "video_urls.txt")
    with open(video_list_path, 'w', encoding='utf-8') as f:
        for url in all_video_urls:
            f.write(f"{url}\n")
    print(f"è§†é¢‘URLåˆ—è¡¨å·²ä¿å­˜åˆ°: {video_list_path}")
    
    # ä¸‹è½½æ‰€æœ‰è§†é¢‘
    download_videos(all_video_urls, output_path)

def download_videos(video_urls, output_path):
    """ä¸‹è½½è§†é¢‘åˆ—è¡¨"""
    print("\nå¼€å§‹è‡ªåŠ¨ä¸‹è½½...")
    
    success_count = 0
    failed_urls = []
    
    for i, video_url in enumerate(video_urls, 1):
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨ä¸‹è½½ç¬¬ {i}/{len(video_urls)} ä¸ªè§†é¢‘")
        print(f"URL: {video_url}")
        print(f"{'='*60}")
        
        if download_video(video_url, output_path):
            success_count += 1
            print(f"âœ… ç¬¬ {i} ä¸ªè§†é¢‘ä¸‹è½½æˆåŠŸ")
        else:
            failed_urls.append((i, video_url))
            print(f"âŒ ç¬¬ {i} ä¸ªè§†é¢‘ä¸‹è½½å¤±è´¥")
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(2)
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\n{'='*60}")
    print("ä¸‹è½½å®Œæˆï¼ç»Ÿè®¡ç»“æœï¼š")
    print(f"æ€»å…±è§†é¢‘æ•°é‡: {len(video_urls)}")
    print(f"æˆåŠŸä¸‹è½½: {success_count}")
    print(f"å¤±è´¥æ•°é‡: {len(failed_urls)}")
    print(f"æˆåŠŸç‡: {success_count/len(video_urls)*100:.1f}%")
    
    if failed_urls:
        print(f"\nå¤±è´¥çš„è§†é¢‘é“¾æ¥:")
        for idx, url in failed_urls:
            print(f"  {idx}. {url}")
    
    # æä¾›å…³äºå—ä¿æŠ¤å†…å®¹çš„å»ºè®®
    if success_count == 0 and len(video_urls) > 0:
        print(f"\n{'='*60}")
        print("âš ï¸  é‡è¦æç¤ºï¼š")
        print("æ‰€æœ‰è§†é¢‘ä¸‹è½½å¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºï¼š")
        print("1. è§†é¢‘å†…å®¹å—ä¿æŠ¤ï¼Œéœ€è¦ç”¨æˆ·è®¤è¯")
        print("2. è§†é¢‘ä»åœ¨å¤„ç†ä¸­ï¼ˆæ˜¾ç¤º'video is still being processed'ï¼‰")
        print("3. éœ€è¦æœ‰æ•ˆçš„ç”¨æˆ·ä¼šè¯æˆ–cookie")
        print("4. è§†é¢‘URLéœ€è¦åŠ¨æ€ç”Ÿæˆæˆ–æˆæƒ")
        print("\nå»ºè®®æ‰‹åŠ¨è®¿é—®è¯¾ç¨‹é¡µé¢æ£€æŸ¥è§†é¢‘è®¿é—®æƒé™")

def download_single_page_video(page_url, output_path="/Users/Daglas/Desktop/skilljar_videos"):
    """ä¸‹è½½å•ä¸ªç½‘é¡µä¸­çš„è§†é¢‘"""
    print(f"å¼€å§‹å¤„ç†å•ä¸ªé¡µé¢: {page_url}")
    
    # è·å–é¡µé¢å†…å®¹
    html_content = get_page_content(page_url)
    if not html_content:
        print("æ— æ³•è·å–é¡µé¢å†…å®¹")
        return
    
    # ä¿å­˜HTMLå†…å®¹ç”¨äºè°ƒè¯•
    debug_html_path = os.path.join(output_path, "debug_single_page.html")
    os.makedirs(output_path, exist_ok=True)
    with open(debug_html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°: {debug_html_path}")
    
    # ç›´æ¥æå–è§†é¢‘URL
    video_urls = extract_video_urls_from_lesson(html_content, page_url, 1)
    
    if not video_urls:
        print("æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥ï¼Œå°è¯•ç›´æ¥æå–è§†é¢‘...")
        
        # å°è¯•å…¶ä»–æå–æ–¹æ³•
        # soup = BeautifulSoup(html_content, 'html.parser')  # æš‚æ—¶æ³¨é‡Šæ‰æœªä½¿ç”¨çš„å˜é‡
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘å¤„ç†ä¸­çš„æ¶ˆæ¯
        if "video is still being processed" in html_content:
            print("âš ï¸  è§†é¢‘ä»åœ¨å¤„ç†ä¸­ï¼Œå¯èƒ½éœ€è¦ç­‰å¾…æˆ–éœ€è¦è®¤è¯")
        
        print("\nå»ºè®®æ‰‹åŠ¨åˆ†ææ–¹æ³•:")
        print("1. æ‰“å¼€æµè§ˆå™¨å¼€å‘è€…å·¥å…· (F12)")
        print("2. è®¿é—®é¡µé¢")
        print("3. åœ¨ç½‘ç»œ(Network)æ ‡ç­¾é¡µä¸­ç­›é€‰mp4/m3u8æ–‡ä»¶")
        print("4. æ‰¾åˆ°è§†é¢‘URLåï¼Œå¯ä»¥æ‰‹åŠ¨æ·»åŠ åˆ°ä¸‹è½½åˆ—è¡¨ä¸­")
        
        manual_list_path = os.path.join(output_path, "manual_video_list.txt")
        with open(manual_list_path, 'w', encoding='utf-8') as f:
            f.write("# æ‰‹åŠ¨æ·»åŠ è§†é¢‘URLåˆ°è¿™é‡Œï¼Œæ¯è¡Œä¸€ä¸ª\n")
            f.write("# ç¤ºä¾‹: https://example.com/video.mp4\n")
        print(f"æ‰‹åŠ¨ä¸‹è½½æ¨¡æ¿å·²åˆ›å»º: {manual_list_path}")
        return
    
    print(f"æ‰¾åˆ° {len(video_urls)} ä¸ªå¯èƒ½çš„è§†é¢‘é“¾æ¥:")
    for i, url in enumerate(video_urls, 1):
        print(f"  {i}. {url}")
    
    # ä¿å­˜URLåˆ—è¡¨
    url_list_path = os.path.join(output_path, "single_page_video_urls.txt")
    with open(url_list_path, 'w', encoding='utf-8') as f:
        for url in video_urls:
            f.write(f"{url}\n")
    print(f"è§†é¢‘URLåˆ—è¡¨å·²ä¿å­˜åˆ°: {url_list_path}")
    
    # ä¸‹è½½è§†é¢‘
    download_videos(video_urls, output_path)

def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤è¾“å‡ºç›®å½•
    output_path = "/Users/Daglas/Desktop/skilljar_videos"
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # ç¬¬ä¸€ä¸ªå‚æ•°å¯èƒ½æ˜¯URLæˆ–è¾“å‡ºè·¯å¾„
        first_arg = sys.argv[1]
        
        if first_arg.startswith('http'):
            # ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯URL
            target_url = first_arg
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¬äºŒä¸ªå‚æ•°ï¼ˆè¾“å‡ºè·¯å¾„ï¼‰
            if len(sys.argv) > 2:
                output_path = sys.argv[2]
        else:
            # ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¾“å‡ºè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤URL
            output_path = first_arg
            target_url = "https://anthropic.skilljar.com/claude-code-in-action"
    else:
        # æ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
        target_url = "https://anthropic.skilljar.com/claude-code-in-action"
    
    print("Skilljarè§†é¢‘ä¸‹è½½å·¥å…·")
    print("=" * 50)
    print(f"ç›®æ ‡URL: {target_url}")
    print(f"è¾“å‡ºç›®å½•: {output_path}")
    print("=" * 50)
    
    # åˆ¤æ–­æ˜¯è¯¾ç¨‹é¡µé¢è¿˜æ˜¯å•ä¸ªé¡µé¢
    # æ›´å¯é çš„æ£€æµ‹æ–¹æ³•ï¼šæ£€æŸ¥URLè·¯å¾„éƒ¨åˆ†æ˜¯å¦æœ‰æ•°å­—ID
    parsed_url = urlparse(target_url)
    path_parts = parsed_url.path.strip('/').split('/')
    
    # å•ä¸ªé¡µé¢çš„ç‰¹å¾ï¼šè·¯å¾„åŒ…å«æ•°å­—IDä¸”ä¸æ˜¯è¯¾ç¨‹ä¸»é¡µ
    is_single_page = (
        "/claude-code-in-action/" in target_url and
        len(path_parts) >= 2 and
        path_parts[-1].isdigit() and  # æœ€åä¸€éƒ¨åˆ†æ˜¯æ•°å­—
        path_parts[0] == "claude-code-in-action"
    )
    
    if is_single_page:
        # å•ä¸ªè¯¾ç¨‹é¡µé¢
        print("æ£€æµ‹åˆ°å•ä¸ªè¯¾ç¨‹é¡µé¢ï¼Œå¼€å§‹ä¸‹è½½å•ä¸ªè§†é¢‘...")
        download_single_page_video(target_url, output_path)
    else:
        # è¯¾ç¨‹ä¸»é¡µ
        print("æ£€æµ‹åˆ°è¯¾ç¨‹ä¸»é¡µï¼Œå¼€å§‹ä¸‹è½½æ•´ä¸ªè¯¾ç¨‹...")
        download_skilljar_course(target_url, output_path)

if __name__ == "__main__":
    main()